---

layout: post
category: threads
title: "r/MachineLearning - [News] Megatron-LM: NVIDIA trains 8.3B GPT-2 using model and data parallelism on 512 GPUs. SOTA in language modelling and SQUAD. Details awaited."
date: 2019-08-13 17:22:34
link: https://vrhk.co/2YNyqNN
image: https://external-preview.redd.it/wMIrpOFgZZI3fD58iMnrGavaQ34PRlx4SyJMIlvzf8c.jpg?auto=webp&s=3745ef8897d4ed666af18eeb8291a21e8cc06924
domain: reddit.com
author: "reddit"
icon: https://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "0 votes and 0 comments so far on Reddit"

---

### r/MachineLearning - [News] Megatron-LM: NVIDIA trains 8.3B GPT-2 using model and data parallelism on 512 GPUs. SOTA in language modelling and SQUAD. Details awaited.

0 votes and 0 comments so far on Reddit