---

layout: post
category: product
title: "Import AI: 183: Curve-fitting conversation with Meena; GANs show us our climate change future; and what compute-data arbitrage means  "
date: 2020-02-03 16:26:39
link: https://vrhk.co/37V86Sm
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Can curve-fitting make for good conversation?&hellip;Google&rsquo;s &ldquo;Meena&rdquo; chatbot suggests it can&hellip;Google researchers have trained a chatbot with uncannily good conversational skills. The bot, named Meena, is a 2.6 billion parameter language model trained on 341GB of text data, filtered from public domain social media conversations. Meena uses a seq2seq model (the same sort of technology that powers Google&rsquo;s &ldquo;Smart Compose&rdquo; feature in gmail), paired with an Evolved Transformer encoder and decoder &ndash; it&rsquo;s interesting to see something like this depend so much on a component developed via neural architecture search. Can it talk? Meena is a pretty good conversationalist, judging by transcripts uploaded to GitHub by Google. It also seems able to invent jokes (e.g., Human: do horses go to Harvard? Meena: Horses go to Hayvard. Human: that&rsquo;s a pretty good joke, I feel like you led me into it. Meena: You were trying to steer it elsewhere, I can see it.)A metric for good conversation: Google developed the &lsquo;Sensibleness and Specificity Average&rsquo; (SSA) measure, which it uses to evaluate how good Meena is in conversation. This metric evaluates the outputs of language models for two traits &ndash; is the response sensible, and is the response specifically tied to what is currently being discussed. To calculate the SSA for a given chatbot, the researchers have a team of crowd workers evaluate some of the outputs of the models, then they use this to create an SSA score.  &nbsp; Humans vs Machines: The best-performing version of Meena gets an SSA of 79%, compared to 86% for an average human. By comparison, other state-of-the-art systems such as DialoGPT (51%) and Cleverbot (44%) do much more poorly. Different release strategy: Along with their capabilities, modern neural language models have also been notable for the different release strategies adopted by the organizations that build them &ndash; OpenAI announced GPT-2 but didn&rsquo;t release it all at once, releasing the model over several months along with research into its potential for misinformation, and its tendencies for biases. Microsoft announced DialoGPT but didn&rsquo;t provide a sampling interface in an attempt to minimize opportunistic misuse, and other companies like NVIDIA have alluded to larger language models (e.g., Megatron), but not released any parts of them.  &nbsp; With Meena, Google is also adopting a different release strategy. &ldquo;Tackling safety and bias in the models is a key focus area for us, and given the challenges related to this, we are not currently releasing an external research demo,&rdquo; they write. &ldquo;We are evaluating the risks and benefits associated with externalizing the model checkpoint, however&rdquo;. Why this matters: How close can massively-scaled function approximation get us to human-grade conversation? Can it get us there at all? Research like this pushes the limits of a certain kind of deliberately naive approach to learning language, and it&rsquo;s curious that we&rsquo;re developing more and more superficially capable systems, despite the lack of domain knowledge and handwritten systems inherent to these approaches.&nbsp;  &nbsp; Read more: Towards a Human-like Open-Domain Chatbot (arXiv).  &nbsp; Read more: Towards a Conversational Agent that Can Chat About&hellip; Anything (Google AI Blog). ####################################################Chinese government use drones to remotely police people in coronavirus-hit areas:&hellip;sTaY hEaLtHy CiTiZeN!&hellip;Chinese security officials are using drones to remotely surveil and talk to people in coronavirus-hit areas of the country. &ldquo;According to a viral video spread on China&rsquo;s Twitter-like Sina Weibo on Friday, officials in a town in Chengdu, Southwest China&rsquo;s Sichuan Province, spotted some people playing mah-jong in a public place.  &nbsp; &ldquo;Playing mah-jong outside is banned during the epidemic. You have been spotted. Stop playing and leave the site as soon as possible,&rdquo; a local official said through a microphone while looking at the screen for a drone.  &nbsp; &ldquo;Don&rsquo;t look at the drone, child. Ask your father to leave immediately,&rdquo; the official said to a child who was looking curiously up at the drone beside the mah-jong table.&rdquo; &ndash; via Global Times.Why this matters: This is a neat illustration of the omni-use nature of technology; here, the drones are being used for a societally-beneficial use (preventing viral transmission), but it&rsquo;s clear they could be used for chilling purposes as well. Perhaps one outcome of the coronavirus outbreak will be a normalization for a certain form of drone surveillance in China? &nbsp; Read more: Drones creatively used in rural areas in battle against coronavirus (Global Times). &nbsp; Watch this video of a drone being used to instruct someone to go home and put on a respirator mask (Global Times, Twitter). &nbsp; ####################################################
Want smarter AI? Train something with an ego!&hellip;Generalization? It&rsquo;s easier if you&rsquo;re self-centered&hellip;Researchers with New York University think that there are a few easy ways to improve generalization of agents trained via reinforcement learning &ndash; and it&rsquo;s all about ego! Specifically, their research suggests that if you can make technical tweaks that make a game more egocentric, that is, more tightly gear the observations around a privileged agent-centered perspective, then your agent will probably generalize better. Specifically, they propose &ldquo;rotating, translating, and cropping the observation around the agent&rsquo;s avatar&rdquo;, to train more general systems.  &nbsp; &ldquo;A local, ego-centric view, allows for better learning in our experiments and the policies learned generalize much better to new environments even when trained on only five environments&rdquo;, they write. The secrets to (forced) generalization:&ndash; Self-centered (aka, translation): Warp the game world so that the agent is always at the dead center of the screen &ndash; this means it&rsquo;ll learn about positions relative to its own consistent frame. &ndash; Rotation: Change the orientation of the game map so that it faces the same direction as the player&rsquo;s avatar. &ldquo;Rotation helps the agent to learn navigation as it simplifies the task. For example: if you want to reach for something on the right, the agent just rotates until that object is above,&rdquo; they explain. &ndash; Zooming in (cropping): Crop the observation around the player, which reduces the state space the agent sees and needs to learn about (by comparison, seeing really complicated environments can make it hard for an agent to learn, as it takes it a looooong time to figure out the underlying dynamics.Testing: They test out their approach on two variants of the game Zelda, the first is a complex Zelda-clone built in the General Video Game AI (GVGAI) framework; the second is a simplified version of the same game. They find that A3C-based agents trained in Zelda with a full set of variations (translation, rotation, cropping) generalize far better than those trained on the game alone (though their test scores of 22% are still pretty poor, compared to what a human might get). Why this matters: Papers like this show how much tweaking goes on behind the scenes to set up training in such a way you get better or more effective learning. It also gives us some clues about the importance of ego-centric views in general, and makes me reflect on the fact I&rsquo;ve spent my entire life learning via an ego-centric/world-centric view. How might my mind be different if my eyeballs were floating high above me, looking at me from different angles, with me uncentered in my field-of-vision? What might I have &lsquo;learned&rsquo; about the world, then, and might I &ndash; similar to RL agents trained in …"

---

### Import AI: 183: Curve-fitting conversation with Meena; GANs show us our climate change future; and what compute-data arbitrage means  

Can curve-fitting make for good conversation?&hellip;Google&rsquo;s &ldquo;Meena&rdquo; chatbot suggests it can&hellip;Google researchers have trained a chatbot with uncannily good conversational skills. The bot, named Meena, is a 2.6 billion parameter language model trained on 341GB of text data, filtered from public domain social media conversations. Meena uses a seq2seq model (the same sort of technology that powers Google&rsquo;s &ldquo;Smart Compose&rdquo; feature in gmail), paired with an Evolved Transformer encoder and decoder &ndash; it&rsquo;s interesting to see something like this depend so much on a component developed via neural architecture search. Can it talk? Meena is a pretty good conversationalist, judging by transcripts uploaded to GitHub by Google. It also seems able to invent jokes (e.g., Human: do horses go to Harvard? Meena: Horses go to Hayvard. Human: that&rsquo;s a pretty good joke, I feel like you led me into it. Meena: You were trying to steer it elsewhere, I can see it.)A metric for good conversation: Google developed the &lsquo;Sensibleness and Specificity Average&rsquo; (SSA) measure, which it uses to evaluate how good Meena is in conversation. This metric evaluates the outputs of language models for two traits &ndash; is the response sensible, and is the response specifically tied to what is currently being discussed. To calculate the SSA for a given chatbot, the researchers have a team of crowd workers evaluate some of the outputs of the models, then they use this to create an SSA score.  &nbsp; Humans vs Machines: The best-performing version of Meena gets an SSA of 79%, compared to 86% for an average human. By comparison, other state-of-the-art systems such as DialoGPT (51%) and Cleverbot (44%) do much more poorly. Different release strategy: Along with their capabilities, modern neural language models have also been notable for the different release strategies adopted by the organizations that build them &ndash; OpenAI announced GPT-2 but didn&rsquo;t release it all at once, releasing the model over several months along with research into its potential for misinformation, and its tendencies for biases. Microsoft announced DialoGPT but didn&rsquo;t provide a sampling interface in an attempt to minimize opportunistic misuse, and other companies like NVIDIA have alluded to larger language models (e.g., Megatron), but not released any parts of them.  &nbsp; With Meena, Google is also adopting a different release strategy. &ldquo;Tackling safety and bias in the models is a key focus area for us, and given the challenges related to this, we are not currently releasing an external research demo,&rdquo; they write. &ldquo;We are evaluating the risks and benefits associated with externalizing the model checkpoint, however&rdquo;. Why this matters: How close can massively-scaled function approximation get us to human-grade conversation? Can it get us there at all? Research like this pushes the limits of a certain kind of deliberately naive approach to learning language, and it&rsquo;s curious that we&rsquo;re developing more and more superficially capable systems, despite the lack of domain knowledge and handwritten systems inherent to these approaches.&nbsp;  &nbsp; Read more: Towards a Human-like Open-Domain Chatbot (arXiv).  &nbsp; Read more: Towards a Conversational Agent that Can Chat About&hellip; Anything (Google AI Blog). ####################################################Chinese government use drones to remotely police people in coronavirus-hit areas:&hellip;sTaY hEaLtHy CiTiZeN!&hellip;Chinese security officials are using drones to remotely surveil and talk to people in coronavirus-hit areas of the country. &ldquo;According to a viral video spread on China&rsquo;s Twitter-like Sina Weibo on Friday, officials in a town in Chengdu, Southwest China&rsquo;s Sichuan Province, spotted some people playing mah-jong in a public place.  &nbsp; &ldquo;Playing mah-jong outside is banned during the epidemic. You have been spotted. Stop playing and leave the site as soon as possible,&rdquo; a local official said through a microphone while looking at the screen for a drone.  &nbsp; &ldquo;Don&rsquo;t look at the drone, child. Ask your father to leave immediately,&rdquo; the official said to a child who was looking curiously up at the drone beside the mah-jong table.&rdquo; &ndash; via Global Times.Why this matters: This is a neat illustration of the omni-use nature of technology; here, the drones are being used for a societally-beneficial use (preventing viral transmission), but it&rsquo;s clear they could be used for chilling purposes as well. Perhaps one outcome of the coronavirus outbreak will be a normalization for a certain form of drone surveillance in China? &nbsp; Read more: Drones creatively used in rural areas in battle against coronavirus (Global Times). &nbsp; Watch this video of a drone being used to instruct someone to go home and put on a respirator mask (Global Times, Twitter). &nbsp; ####################################################
Want smarter AI? Train something with an ego!&hellip;Generalization? It&rsquo;s easier if you&rsquo;re self-centered&hellip;Researchers with New York University think that there are a few easy ways to improve generalization of agents trained via reinforcement learning &ndash; and it&rsquo;s all about ego! Specifically, their research suggests that if you can make technical tweaks that make a game more egocentric, that is, more tightly gear the observations around a privileged agent-centered perspective, then your agent will probably generalize better. Specifically, they propose &ldquo;rotating, translating, and cropping the observation around the agent&rsquo;s avatar&rdquo;, to train more general systems.  &nbsp; &ldquo;A local, ego-centric view, allows for better learning in our experiments and the policies learned generalize much better to new environments even when trained on only five environments&rdquo;, they write. The secrets to (forced) generalization:&ndash; Self-centered (aka, translation): Warp the game world so that the agent is always at the dead center of the screen &ndash; this means it&rsquo;ll learn about positions relative to its own consistent frame. &ndash; Rotation: Change the orientation of the game map so that it faces the same direction as the player&rsquo;s avatar. &ldquo;Rotation helps the agent to learn navigation as it simplifies the task. For example: if you want to reach for something on the right, the agent just rotates until that object is above,&rdquo; they explain. &ndash; Zooming in (cropping): Crop the observation around the player, which reduces the state space the agent sees and needs to learn about (by comparison, seeing really complicated environments can make it hard for an agent to learn, as it takes it a looooong time to figure out the underlying dynamics.Testing: They test out their approach on two variants of the game Zelda, the first is a complex Zelda-clone built in the General Video Game AI (GVGAI) framework; the second is a simplified version of the same game. They find that A3C-based agents trained in Zelda with a full set of variations (translation, rotation, cropping) generalize far better than those trained on the game alone (though their test scores of 22% are still pretty poor, compared to what a human might get). Why this matters: Papers like this show how much tweaking goes on behind the scenes to set up training in such a way you get better or more effective learning. It also gives us some clues about the importance of ego-centric views in general, and makes me reflect on the fact I&rsquo;ve spent my entire life learning via an ego-centric/world-centric view. How might my mind be different if my eyeballs were floating high above me, looking at me from different angles, with me uncentered in my field-of-vision? What might I have &lsquo;learned&rsquo; about the world, then, and might I &ndash; similar to RL agents trained in …