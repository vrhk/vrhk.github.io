---

layout: post
category: threads
title: "[D] Would you ever use a self-attention layer when the set / sequence is one element?"
date: 2020-05-03 23:27:32
link: https://vrhk.co/3bZWHCX
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "If you're computing a function w/ a single continuous vector input and output, would it ever make sense to use a self-attention layer? I'm..."

---

### [D] Would you ever use a self-attention layer when the set / sequence is one element?

If you're computing a function w/ a single continuous vector input and output, would it ever make sense to use a self-attention layer? I'm...