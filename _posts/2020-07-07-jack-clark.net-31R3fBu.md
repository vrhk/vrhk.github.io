---

layout: post
category: product
title: "Import AI 204: Chinese elevator surveillance; Enron emails train polite AI; and a pessimistic view on AGI"
date: 2020-07-07 16:56:36
link: https://vrhk.co/31R3fBu
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s0.wp.com/i/webclip.png
excerpt: "A somewhat abbreviated issue, this week: Due to a combination of things (primarily related to societal complications from COVID-19), I&rsquo;ve found I have less time I&rsquo;m able to devote to this newsletter, among other things. I think many people in COVID-hit countries that are lucky enough to still be employed are having this experience &ndash; even though notionally you should have more time due to no commute and the elimination of various other factors, it feels like you have less time than before. A curious and somewhat unpleasant trait of this current crisis! I hope to resume more regularly scheduled service at typical lengths soon. Thank you as always for reading and writing in, and I hope you and your loved ones are safe during this chaotic time. ####################################################
AI &amp; Creativity: Gwern on writing with GPT3:What is it like to try and write fiction with GPT-3, OpenAI&rsquo;s large-scale language model? Gwern has written a dense, compelling essay about the weirdness of writing with this generative model. Read on to get their take on &lsquo;prompts as programming&rsquo;,&nbsp; the strengths and weaknesses of the GPT-3 model, and to see an immense set of examples of GPT-3 in action. The strange and confusing fun of it all: Gwern has put together a massive collection of generations using the GPT-3 model &ndash; I particularly liked some of the poetry experiments, such as Plath, Whitman, and Cummings. I also think this (machine-generated) extract from a completion of Dr. Seuss&rsquo;s &lsquo;Oh, The Places You&rsquo;ll Go&lsquo;, is quite charming:  &nbsp; &ldquo;You have brains in your head. &nbsp; You have feet in your shoes. &nbsp; You can steer yourself any direction you choose. &nbsp; You&rsquo;re on your way!&rdquo; &nbsp; Read more: GPT-3 Creative Fiction (Gwern).&nbsp;
####################################################
Hey dude, where&rsquo;s my AGI?&hellip;Tired of AI hype? Read this&hellip;In an essay in Nature, an author writes that &ldquo;although development of artificial intelligence for specific purposes has been impressive, we have not come much closer to developing artificial general intelligence&rdquo;. What follows is a narrative about the development of AI and &ldquo;Big Data&rdquo; technologies in the past half century or so, along with discussion of where computers do well and where they do poorly. Much of the criticism of contemporary AI systems can be paraphrased as &lsquo;curve fitting isn&rsquo;t smart&rsquo; &ndash; these technologies, though powerful, are not able to generate capabilities that we should describe as intelligent. &ldquo;The real problem is that computers are not in the world, because they are not embodied,&rdquo; the author writes. Why this matters: I think it&rsquo;s helpful to have more of a sober discourse about whether we&rsquo;re making progress towards AGI or whether we&rsquo;re just developing increasingly capable narrow AI systems. However, one missing piece in this article is a discussion of some of the more recent innovations in generative models &ndash; e.g, the author has a &lsquo;conversation&rsquo; with a chatbot program called Mitsuku and uses this to bolster their arguments about the generally poor capabilities of AI systems. How might their conversation had gone if they&rsquo;d experimented with GPT2 or GPT3 (with context-stuffing via the context window), I wonder? &nbsp; Read more: Why general artificial intelligence will not be realized (Nature, Humanities and Social Sciences Communications). ####################################################
Making language models more polite, via 1.39 million Enron(?!) emails:&hellip;READ THIS &ndash; NOW!&hellip;A few years ago, people developed style transfer techniques for neural nets that let you take a picture, then morph it to have a different style, like becoming a cartoon, or being re-rendered in an impressionist painting style. In recent years, we&rsquo;ve started to be able to do similar things for text, via techniques like fine-tuning. Now, researchers with Carnegie Mellon University, have developed a dataset to help them build systems that can turn text from impolite to polite. The Enron dataset: For the research, they collected a dataset of 1.39 million instances from the &lsquo;Enron&rsquo; email corpus, automatically labelled for politeness with scores from 0 to 1. They&rsquo;ve made the dataset available on GitHub, so it could be an interesting fine-tuning resource. Interesting ethics: The people also hints at some of the ethical challenges involved in doing this sort of research, for instance, by adopting a &ldquo;data driven definition of politeness&rdquo; to automatically clean the email corpus.&nbsp;
Politeness style transfer: They use this dataset to develop a system that does what they call &lsquo;politeness transfer&rsquo;, for example, by converting the phrase &ldquo;send me the data&rdquo; to &ldquo;could you please send me the data?&rdquo;?. They also explore some things with more extreme (autonomous!) editorial choices, like converting the sentiment of a sentence, for instance changing &ldquo;their chips are ok, but their salsa is really bland&rdquo; to &ldquo;their chips are great, but their salsa is really delicious&rdquo;.Why this matters: Reality Editing: The outputs of generative models are like a funhouse mirror version of reality &ndash; they reflect back the things in their training corpus, magnifying and minimizing different aspects of their data distribution. One of the core challenges of AI research for the next few years will be figuring out how to more tightly constrain the outputs of these models so they have more of a desired trait, like politeness, or less of a negative trait, like tendencies to express harmful biases. Datasets and experiments like this give us some of the tools (the data) and ideas that can help us figure out how to better align model outputs with societal requirements.&nbsp;  &nbsp; Read more: Politeness Transfer: A Tag and Generate Approach (arXiv).  &nbsp; Get the dataset and code here (GitHub).####################################################
Coming soon: Elevator surveillance cameras&hellip;100,000 test deployments and counting&hellip;A team from the Shanghai Research Institute has developed a system to automatically identify &ldquo;abnormal activity&rdquo;, such as drug dealing, prostitution, over-crowded residents, and so on. The system is currently in a research phase and deployed on around 100,000 elevators, the authors write. They decided to try out elevator-based surveillance because &ldquo;we find that elevator could be the most feasible and suitable environment to take operation on because it&rsquo;s legal and reasonable to deploy public surveillance and people take elevator widely and frequently enough&rdquo;. They use the system to analyze large amounts of data (here: around one million records for each floor from 100,000 distinct elevators &ndash; the resulting system spits out 643 outliers; in a subsequent analysis, they identify a couple of anomalies worthy of investigation by a local property manager, such as indications of a catering service being run from an apartment, and of an over-crowded residence. What they did: They concoct a system that uses YOLACT to do instance segmentation on people in the video, FaceNet to capture and embed the faces of individuals in the elevator (aiding re-identification in different images),&nbsp; and their own architecture called GraftNet (based on the Inception v3 classification architecture) to learn to assign multiple labels for elevator passengers (labels include: pregnant, oldage, courier, adult holding baby, etc). 

Why this matters: AI and social control: The disturbing thing about this paper is how simple it is &ndash; these are reasonably well understood techniques and systems, and this is simply an application of them. I think it&rsquo;s hard for us to comprehend what kinds of su…"

---

### Import AI 204: Chinese elevator surveillance; Enron emails train polite AI; and a pessimistic view on AGI

A somewhat abbreviated issue, this week: Due to a combination of things (primarily related to societal complications from COVID-19), I&rsquo;ve found I have less time I&rsquo;m able to devote to this newsletter, among other things. I think many people in COVID-hit countries that are lucky enough to still be employed are having this experience &ndash; even though notionally you should have more time due to no commute and the elimination of various other factors, it feels like you have less time than before. A curious and somewhat unpleasant trait of this current crisis! I hope to resume more regularly scheduled service at typical lengths soon. Thank you as always for reading and writing in, and I hope you and your loved ones are safe during this chaotic time. ####################################################
AI &amp; Creativity: Gwern on writing with GPT3:What is it like to try and write fiction with GPT-3, OpenAI&rsquo;s large-scale language model? Gwern has written a dense, compelling essay about the weirdness of writing with this generative model. Read on to get their take on &lsquo;prompts as programming&rsquo;,&nbsp; the strengths and weaknesses of the GPT-3 model, and to see an immense set of examples of GPT-3 in action. The strange and confusing fun of it all: Gwern has put together a massive collection of generations using the GPT-3 model &ndash; I particularly liked some of the poetry experiments, such as Plath, Whitman, and Cummings. I also think this (machine-generated) extract from a completion of Dr. Seuss&rsquo;s &lsquo;Oh, The Places You&rsquo;ll Go&lsquo;, is quite charming:  &nbsp; &ldquo;You have brains in your head. &nbsp; You have feet in your shoes. &nbsp; You can steer yourself any direction you choose. &nbsp; You&rsquo;re on your way!&rdquo; &nbsp; Read more: GPT-3 Creative Fiction (Gwern).&nbsp;
####################################################
Hey dude, where&rsquo;s my AGI?&hellip;Tired of AI hype? Read this&hellip;In an essay in Nature, an author writes that &ldquo;although development of artificial intelligence for specific purposes has been impressive, we have not come much closer to developing artificial general intelligence&rdquo;. What follows is a narrative about the development of AI and &ldquo;Big Data&rdquo; technologies in the past half century or so, along with discussion of where computers do well and where they do poorly. Much of the criticism of contemporary AI systems can be paraphrased as &lsquo;curve fitting isn&rsquo;t smart&rsquo; &ndash; these technologies, though powerful, are not able to generate capabilities that we should describe as intelligent. &ldquo;The real problem is that computers are not in the world, because they are not embodied,&rdquo; the author writes. Why this matters: I think it&rsquo;s helpful to have more of a sober discourse about whether we&rsquo;re making progress towards AGI or whether we&rsquo;re just developing increasingly capable narrow AI systems. However, one missing piece in this article is a discussion of some of the more recent innovations in generative models &ndash; e.g, the author has a &lsquo;conversation&rsquo; with a chatbot program called Mitsuku and uses this to bolster their arguments about the generally poor capabilities of AI systems. How might their conversation had gone if they&rsquo;d experimented with GPT2 or GPT3 (with context-stuffing via the context window), I wonder? &nbsp; Read more: Why general artificial intelligence will not be realized (Nature, Humanities and Social Sciences Communications). ####################################################
Making language models more polite, via 1.39 million Enron(?!) emails:&hellip;READ THIS &ndash; NOW!&hellip;A few years ago, people developed style transfer techniques for neural nets that let you take a picture, then morph it to have a different style, like becoming a cartoon, or being re-rendered in an impressionist painting style. In recent years, we&rsquo;ve started to be able to do similar things for text, via techniques like fine-tuning. Now, researchers with Carnegie Mellon University, have developed a dataset to help them build systems that can turn text from impolite to polite. The Enron dataset: For the research, they collected a dataset of 1.39 million instances from the &lsquo;Enron&rsquo; email corpus, automatically labelled for politeness with scores from 0 to 1. They&rsquo;ve made the dataset available on GitHub, so it could be an interesting fine-tuning resource. Interesting ethics: The people also hints at some of the ethical challenges involved in doing this sort of research, for instance, by adopting a &ldquo;data driven definition of politeness&rdquo; to automatically clean the email corpus.&nbsp;
Politeness style transfer: They use this dataset to develop a system that does what they call &lsquo;politeness transfer&rsquo;, for example, by converting the phrase &ldquo;send me the data&rdquo; to &ldquo;could you please send me the data?&rdquo;?. They also explore some things with more extreme (autonomous!) editorial choices, like converting the sentiment of a sentence, for instance changing &ldquo;their chips are ok, but their salsa is really bland&rdquo; to &ldquo;their chips are great, but their salsa is really delicious&rdquo;.Why this matters: Reality Editing: The outputs of generative models are like a funhouse mirror version of reality &ndash; they reflect back the things in their training corpus, magnifying and minimizing different aspects of their data distribution. One of the core challenges of AI research for the next few years will be figuring out how to more tightly constrain the outputs of these models so they have more of a desired trait, like politeness, or less of a negative trait, like tendencies to express harmful biases. Datasets and experiments like this give us some of the tools (the data) and ideas that can help us figure out how to better align model outputs with societal requirements.&nbsp;  &nbsp; Read more: Politeness Transfer: A Tag and Generate Approach (arXiv).  &nbsp; Get the dataset and code here (GitHub).####################################################
Coming soon: Elevator surveillance cameras&hellip;100,000 test deployments and counting&hellip;A team from the Shanghai Research Institute has developed a system to automatically identify &ldquo;abnormal activity&rdquo;, such as drug dealing, prostitution, over-crowded residents, and so on. The system is currently in a research phase and deployed on around 100,000 elevators, the authors write. They decided to try out elevator-based surveillance because &ldquo;we find that elevator could be the most feasible and suitable environment to take operation on because it&rsquo;s legal and reasonable to deploy public surveillance and people take elevator widely and frequently enough&rdquo;. They use the system to analyze large amounts of data (here: around one million records for each floor from 100,000 distinct elevators &ndash; the resulting system spits out 643 outliers; in a subsequent analysis, they identify a couple of anomalies worthy of investigation by a local property manager, such as indications of a catering service being run from an apartment, and of an over-crowded residence. What they did: They concoct a system that uses YOLACT to do instance segmentation on people in the video, FaceNet to capture and embed the faces of individuals in the elevator (aiding re-identification in different images),&nbsp; and their own architecture called GraftNet (based on the Inception v3 classification architecture) to learn to assign multiple labels for elevator passengers (labels include: pregnant, oldage, courier, adult holding baby, etc). 

Why this matters: AI and social control: The disturbing thing about this paper is how simple it is &ndash; these are reasonably well understood techniques and systems, and this is simply an application of them. I think it&rsquo;s hard for us to comprehend what kinds of su…