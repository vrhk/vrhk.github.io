---

layout: post
category: product
title: "Import AI 178: StyleGAN weaponization; Urdu MNIST; plus, the AI Index 2019"
date: 2019-12-23 11:06:35
link: https://vrhk.co/38ZQXrH
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "AI Index: 2019 edition:&hellip;What data can we use to help us think about the impact of AI?&hellip;The AI Index, a Stanford-backed initiative to assess the progress and impact of AI, has launched its 2019 report. The new report contains a vast amount of data relating to AI, covering areas ranging from bibliometrics, to technical progress, to analysis of diversity within the field of AI. (Disclaimer: I&rsquo;m on the Steering Committee of the AI Index and spent a bunch of this year working on this report).Key statistics:&ndash; 300%: Growth in volume of peer-reviewed AI papers published worldwide. &ndash; 800%: Growth in NeurIPS attendance from 2012 to 2019 &ndash; $70 billion: Total amount invested worldwide in AI in 2019, spread across VC funding, M&amp;A, and IPOs. &ndash; 40: Number of academics who moved to industry in 2018, up from 15 in 2012. NLP progress: In the technology section, the Index highlights the NLP advances that have been going on in the past year by analyzing results on GLUE and SuperGLUE. I asked Sam Bowman what he thought about progress in this part of the field and he said it&rsquo;s clear the technology is advancing, but it&rsquo;s also obvious that we can&rsquo;t easily measure the weaknesses of existing methods.  &nbsp; &ldquo;We know now how to solve an overwhelming majority of the sentence- or paragraph-level text classification benchmark datasets that we&rsquo;ve been able to come up with to date. GLUE and SuperGLUE demonstrate this out nicely, and you can see similar trends across the field of NLP. I don&rsquo;t think we have been in a position even remotely like this before: We&rsquo;re solving hard, AI-oriented challenge tasks just about as fast as we can dream them up,&rdquo; Sam says. &ldquo;I want to emphasize, though, that we haven&rsquo;t solved language understanding yet in any satisfying way.&rdquo;  &nbsp; Read more: The 2019 AI Index report (PDF, official AI Index website). &nbsp; Read past reports here (official AI Index website).&nbsp; ####################################################Diversity in AI data: Urdu MNIST:Researchers with COMSATS University Islamabad and the National University of Ireland have put together a dataset of handwritten Urdu characters and digits, hoping to make it easier for people to train machine learning systems to automatically parse images of Urdu text.The dataset consists of handwritten examples of 10 digits and 40 characters, written by more than 900 individuals, totally more than 45,000 discreet images. &ldquo;The individuals belong to different age groups in the range of 22 to 60 years,&rdquo; they write. The writing styles vary across individuals, increasing the diversity of the dataset.  &nbsp; Get the dataset: For non-commercial uses of the dataset, you can write to the corresponding author ()&nbsp; of the paper to request access to it. (This feels like a bit of a shame &ndash; sticking the dataset on GitHub might help more people discover and use the dataset.)Why this matters: Digitization, much like globalization, has unevenly distributed benefits: places which have invested heavily in digitization have benefited by being able to turn the substance of a culture into a (typically free) digital export, which conditions the environment that other machine learning researchers work in. By digitizing things that are not currently well represented, like Urdu, we broadening the range of cultures represented in the material of AI development.  &nbsp; Read more: Pioneer dataset and automatic recognition of Urdu handwritten characters using a deep autoencoder and convolutional neural network (Arxiv). ####################################################What are the most popular machine learning frameworks used on Kaggle?&hellip;Where tried&amp;tested beats new and flashy&hellip;Kaggle, a platform for algorithmic challenges and development, has released the results of a survey trying to identify the most popular machine learning tools used by developers on the service. These statistics have a pretty significant signal in them, because the frameworks used on kaggle are typically being used to solve real-world tasks or challenges, so popularity here may correlate to practical utility as well. The five most popular frameworks in 2019:&ndash; Scikit-learn&ndash; TensorFlow&ndash; Keras&ndash; RandomForest&ndash; Xgboost(Honorable mention: PyTorch in sixth place). How does this compare to 2018? There hasn&rsquo;t been huge change; in 2018, the popular tools were: Scikit-learn, TensorFlow, Keras, RandomForest, and Caret (with PyTorch in sixth place again). Why this matters: Tools define the scope of what people can build, and any tool also imparts some of the ideology used to construct it; the diversity of today&rsquo;s programming languages typically reflect strong quasi-political preferences on the part of their core developers (compare the utterly restrained &lsquo;Rust&rsquo; language to the more expressive happy-to-let-you-step-on-a-rake coding style inherent to Python, for instance). As AI influences more and more of society, it&rsquo;ll be valuable to track which tools are popular and which &ndash; such as TensorFlow, Keras, and PyTorch &ndash; are predominantly developed by the private sector.  &nbsp; Read more: Most popular machine learning frameworks, 2019 (Kaggle). ####################################################Digital phrenology and dangerous datasets: Gait identification:&hellip;Can we spot a liar from their walk &ndash; and should we even try to?&hellip;Back in the 19th century a load of intellectuals thought a decent way to talk about differences between humans was by making arbitrary judgements about their mental character by analyzing their physical appearance, ranging from the color of their skin to the dimensions of their skull. This was a bad idea. Now, that same approach to science has returned at-scale with the advent of machine learning technologies, where researchers are developing classification systems based on similarly wobbly scientific assumptions. The Liar&rsquo;s Walk: New research from the University of North Carolina and the University of Maryland tries to train a machine learning classifier to spot deceptive people by the gait of their walk. The research is worth reading about in part because of how it seems to ignore the manifold ethical implications of developing such a system, and also barely interrogates its own underlying premise (that it&rsquo;s possible to look at someone&rsquo;s gait and work out if they&rsquo;re being deceptive or not). The researchers say such classifiers could be used for public safety in places like train stations and airports. That may well be true, but the research would need to actually work for this to be the case &ndash; and I&rsquo;m not sure it does. Garbage (data) in and garbage (data) out: Here, the researchers commit a cardinal sin of machine learning research: they make a really crappy dataset and base their research project on this. Specifically, the researchers recruited 88 participants from a university campus, then had the participants walk around in natural and deceptive ways around the campus. They then trained a classifier to ID deceptive versus honest walks, obtaining an &ldquo;accuracy&rdquo; of 93.4%&nbsp; on classifying people&rsquo;s movements. But this accuracy figure is an illusion &ndash; really, given the wobbly ground on which this paper is based. V1 versus V2: I publicized this paper on Twitter a few days prior to this issue going out; since then, the authors have updated the paper to a &lsquo;v2&rsquo; version, which includes a lengthier discussion of limitations and inherent issues with the approach at the end &ndash; this feels like an improvement, though I&rsquo;m still generally uneasy about the way they&rsquo;ve contextualized this research. However, it&rsquo;s crucial that as a community we note when people appear to update in response to criticism, and I&rsquo;m hopeful this is the…"

---

### Import AI 178: StyleGAN weaponization; Urdu MNIST; plus, the AI Index 2019

AI Index: 2019 edition:&hellip;What data can we use to help us think about the impact of AI?&hellip;The AI Index, a Stanford-backed initiative to assess the progress and impact of AI, has launched its 2019 report. The new report contains a vast amount of data relating to AI, covering areas ranging from bibliometrics, to technical progress, to analysis of diversity within the field of AI. (Disclaimer: I&rsquo;m on the Steering Committee of the AI Index and spent a bunch of this year working on this report).Key statistics:&ndash; 300%: Growth in volume of peer-reviewed AI papers published worldwide. &ndash; 800%: Growth in NeurIPS attendance from 2012 to 2019 &ndash; $70 billion: Total amount invested worldwide in AI in 2019, spread across VC funding, M&amp;A, and IPOs. &ndash; 40: Number of academics who moved to industry in 2018, up from 15 in 2012. NLP progress: In the technology section, the Index highlights the NLP advances that have been going on in the past year by analyzing results on GLUE and SuperGLUE. I asked Sam Bowman what he thought about progress in this part of the field and he said it&rsquo;s clear the technology is advancing, but it&rsquo;s also obvious that we can&rsquo;t easily measure the weaknesses of existing methods.  &nbsp; &ldquo;We know now how to solve an overwhelming majority of the sentence- or paragraph-level text classification benchmark datasets that we&rsquo;ve been able to come up with to date. GLUE and SuperGLUE demonstrate this out nicely, and you can see similar trends across the field of NLP. I don&rsquo;t think we have been in a position even remotely like this before: We&rsquo;re solving hard, AI-oriented challenge tasks just about as fast as we can dream them up,&rdquo; Sam says. &ldquo;I want to emphasize, though, that we haven&rsquo;t solved language understanding yet in any satisfying way.&rdquo;  &nbsp; Read more: The 2019 AI Index report (PDF, official AI Index website). &nbsp; Read past reports here (official AI Index website).&nbsp; ####################################################Diversity in AI data: Urdu MNIST:Researchers with COMSATS University Islamabad and the National University of Ireland have put together a dataset of handwritten Urdu characters and digits, hoping to make it easier for people to train machine learning systems to automatically parse images of Urdu text.The dataset consists of handwritten examples of 10 digits and 40 characters, written by more than 900 individuals, totally more than 45,000 discreet images. &ldquo;The individuals belong to different age groups in the range of 22 to 60 years,&rdquo; they write. The writing styles vary across individuals, increasing the diversity of the dataset.  &nbsp; Get the dataset: For non-commercial uses of the dataset, you can write to the corresponding author ()&nbsp; of the paper to request access to it. (This feels like a bit of a shame &ndash; sticking the dataset on GitHub might help more people discover and use the dataset.)Why this matters: Digitization, much like globalization, has unevenly distributed benefits: places which have invested heavily in digitization have benefited by being able to turn the substance of a culture into a (typically free) digital export, which conditions the environment that other machine learning researchers work in. By digitizing things that are not currently well represented, like Urdu, we broadening the range of cultures represented in the material of AI development.  &nbsp; Read more: Pioneer dataset and automatic recognition of Urdu handwritten characters using a deep autoencoder and convolutional neural network (Arxiv). ####################################################What are the most popular machine learning frameworks used on Kaggle?&hellip;Where tried&amp;tested beats new and flashy&hellip;Kaggle, a platform for algorithmic challenges and development, has released the results of a survey trying to identify the most popular machine learning tools used by developers on the service. These statistics have a pretty significant signal in them, because the frameworks used on kaggle are typically being used to solve real-world tasks or challenges, so popularity here may correlate to practical utility as well. The five most popular frameworks in 2019:&ndash; Scikit-learn&ndash; TensorFlow&ndash; Keras&ndash; RandomForest&ndash; Xgboost(Honorable mention: PyTorch in sixth place). How does this compare to 2018? There hasn&rsquo;t been huge change; in 2018, the popular tools were: Scikit-learn, TensorFlow, Keras, RandomForest, and Caret (with PyTorch in sixth place again). Why this matters: Tools define the scope of what people can build, and any tool also imparts some of the ideology used to construct it; the diversity of today&rsquo;s programming languages typically reflect strong quasi-political preferences on the part of their core developers (compare the utterly restrained &lsquo;Rust&rsquo; language to the more expressive happy-to-let-you-step-on-a-rake coding style inherent to Python, for instance). As AI influences more and more of society, it&rsquo;ll be valuable to track which tools are popular and which &ndash; such as TensorFlow, Keras, and PyTorch &ndash; are predominantly developed by the private sector.  &nbsp; Read more: Most popular machine learning frameworks, 2019 (Kaggle). ####################################################Digital phrenology and dangerous datasets: Gait identification:&hellip;Can we spot a liar from their walk &ndash; and should we even try to?&hellip;Back in the 19th century a load of intellectuals thought a decent way to talk about differences between humans was by making arbitrary judgements about their mental character by analyzing their physical appearance, ranging from the color of their skin to the dimensions of their skull. This was a bad idea. Now, that same approach to science has returned at-scale with the advent of machine learning technologies, where researchers are developing classification systems based on similarly wobbly scientific assumptions. The Liar&rsquo;s Walk: New research from the University of North Carolina and the University of Maryland tries to train a machine learning classifier to spot deceptive people by the gait of their walk. The research is worth reading about in part because of how it seems to ignore the manifold ethical implications of developing such a system, and also barely interrogates its own underlying premise (that it&rsquo;s possible to look at someone&rsquo;s gait and work out if they&rsquo;re being deceptive or not). The researchers say such classifiers could be used for public safety in places like train stations and airports. That may well be true, but the research would need to actually work for this to be the case &ndash; and I&rsquo;m not sure it does. Garbage (data) in and garbage (data) out: Here, the researchers commit a cardinal sin of machine learning research: they make a really crappy dataset and base their research project on this. Specifically, the researchers recruited 88 participants from a university campus, then had the participants walk around in natural and deceptive ways around the campus. They then trained a classifier to ID deceptive versus honest walks, obtaining an &ldquo;accuracy&rdquo; of 93.4%&nbsp; on classifying people&rsquo;s movements. But this accuracy figure is an illusion &ndash; really, given the wobbly ground on which this paper is based. V1 versus V2: I publicized this paper on Twitter a few days prior to this issue going out; since then, the authors have updated the paper to a &lsquo;v2&rsquo; version, which includes a lengthier discussion of limitations and inherent issues with the approach at the end &ndash; this feels like an improvement, though I&rsquo;m still generally uneasy about the way they&rsquo;ve contextualized this research. However, it&rsquo;s crucial that as a community we note when people appear to update in response to criticism, and I&rsquo;m hopeful this is the…