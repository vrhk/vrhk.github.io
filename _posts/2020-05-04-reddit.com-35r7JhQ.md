---

layout: post
category: threads
title: "[Discussion] Why does everyone train on an even number of GPUs?"
date: 2020-05-04 05:27:29
link: https://vrhk.co/35r7JhQ
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Sorry if this is a noob question, but in every single research paper, blog post, or forum I read, when it gets to training, they always say that..."

---

### [Discussion] Why does everyone train on an even number of GPUs?

Sorry if this is a noob question, but in every single research paper, blog post, or forum I read, when it gets to training, they always say that...