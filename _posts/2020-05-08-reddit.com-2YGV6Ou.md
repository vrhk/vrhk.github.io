---

layout: post
category: threads
title: "What are your thoughts on Batch Normalization and Dropout between LSTM (or GRU) layers? (i.e. for stacked LSTM/GRU either uni or bidirectional) [discussion]"
date: 2020-05-08 20:28:40
link: https://vrhk.co/2YGV6Ou
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "I know there are papers that show effectiveness of Recurrent Batch Norm ([<https://arxiv.org/abs/1603.09025>](<https://arxiv.org/abs/1603.09025>)) and..."

---

### What are your thoughts on Batch Normalization and Dropout between LSTM (or GRU) layers? (i.e. for stacked LSTM/GRU either uni or bidirectional) [discussion]

I know there are papers that show effectiveness of Recurrent Batch Norm ([<https://arxiv.org/abs/1603.09025>](<https://arxiv.org/abs/1603.09025>)) and...