---

layout: post
category: engineering
title: "Pretraining BERT with Layer-wise Adaptive Learning Rates"
date: 2019-12-05 18:55:26
link: https://vrhk.co/2qo9Djg
image: https://devblogs.nvidia.com/wp-content/uploads/2019/12/BERT-Phase1-pretraining.png
domain: devblogs.nvidia.com
author: "NVIDIA Developer Blog"
icon: https://devblogs.nvidia.com/favicon.ico
excerpt: "Training with larger batches is a straightforward way to scale training of deep neural networks to larger numbers of accelerators and reduce the training time. However, as the batch size increases, numerical instability can appear in the training process. The purpose of this blog is to provide an overview of one class of solutions to …"

---

### Pretraining BERT with Layer-wise Adaptive Learning Rates | NVIDIA Developer Blog

Training with larger batches is a straightforward way to scale training of deep neural networks to larger numbers of accelerators and reduce the training time. However, as the batch size increases, numerical instability can appear in the training process. The purpose of this blog is to provide an overview of one class of solutions to …