---

layout: post
category: product
title: "Import AI 191: Google uses AI to design better chips; how half a million Euros relates to AGI; and how you can help form an African NLP community"
date: 2020-03-30 18:36:36
link: https://vrhk.co/3bIqW0F
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s0.wp.com/i/webclip.png
excerpt: "Nice machine translation system you&rsquo;ve got there &ndash; think it can handle XTREME?&hellip;New benchmark tests transfer across 40 languages across 12 language families&hellip;In the Hitchhiker&rsquo;s Guide to the Galaxy there&rsquo;s a technology called a &lsquo;babelfish&rsquo; &ndash; a little in-ear creature that cheerfully translates between all the languages in the universe. AI researchers have recently been building a smaller, human-scale version of this babelfish, by training large language models on fractions of the internet to aid translation between languages. Now, researchers with Carnegie Mellon University, DeepMind, and Google Research have built XTREME, a benchmark for testing out how advanced our translation systems are becoming, and identifying where they fail.XTREME, short for the Cross-lingual TRansfer Evaluation of Multilingual Encoders benchmark, covers 40 diverse languages across 12 language families. XTREME tests out zero-shot cross-lingual transfer, so it provides training data in English, but doesn&rsquo;t provide training data in the target languages. One of the main things XTREME will help us test is how well we can build robust multi-lingual models via massive internet-scale pre-training (e.g., one of the baselines they use is mBERT, a multilingual version of BERT), and where these models display good generalization and where they fail. The benchmark includes nine tasks that require reasoning about different levels of syntax or semantics in these different languages.Designing a &lsquo;just hard enough&rsquo; benchmark: XTREME is built to be challenging, so contemporary systems&rsquo; &ldquo;cross-language performance falls short of human performance&rdquo;. At the same time, it has been built so tasks can be trainable on a single GPU for less than a day, which should make it easier for more people to conduct research against XTREME. XTREME implements nine tasks across four categories &ndash; classification, structured prediction, question-answering, and retrieval. Specific tasks include: XNLI, PAWS-X, POS, NER, XQuAD, MLQA, TyDiQA-GoldP, BUCC, and Tatoeba. &nbsp; XTREME tests transfer across 40 languages: Afrikaans, Arabic, Basque, Bengali, Bulgarian, Burmese, Dutch, English, Estonian, Finnish, French, Georgian, German, Greek, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Javanese, Kazakh, Korean, Malay, Malayalam, Mandarin, Marathi, Persian, Portuguese, Russian, Spanish, Swahili, Tagalog, Tamil, Telugu, Thai, Turkish, Urdu, Vietnamese, Yoruba. What is hard and what is easy? Somewhat unsurprisingly, the researchers find that they see generally higher performance on Indo-European languages and lower performance for other language families, likely due to a combination of the more extreme differences between these languages, and also underlying data availability.Why this matters: XTREME is a challenging, multi-task benchmark that tries to test out the generalization capabilities of large language models. In many ways, XTREME is a symptom of underlying advances in language processing &ndash; it exists, because we&rsquo;ve started to saturate performance on many single-language or single-task benchmarks, and we&rsquo;re now at the stage where we&rsquo;re trying to holistically analyze massive models via multi-task training. I expect benchmarks like this will help us develop a sense for the limits of generalization of current techniques, and will highlight areas where more data might lead to better inter-language translation capabilities.  &nbsp; Read more: XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization (arXiv).####################################################Google uses RL to figure out how to allocate hardware to machine learning models: &hellip;Bin packing? More like chip packing!&hellip;In machine learning workloads, you have what&rsquo;s called a computational graph, which describes a set of operations and the relationships between them. When deploying large ML systems, you need to perform something called Placement Optimization to map the nodes of the graph onto resources in accordance with an objective, like minimizing the time it takes to train a system, or run inference on the system.  &nbsp; Research from Google Brain shows how we might be able to use reinforcement learning approaches to develop AI systems that do a range of useful things, like learning how to map different computational graphs to different hardware resources to satisfy an objective, or how to map chip components onto a chip canvas, or how to map out different parts of FPGAs.RL for bin-packing: The authors show how you can frame placement as a reinforcement learning problem, without needing to boil the ocean: &ldquo;instead of finding the absolute best placement, one can train a policy that generates a probability distribution of nodes to placement locations such that it maximizes the expected reward generated by those placement&rdquo;.&nbsp; Interestingly, the paper doesn&rsquo;t include many specific discussions of how well this works &ndash; my assumption is that&rsquo;s because Google is actively testing this out, and has emitted this paper to give some tips and tricks to others, but doesn&rsquo;t want to reveal proprietary information. I could be wrong, though. Tips &amp; tricks: If you want to train AI systems to help allocate hardware sensibly, then the authors have some tips. These include: &ndash; Reward function: Ensure your reward function is fast to evaluate (think: sub-seconds); ensure your reward function is able to reflect reality (e.g., &ldquo;for TensorFlow placement, the proxy reward could be a composite function of total memory per device, number of inter-device (and therefore expensive) edges induced by the placement, imbalance of computation placed on each device&rdquo;). &ndash; Constraints: RL systems that do this kind of work need to be sensitive to constraints. For example, &ldquo;in device placement, the memory footprint of the nodes placed onto a single device should not exceed the memory limit of that device&rdquo;. You can simply penalize the policy to discourage it from learning this, but that doesn&rsquo;t make it easy for it to learn how far away it was from getting stuff right. A different approach is to come up with policies that can only generate feasible placements, though this requires more human oversight. &ndash; Representations: Figuring out which sorts of representations to use is, as most AI researchers know, half the challenge in a problem. It&rsquo;s no different here. Some promising ways of getting good representations for this sort of problem include using graph convolutional neural networks, the researchers write. Why this matters: We&rsquo;re starting to use machine learning to optimize the infrastructure of computation itself. That&rsquo;s pretty cool! It gets even cooler when you zoom out: in research papers published in recent years Google has gone from the abstract level of optimizing data center power usage, to optimizing things like how it builds and indexes items in databases, to figuring out how to place chip components themselves, and more (see: its work on C++ server memory allocation). ML is burrowing deeper and deeper into the technical stacks of large organizations, leading to fractal-esque levels of self-optimization from the large (data centers!) to the tiny (placement of one type of processing core on one chip sitting on one motherboard in one server inside a rack inside a data center). How far will this go? And how might companies that implement this stuff diverge in capabilities and cadence of execution from ones which don&rsquo;t? &nbsp; Read more: Placement Optimization with Deep Reinforcement Learning (arXiv). ####################################################Introducing the new Hutter Prize: &euro;500,000 for better compression:&hellip;And why people think compression gets us closer to AGI&hellip;For many yea…"

---

### Import AI 191: Google uses AI to design better chips; how half a million Euros relates to AGI; and how you can help form an African NLP community

Nice machine translation system you&rsquo;ve got there &ndash; think it can handle XTREME?&hellip;New benchmark tests transfer across 40 languages across 12 language families&hellip;In the Hitchhiker&rsquo;s Guide to the Galaxy there&rsquo;s a technology called a &lsquo;babelfish&rsquo; &ndash; a little in-ear creature that cheerfully translates between all the languages in the universe. AI researchers have recently been building a smaller, human-scale version of this babelfish, by training large language models on fractions of the internet to aid translation between languages. Now, researchers with Carnegie Mellon University, DeepMind, and Google Research have built XTREME, a benchmark for testing out how advanced our translation systems are becoming, and identifying where they fail.XTREME, short for the Cross-lingual TRansfer Evaluation of Multilingual Encoders benchmark, covers 40 diverse languages across 12 language families. XTREME tests out zero-shot cross-lingual transfer, so it provides training data in English, but doesn&rsquo;t provide training data in the target languages. One of the main things XTREME will help us test is how well we can build robust multi-lingual models via massive internet-scale pre-training (e.g., one of the baselines they use is mBERT, a multilingual version of BERT), and where these models display good generalization and where they fail. The benchmark includes nine tasks that require reasoning about different levels of syntax or semantics in these different languages.Designing a &lsquo;just hard enough&rsquo; benchmark: XTREME is built to be challenging, so contemporary systems&rsquo; &ldquo;cross-language performance falls short of human performance&rdquo;. At the same time, it has been built so tasks can be trainable on a single GPU for less than a day, which should make it easier for more people to conduct research against XTREME. XTREME implements nine tasks across four categories &ndash; classification, structured prediction, question-answering, and retrieval. Specific tasks include: XNLI, PAWS-X, POS, NER, XQuAD, MLQA, TyDiQA-GoldP, BUCC, and Tatoeba. &nbsp; XTREME tests transfer across 40 languages: Afrikaans, Arabic, Basque, Bengali, Bulgarian, Burmese, Dutch, English, Estonian, Finnish, French, Georgian, German, Greek, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Javanese, Kazakh, Korean, Malay, Malayalam, Mandarin, Marathi, Persian, Portuguese, Russian, Spanish, Swahili, Tagalog, Tamil, Telugu, Thai, Turkish, Urdu, Vietnamese, Yoruba. What is hard and what is easy? Somewhat unsurprisingly, the researchers find that they see generally higher performance on Indo-European languages and lower performance for other language families, likely due to a combination of the more extreme differences between these languages, and also underlying data availability.Why this matters: XTREME is a challenging, multi-task benchmark that tries to test out the generalization capabilities of large language models. In many ways, XTREME is a symptom of underlying advances in language processing &ndash; it exists, because we&rsquo;ve started to saturate performance on many single-language or single-task benchmarks, and we&rsquo;re now at the stage where we&rsquo;re trying to holistically analyze massive models via multi-task training. I expect benchmarks like this will help us develop a sense for the limits of generalization of current techniques, and will highlight areas where more data might lead to better inter-language translation capabilities.  &nbsp; Read more: XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization (arXiv).####################################################Google uses RL to figure out how to allocate hardware to machine learning models: &hellip;Bin packing? More like chip packing!&hellip;In machine learning workloads, you have what&rsquo;s called a computational graph, which describes a set of operations and the relationships between them. When deploying large ML systems, you need to perform something called Placement Optimization to map the nodes of the graph onto resources in accordance with an objective, like minimizing the time it takes to train a system, or run inference on the system.  &nbsp; Research from Google Brain shows how we might be able to use reinforcement learning approaches to develop AI systems that do a range of useful things, like learning how to map different computational graphs to different hardware resources to satisfy an objective, or how to map chip components onto a chip canvas, or how to map out different parts of FPGAs.RL for bin-packing: The authors show how you can frame placement as a reinforcement learning problem, without needing to boil the ocean: &ldquo;instead of finding the absolute best placement, one can train a policy that generates a probability distribution of nodes to placement locations such that it maximizes the expected reward generated by those placement&rdquo;.&nbsp; Interestingly, the paper doesn&rsquo;t include many specific discussions of how well this works &ndash; my assumption is that&rsquo;s because Google is actively testing this out, and has emitted this paper to give some tips and tricks to others, but doesn&rsquo;t want to reveal proprietary information. I could be wrong, though. Tips &amp; tricks: If you want to train AI systems to help allocate hardware sensibly, then the authors have some tips. These include: &ndash; Reward function: Ensure your reward function is fast to evaluate (think: sub-seconds); ensure your reward function is able to reflect reality (e.g., &ldquo;for TensorFlow placement, the proxy reward could be a composite function of total memory per device, number of inter-device (and therefore expensive) edges induced by the placement, imbalance of computation placed on each device&rdquo;). &ndash; Constraints: RL systems that do this kind of work need to be sensitive to constraints. For example, &ldquo;in device placement, the memory footprint of the nodes placed onto a single device should not exceed the memory limit of that device&rdquo;. You can simply penalize the policy to discourage it from learning this, but that doesn&rsquo;t make it easy for it to learn how far away it was from getting stuff right. A different approach is to come up with policies that can only generate feasible placements, though this requires more human oversight. &ndash; Representations: Figuring out which sorts of representations to use is, as most AI researchers know, half the challenge in a problem. It&rsquo;s no different here. Some promising ways of getting good representations for this sort of problem include using graph convolutional neural networks, the researchers write. Why this matters: We&rsquo;re starting to use machine learning to optimize the infrastructure of computation itself. That&rsquo;s pretty cool! It gets even cooler when you zoom out: in research papers published in recent years Google has gone from the abstract level of optimizing data center power usage, to optimizing things like how it builds and indexes items in databases, to figuring out how to place chip components themselves, and more (see: its work on C++ server memory allocation). ML is burrowing deeper and deeper into the technical stacks of large organizations, leading to fractal-esque levels of self-optimization from the large (data centers!) to the tiny (placement of one type of processing core on one chip sitting on one motherboard in one server inside a rack inside a data center). How far will this go? And how might companies that implement this stuff diverge in capabilities and cadence of execution from ones which don&rsquo;t? &nbsp; Read more: Placement Optimization with Deep Reinforcement Learning (arXiv). ####################################################Introducing the new Hutter Prize: &euro;500,000 for better compression:&hellip;And why people think compression gets us closer to AGI&hellip;For many yea…