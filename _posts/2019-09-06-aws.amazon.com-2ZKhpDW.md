---

layout: post
category: engineering
title: "Performing batch inference with TensorFlow Serving in Amazon SageMaker"
date: 2019-09-06 19:01:27
link: https://vrhk.co/2ZKhpDW
image: https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2019/08/21/inference-with-tensorflow-1.gif
domain: aws.amazon.com
author: "Amazon Web Services"
icon: http://a0.awsstatic.com/main/images/site/touch-icon-iphone-114-smile.png
excerpt: "After you’ve trained and exported a TensorFlow model, you can use Amazon SageMaker to perform inferences using your model. You can either: Deploy your model to an endpoint to obtain real-time inferences from your model. Use batch transform to obtain inferences on an entire dataset stored in Amazon S3. In the case of batch transform, […]"

---

### Performing batch inference with TensorFlow Serving in Amazon SageMaker | Amazon Web Services

After you’ve trained and exported a TensorFlow model, you can use Amazon SageMaker to perform inferences using your model. You can either: Deploy your model to an endpoint to obtain real-time inferences from your model. Use batch transform to obtain inferences on an entire dataset stored in Amazon S3. In the case of batch transform, […]