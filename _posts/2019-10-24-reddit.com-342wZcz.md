---

layout: post
category: threads
title: "[D] Layer Complexity of Recurrent NNs in the Transformer Paper"
date: 2019-10-24 14:22:27
link: https://vrhk.co/342wZcz
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "[<https://arxiv.org/pdf/1706.03762.pdf>](<https://arxiv.org/pdf/1706.03762.pdf>) Table 1 of this paper says the layer complexity of self-attention NNs..."

---

### [D] Layer Complexity of Recurrent NNs in the Transformer Paper

[<https://arxiv.org/pdf/1706.03762.pdf>](<https://arxiv.org/pdf/1706.03762.pdf>) Table 1 of this paper says the layer complexity of self-attention NNs...