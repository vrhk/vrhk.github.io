---

layout: post
category: threads
title: "[D] Paper Explained - Linformer: Self-Attention with Linear Complexity (Full Video Analysis)"
date: 2020-06-11 14:17:38
link: https://vrhk.co/2XSQK68
image: https://external-preview.redd.it/YgtTpWfTJS7eMBvyWMyTXWUUk2LkRR8_6uQIqV5sH5c.jpg?width=480&height=251.308900524&auto=webp&crop=480:251.308900524,smart&s=4b140477dd0a9a043693363b81957632c237be2f
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "[<https://youtu.be/-\_2AF9Lhweo>](<https://youtu.be/-_2AF9Lhweo>) Transformers are notoriously resource-intensive because their self-attention..."

---

### [D] Paper Explained - Linformer: Self-Attention with Linear Complexity (Full Video Analysis)

[<https://youtu.be/-\_2AF9Lhweo>](<https://youtu.be/-_2AF9Lhweo>) Transformers are notoriously resource-intensive because their self-attention...