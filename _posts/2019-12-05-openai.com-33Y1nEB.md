---

layout: post
category: research
title: "Deep Double Descent"
date: 2019-12-05 17:38:25
link: https://vrhk.co/33Y1nEB
image: https://openai.com/content/images/2019/12/Frame-1--3-.png
domain: openai.com
author: "OpenAI"
icon: https://openai.com/assets/images/favicon.png
excerpt: "Contrary to conventional wisdom, we find that the performance of CNNs, ResNets, and transformers is non-monotonic: it first improves, then gets worse, and then improves again with increasing model size, data size, or training time. This effect is often avoided through careful regularization. While this behavior appears to be fairly"

---

### Deep Double Descent

Contrary to conventional wisdom, we find that the performance of CNNs, ResNets, and transformers is non-monotonic: it first improves, then gets worse, and then improves again with increasing model size, data size, or training time. This effect is often avoided through careful regularization. While this behavior appears to be fairly