---

layout: post
category: product
title: "Import AI 211: In AI dogfight, Machines: 5, Humans: 0; Baidu releases a YOLO variant; and the Bitter Lesson and video action recognition"
date: 2020-08-24 17:56:32
link: https://vrhk.co/2EsOwmK
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Which is the best system for video action recognition? Simple 2D convnets, says survey:&hellip; Richard Sutton&rsquo;s &lsquo;bitter lesson&rsquo; strikes again&hellip;Researchers with MIT have analyzed the performance of fourteen different models used for video action recognition &ndash; correctly labeling something in a video, a generically useful AI capability. The results show that simple techniques tend to beat complex ones. Specifically, the researchers benchmark a range of 2D convolutional networks (C2Ds) against temporal segment networks (TSNs), Long-Term Recurrent Convolutional Neural Nets (LCRNs) and Temporal Shift Modules (TSMs). They find the simple stuff &ndash; 2D convnets &ndash; perform best. The bitter lesson results: Convolutional net models &ldquo;significantly outperform&rdquo; the other models they test. Specifically, the Inception-ResNet-v2, ResNet50, DenseNet201, and MobileNetv2 are all top performers. These results also highlight some of the ideas in Sutton&rsquo;s &lsquo;bitter lesson&lsquo; essay &ndash; namely that simpler things that scale better tend to beat the smart stuff. &ldquo;2D approaches can yield results comparable to their more complex 3D counterparts, and model depth, rather than input feature scale, is the critical component to an architecture&rsquo;s ability to extract a video&rsquo;s semantic action information,&rdquo; they write.  &nbsp; Read more: Accuracy and Performance Comparison of Video Action Recognition Approaches (arXiv).
###################################################Free education resources &ndash;  releases a ton of stuff:&hellip;What has code, tutorials, and reference guides, costs zero bucks, and is made by nice people? This stuff!&hellip;The terrifically nice people at  have released a rewrite of their fastai framework, bringing with it new libraries, as well as an educational course &ndash; practical deep learning for coders &ndash; as well as an O&rsquo;Reilly book and a &lsquo;Practical Data Ethics&rsquo; course. Why this matters: fastai is a library built around the idea that the best way to help people learn technology is to make it easy for them to build high performance stuff while learning about things. &ldquo;Fastai is organized around two main design goals: to be approachable and rapidly productive, while also being deeply hackable and configurable,&rdquo; they write.  &nbsp; Read more:  releases new deep learning course, four libraries, and 600-page book ( website).
###################################################Amazon Echo + School = Uh-oh:Mark Riedl, an AI professor, says on the first day of COVId-induced remote school for their 1st grader, the teacher read a story about a character named Echo the Owl. &ldquo;It kept triggering peoples&rsquo; Amazon Echos,&rdquo; Mark writes. &ldquo;One of the echos asked if anyone wanted to buy a bible&rdquo;. &nbsp; &nbsp; Read the tweet here (Mark Riedl, twitter).###################################################ICLR develops a code of conduct for its research community:&hellip;Towards an AI hippocratic oath&hellip;ICLR, a popular and prestigious AI conference, has developed a code of ethics that it would like people who submit papers to follow. The code of ethics has the following core tenets researchers should follow:&ndash; Contribute to society and to human well-being. &ndash; Uphold high standards of scientific excellence. &ndash; Avoid harm.&ndash; Be honest, trustworthy and transparent.&ndash; Be fair and take action not to discriminate. &ndash; Respect the work required to produce new ideas and artefacts. &ndash; Respect privacy.&ndash; Honour confidentiality.&nbsp;
Ethics are all well and good &ndash; but how do you enforce them? Right now, the code doesn&rsquo;t seem like it&rsquo;ll be enforced, though ICLR does write &ldquo;The Code [sic] should not be seen as prescriptive but as a set of principles to guide ethical, responsible research.&rdquo; In addition, it says people that submit to ICLR should familiarize themselves with the code and use it as &ldquo;one source of ethical considerations&rdquo;. Why this matters: Machine learning is moving from a frontier part of research rife with experimentation to a more mainstream part of academia (and, alongside this, daily life). It makes sense to try and develop common ethical standards for AI researchers. ICLR&rsquo;s move follows top AI conference NeurIPS requesting researchers write detailed &lsquo;broader impacts&rsquo; segments of their papers (Import AI 189) and computer science researchers such as Brent Hecht arguing researchers should try to discuss the negative impacts of their work (Import AI 105). &nbsp; Read more: ICLR Code of Ethics (official ICLR website). ###################################################AI progress is getting faster, says one Googler:Alex Irpan, a software engineer at Google and part-time AI blogger, has written a post saying their AI timelines have sped up, due to recent progress in the field. In particular, Alex thinks it&rsquo;s now somewhat more tractable to think about building AGI than it was in the past. AGI &ndash; from implausible to plausible: &ldquo;For machine learning, the natural version of this question is, &ldquo;what problems need to be solved to get to artificial general intelligence?&rdquo; What waypoints do you expect the field to hit on the road to get there, and how much uncertainty is there about the path between those waypoints?,&rdquo; Irpan writes. &ldquo;I feel like more of those waypoints are coming into focus. If you asked 2015-me how we&rsquo;d build AGI, I&rsquo;d tell you I have no earthly idea. I didn&rsquo;t feel like we had meaningful in-roads on any of the challenges I&rsquo;d associate with human-level intelligence. If you ask 2020-me how we&rsquo;d build AGI, I still see a lot of gaps, but I have some idea how it could happen, assuming you get lucky,&rdquo; Irpan writes. &nbsp; Read more: My AI Timelines Have Sped Up (Alex Irpan, blog). ###################################################Baidu publishes high-performance video object detector, Yolo-PP:&hellip;After YOLO&rsquo;s creator swore off developing the tech, others continued&hellip;Baidu has published YOLO-PP, an object detection system. YOLO-PP isn&rsquo;t the most accurate system in the world, but it does run at an extremely high FPS with reasonable accuracy. The authors have released the code on GitHub.
What does YOLO get used for? YOLO is a fairly generic object detection network designed to be run over streams of imagery, so it can be used for things like tracking pedestrians, labeling objects in factories, annotating satellite imagery, and more. (Its notable that a team from Baidu is releasing a YOLO model, as one can surmise this is because Baidu uses this stuff internally. In potentially related news, a Baidu team recently won the multi-class multi-movement vehicle counting and traffic anomaly detection components of a smart city AI challengeWhat they did: YOLO is, by design, built for real world object detection, so YOLO models have grown increasingly baroque over time as developers build in various technical tricks to further improve performance. The Baidu authors state this themselves: &ldquo;This paper is not intended to introduce a novel object detector. It is more like a receipt, which tell you how to build a better detector step by step.&rdquo; Their evidence is that their PP-YOLO model gets a score of 45.2% mAP on the COCO dataset while running inference faster than YOLOv4.  &nbsp; Specific tricks: Some of the tricks they use include a larger batch size, spatial pyramid pooling, using high-performance pre-trained models, and more. YOLO&rsquo;s strange, dramatic history: PP-YOLO is an extension of YOLO-v3 and in benchmark tests has better performance than YOLO-v4 (a successor to YOLO-v3 developed by someone else). Joseph Redmon, the original YOLO developer (see: YOLOv3 release im Import AI 88), stopped doing computer vision research o…"

---

### Import AI 211: In AI dogfight, Machines: 5, Humans: 0; Baidu releases a YOLO variant; and the Bitter Lesson and video action recognition

Which is the best system for video action recognition? Simple 2D convnets, says survey:&hellip; Richard Sutton&rsquo;s &lsquo;bitter lesson&rsquo; strikes again&hellip;Researchers with MIT have analyzed the performance of fourteen different models used for video action recognition &ndash; correctly labeling something in a video, a generically useful AI capability. The results show that simple techniques tend to beat complex ones. Specifically, the researchers benchmark a range of 2D convolutional networks (C2Ds) against temporal segment networks (TSNs), Long-Term Recurrent Convolutional Neural Nets (LCRNs) and Temporal Shift Modules (TSMs). They find the simple stuff &ndash; 2D convnets &ndash; perform best. The bitter lesson results: Convolutional net models &ldquo;significantly outperform&rdquo; the other models they test. Specifically, the Inception-ResNet-v2, ResNet50, DenseNet201, and MobileNetv2 are all top performers. These results also highlight some of the ideas in Sutton&rsquo;s &lsquo;bitter lesson&lsquo; essay &ndash; namely that simpler things that scale better tend to beat the smart stuff. &ldquo;2D approaches can yield results comparable to their more complex 3D counterparts, and model depth, rather than input feature scale, is the critical component to an architecture&rsquo;s ability to extract a video&rsquo;s semantic action information,&rdquo; they write.  &nbsp; Read more: Accuracy and Performance Comparison of Video Action Recognition Approaches (arXiv).
###################################################Free education resources &ndash;  releases a ton of stuff:&hellip;What has code, tutorials, and reference guides, costs zero bucks, and is made by nice people? This stuff!&hellip;The terrifically nice people at  have released a rewrite of their fastai framework, bringing with it new libraries, as well as an educational course &ndash; practical deep learning for coders &ndash; as well as an O&rsquo;Reilly book and a &lsquo;Practical Data Ethics&rsquo; course. Why this matters: fastai is a library built around the idea that the best way to help people learn technology is to make it easy for them to build high performance stuff while learning about things. &ldquo;Fastai is organized around two main design goals: to be approachable and rapidly productive, while also being deeply hackable and configurable,&rdquo; they write.  &nbsp; Read more:  releases new deep learning course, four libraries, and 600-page book ( website).
###################################################Amazon Echo + School = Uh-oh:Mark Riedl, an AI professor, says on the first day of COVId-induced remote school for their 1st grader, the teacher read a story about a character named Echo the Owl. &ldquo;It kept triggering peoples&rsquo; Amazon Echos,&rdquo; Mark writes. &ldquo;One of the echos asked if anyone wanted to buy a bible&rdquo;. &nbsp; &nbsp; Read the tweet here (Mark Riedl, twitter).###################################################ICLR develops a code of conduct for its research community:&hellip;Towards an AI hippocratic oath&hellip;ICLR, a popular and prestigious AI conference, has developed a code of ethics that it would like people who submit papers to follow. The code of ethics has the following core tenets researchers should follow:&ndash; Contribute to society and to human well-being. &ndash; Uphold high standards of scientific excellence. &ndash; Avoid harm.&ndash; Be honest, trustworthy and transparent.&ndash; Be fair and take action not to discriminate. &ndash; Respect the work required to produce new ideas and artefacts. &ndash; Respect privacy.&ndash; Honour confidentiality.&nbsp;
Ethics are all well and good &ndash; but how do you enforce them? Right now, the code doesn&rsquo;t seem like it&rsquo;ll be enforced, though ICLR does write &ldquo;The Code [sic] should not be seen as prescriptive but as a set of principles to guide ethical, responsible research.&rdquo; In addition, it says people that submit to ICLR should familiarize themselves with the code and use it as &ldquo;one source of ethical considerations&rdquo;. Why this matters: Machine learning is moving from a frontier part of research rife with experimentation to a more mainstream part of academia (and, alongside this, daily life). It makes sense to try and develop common ethical standards for AI researchers. ICLR&rsquo;s move follows top AI conference NeurIPS requesting researchers write detailed &lsquo;broader impacts&rsquo; segments of their papers (Import AI 189) and computer science researchers such as Brent Hecht arguing researchers should try to discuss the negative impacts of their work (Import AI 105). &nbsp; Read more: ICLR Code of Ethics (official ICLR website). ###################################################AI progress is getting faster, says one Googler:Alex Irpan, a software engineer at Google and part-time AI blogger, has written a post saying their AI timelines have sped up, due to recent progress in the field. In particular, Alex thinks it&rsquo;s now somewhat more tractable to think about building AGI than it was in the past. AGI &ndash; from implausible to plausible: &ldquo;For machine learning, the natural version of this question is, &ldquo;what problems need to be solved to get to artificial general intelligence?&rdquo; What waypoints do you expect the field to hit on the road to get there, and how much uncertainty is there about the path between those waypoints?,&rdquo; Irpan writes. &ldquo;I feel like more of those waypoints are coming into focus. If you asked 2015-me how we&rsquo;d build AGI, I&rsquo;d tell you I have no earthly idea. I didn&rsquo;t feel like we had meaningful in-roads on any of the challenges I&rsquo;d associate with human-level intelligence. If you ask 2020-me how we&rsquo;d build AGI, I still see a lot of gaps, but I have some idea how it could happen, assuming you get lucky,&rdquo; Irpan writes. &nbsp; Read more: My AI Timelines Have Sped Up (Alex Irpan, blog). ###################################################Baidu publishes high-performance video object detector, Yolo-PP:&hellip;After YOLO&rsquo;s creator swore off developing the tech, others continued&hellip;Baidu has published YOLO-PP, an object detection system. YOLO-PP isn&rsquo;t the most accurate system in the world, but it does run at an extremely high FPS with reasonable accuracy. The authors have released the code on GitHub.
What does YOLO get used for? YOLO is a fairly generic object detection network designed to be run over streams of imagery, so it can be used for things like tracking pedestrians, labeling objects in factories, annotating satellite imagery, and more. (Its notable that a team from Baidu is releasing a YOLO model, as one can surmise this is because Baidu uses this stuff internally. In potentially related news, a Baidu team recently won the multi-class multi-movement vehicle counting and traffic anomaly detection components of a smart city AI challengeWhat they did: YOLO is, by design, built for real world object detection, so YOLO models have grown increasingly baroque over time as developers build in various technical tricks to further improve performance. The Baidu authors state this themselves: &ldquo;This paper is not intended to introduce a novel object detector. It is more like a receipt, which tell you how to build a better detector step by step.&rdquo; Their evidence is that their PP-YOLO model gets a score of 45.2% mAP on the COCO dataset while running inference faster than YOLOv4.  &nbsp; Specific tricks: Some of the tricks they use include a larger batch size, spatial pyramid pooling, using high-performance pre-trained models, and more. YOLO&rsquo;s strange, dramatic history: PP-YOLO is an extension of YOLO-v3 and in benchmark tests has better performance than YOLO-v4 (a successor to YOLO-v3 developed by someone else). Joseph Redmon, the original YOLO developer (see: YOLOv3 release im Import AI 88), stopped doing computer vision research o…