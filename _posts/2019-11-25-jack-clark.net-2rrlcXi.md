---

layout: post
category: product
title: "Import AI 174: Model cards for responsible AI development; how Alexa learns from trial and error; BERT meets Bing"
date: 2019-11-25 12:41:31
link: https://vrhk.co/2rrlcXi
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Amazon uses reinforcement learning to teach Alexa the art of conversation&hellip;Alexa + 180,000 unique dialogues + DQN =Learning conversation through trial and error&hellip;Amazon wants more people to talk to its Alexa AI agent, so Amazon is using reinforcement learning to teach its agent to be better at conversation. In recent tests, Amazon shows that agents trained via reinforcement learning have better performance than those which use purely rule-based systems, laying the ground for a future where personal assistants continuously evolve and adapt to their users.
&nbsp; &nbsp;What Amazon did: For this project, Amazon first constructed a rule-based agent. This agent tries to figure out what actions to select based on user intent at any point in time, where actions could be offering particular &lsquo;skills&rsquo; (eg &lsquo;set an alarm&rsquo;) to the user, providing answers about a particular area of knowledge, launching a skill, and so on. To develop the system, the researchers first deployed a rule-based system to Amazon Alexa users, gathering 180,000 unique dialogues. They use these dialogues to build a user simulator which can then interact with another system by reinforcement learning &ndash; a useful feature, given that it&rsquo;s much faster to use a simulator to train RL systems, than to use real data which needs to be collected from the real world.&nbsp;
&nbsp; &nbsp;How well did it work? To test how well their system worked, Amazon did a real world test. Their baseline system recommended up to five skills based on popularity, then allowed the user to accept or reject the recommendation &ndash; this got a success rate of 46.42%. They then tested out their rule-based and RL-based systems against eachother in an A/B test; the rule-based baseline got 73.41% while the RL policy got 76.99% &ndash; this is already a statistically measurable difference, and along with this the RL policy had significantly shorter dialogues suggesting it was better at figuring out the right suggestion early on.&nbsp;
Why this matters: Soon, the world will be suffused with large, invisible machines, adjusting themselves around us to better entice and delight us. Many of the prototypes of these machines will show up in the form of the systems that underpin personal assistants like Amazon Alexa.&nbsp;&nbsp;&nbsp;Read more: Towards Personalized Dialog Policies for Conversational Skill Discovery (Arxiv).&nbsp;
####################################################
BERT-ageddon: From research into Microsoft and Google&rsquo;s search engines in under a year:&hellip;First, Google. Now Microsoft. Next: DuckDuckGo?&hellip;Microsoft has improved performance of its Bing search engine via the use of BERT, a language model that, along with systems like ULMFiT and GPT-2, has revolutionized natural language processing in recent years. &ldquo;Starting from April of this year, we used large transformer models to deliver the largest quality improvements to our Bing customers in the past year,&rdquo; Microsoft wrote in a blog post discussing the research. &ldquo;These models are now applied to every Bing search query globally making Bing results more relevant and intelligent&rdquo;.&nbsp;
&nbsp;&nbsp;&nbsp;What they did: Getting a model like BERT to support web search isn&rsquo;t easy; models like BERT are quite large and typically take a long time to sample from. Microsoft said an un-optimized version of BERT running on CPUs took 77ms to query. Microsoft reduced this to 6ms by running the model on an Azure NV6 GPU virtual machine and doing some low-level programming to optimize the model implementation. &ldquo;With these GPU optimizations, we were able to use 2000+ Azure GPU Virtual Machines across four regions to serve over 1 million BERT inferences per second worldwide,&rdquo; the company wrote.&nbsp;
&nbsp;&nbsp;&nbsp;Why this matters: BERT came out in late 2018. That&rsquo;s extremely recent! Imagine if someone came up with some prototype machinery for a factory and then six months later that prototype had been integrated into a million factories across the planet &ndash; that&rsquo;s kind of what has happened here. It highlights how rapidly AI can go from research to production and should make us think more deeply about the implications of the technologies we&rsquo;re developing.&nbsp;&nbsp;&nbsp;Read more: Bing delivers its largest improvement in search experience using Azure GPUs (Microsoft Azure blog).&nbsp;
####################################################
Spotting fake test with GPTrueorFalse:&hellip;Is that text you&rsquo;re reading made by a human or made by a machine?&hellip;In the coming years, the internet is going to fill up with text, images, and audio generated by increasingly large, powerful language models. At the same time, we can expect people to invest in building systems to detect the presence of synthetic content. To that end, a developer who goes by &lsquo;thesofakillers&rsquo; has created GPTrue or False, a browser extension that works out if text is generated or not. The extension uses OpenAI&rsquo;s GPT-2 detector model, hosted by Hugging Face.&nbsp;&nbsp;&nbsp;Read more: GPTrue or False Browser Extension (official GitHub page).&nbsp;
####################################################
Bill Gates: AI research wants to be open:&hellip;Microsoft co-founder speaks out in Beijing&hellip;Bill Gates says &ldquo;whoever has an open system will get massively ahead&rdquo; when it comes to developing national artificial intelligence capabilities, according to comments made by Gates at a Bloomberg event in Beijing this week. Gates says open research ecosystems beat closed ecosystems, and that protectionist policies can have a negative effect on technology development.&nbsp;&nbsp;&nbsp;Read more: Bill Gates Says Open Research Beats Erecting Borders in AI (Bloomberg News).&nbsp;
####################################################
MuZero means AI systems can learn the rules of games themselves:&hellip;DeepMind&rsquo;s new system wraps planning and learning into a generic model that learns Go, Chess, Shogi, Space Invaders, and more&hellip;In recent years, some types of progress in AI development have been defined by the creation of systems that can solve tasks in unprecedented ways, then the subsequent simplification and generalization of those systems. Some examples of this include the transition from AlphaGo (which included quiet a lot of world-state as input and some handwritten features and knowledge about the rules of Go) to AlphaGo Zero (which included less world state), and in translation where companies like Google have been replacing single-language-pair translation systems with a single big model that learns to translate between multiple languages at once. Now, DeepMind has announced MuZero, a single algorithm that they use to achieve state-of-the-art scores on tasks as varied as the Atari-57 corpus of games, Go, Chess, and Shogi.&nbsp;
MuZero&rsquo;s trick: The core of MuZero&rsquo;s success is that it combines tree search with a learned model. This means that MuZero can take in observations via standard deep learning components, then transforms those observations into a hidden state which it uses to plan out its next moves, simulating the strategic space of its environment automatically. This makes it easy for the agent to learn a model of the world it is acting in and to figure out how to plan appropriately.&nbsp;&nbsp;&nbsp;&ldquo;There is no direct constraint or requirement for the hidden state to capture all information necessary to reconstruct the original observation, drastically reducing the amount of information the model has to maintain and predict; nor is there any requirement for the hidden state to match the unknown, true state of the environment; nor any other constraints on the semantics of state,&rdquo; the researchers write. &ldquo;Instead, the hidden states are free to represent state in whatever way is relevant to predicting current a…"

---

### Import AI 174: Model cards for responsible AI development; how Alexa learns from trial and error; BERT meets Bing

Amazon uses reinforcement learning to teach Alexa the art of conversation&hellip;Alexa + 180,000 unique dialogues + DQN =Learning conversation through trial and error&hellip;Amazon wants more people to talk to its Alexa AI agent, so Amazon is using reinforcement learning to teach its agent to be better at conversation. In recent tests, Amazon shows that agents trained via reinforcement learning have better performance than those which use purely rule-based systems, laying the ground for a future where personal assistants continuously evolve and adapt to their users.
&nbsp; &nbsp;What Amazon did: For this project, Amazon first constructed a rule-based agent. This agent tries to figure out what actions to select based on user intent at any point in time, where actions could be offering particular &lsquo;skills&rsquo; (eg &lsquo;set an alarm&rsquo;) to the user, providing answers about a particular area of knowledge, launching a skill, and so on. To develop the system, the researchers first deployed a rule-based system to Amazon Alexa users, gathering 180,000 unique dialogues. They use these dialogues to build a user simulator which can then interact with another system by reinforcement learning &ndash; a useful feature, given that it&rsquo;s much faster to use a simulator to train RL systems, than to use real data which needs to be collected from the real world.&nbsp;
&nbsp; &nbsp;How well did it work? To test how well their system worked, Amazon did a real world test. Their baseline system recommended up to five skills based on popularity, then allowed the user to accept or reject the recommendation &ndash; this got a success rate of 46.42%. They then tested out their rule-based and RL-based systems against eachother in an A/B test; the rule-based baseline got 73.41% while the RL policy got 76.99% &ndash; this is already a statistically measurable difference, and along with this the RL policy had significantly shorter dialogues suggesting it was better at figuring out the right suggestion early on.&nbsp;
Why this matters: Soon, the world will be suffused with large, invisible machines, adjusting themselves around us to better entice and delight us. Many of the prototypes of these machines will show up in the form of the systems that underpin personal assistants like Amazon Alexa.&nbsp;&nbsp;&nbsp;Read more: Towards Personalized Dialog Policies for Conversational Skill Discovery (Arxiv).&nbsp;
####################################################
BERT-ageddon: From research into Microsoft and Google&rsquo;s search engines in under a year:&hellip;First, Google. Now Microsoft. Next: DuckDuckGo?&hellip;Microsoft has improved performance of its Bing search engine via the use of BERT, a language model that, along with systems like ULMFiT and GPT-2, has revolutionized natural language processing in recent years. &ldquo;Starting from April of this year, we used large transformer models to deliver the largest quality improvements to our Bing customers in the past year,&rdquo; Microsoft wrote in a blog post discussing the research. &ldquo;These models are now applied to every Bing search query globally making Bing results more relevant and intelligent&rdquo;.&nbsp;
&nbsp;&nbsp;&nbsp;What they did: Getting a model like BERT to support web search isn&rsquo;t easy; models like BERT are quite large and typically take a long time to sample from. Microsoft said an un-optimized version of BERT running on CPUs took 77ms to query. Microsoft reduced this to 6ms by running the model on an Azure NV6 GPU virtual machine and doing some low-level programming to optimize the model implementation. &ldquo;With these GPU optimizations, we were able to use 2000+ Azure GPU Virtual Machines across four regions to serve over 1 million BERT inferences per second worldwide,&rdquo; the company wrote.&nbsp;
&nbsp;&nbsp;&nbsp;Why this matters: BERT came out in late 2018. That&rsquo;s extremely recent! Imagine if someone came up with some prototype machinery for a factory and then six months later that prototype had been integrated into a million factories across the planet &ndash; that&rsquo;s kind of what has happened here. It highlights how rapidly AI can go from research to production and should make us think more deeply about the implications of the technologies we&rsquo;re developing.&nbsp;&nbsp;&nbsp;Read more: Bing delivers its largest improvement in search experience using Azure GPUs (Microsoft Azure blog).&nbsp;
####################################################
Spotting fake test with GPTrueorFalse:&hellip;Is that text you&rsquo;re reading made by a human or made by a machine?&hellip;In the coming years, the internet is going to fill up with text, images, and audio generated by increasingly large, powerful language models. At the same time, we can expect people to invest in building systems to detect the presence of synthetic content. To that end, a developer who goes by &lsquo;thesofakillers&rsquo; has created GPTrue or False, a browser extension that works out if text is generated or not. The extension uses OpenAI&rsquo;s GPT-2 detector model, hosted by Hugging Face.&nbsp;&nbsp;&nbsp;Read more: GPTrue or False Browser Extension (official GitHub page).&nbsp;
####################################################
Bill Gates: AI research wants to be open:&hellip;Microsoft co-founder speaks out in Beijing&hellip;Bill Gates says &ldquo;whoever has an open system will get massively ahead&rdquo; when it comes to developing national artificial intelligence capabilities, according to comments made by Gates at a Bloomberg event in Beijing this week. Gates says open research ecosystems beat closed ecosystems, and that protectionist policies can have a negative effect on technology development.&nbsp;&nbsp;&nbsp;Read more: Bill Gates Says Open Research Beats Erecting Borders in AI (Bloomberg News).&nbsp;
####################################################
MuZero means AI systems can learn the rules of games themselves:&hellip;DeepMind&rsquo;s new system wraps planning and learning into a generic model that learns Go, Chess, Shogi, Space Invaders, and more&hellip;In recent years, some types of progress in AI development have been defined by the creation of systems that can solve tasks in unprecedented ways, then the subsequent simplification and generalization of those systems. Some examples of this include the transition from AlphaGo (which included quiet a lot of world-state as input and some handwritten features and knowledge about the rules of Go) to AlphaGo Zero (which included less world state), and in translation where companies like Google have been replacing single-language-pair translation systems with a single big model that learns to translate between multiple languages at once. Now, DeepMind has announced MuZero, a single algorithm that they use to achieve state-of-the-art scores on tasks as varied as the Atari-57 corpus of games, Go, Chess, and Shogi.&nbsp;
MuZero&rsquo;s trick: The core of MuZero&rsquo;s success is that it combines tree search with a learned model. This means that MuZero can take in observations via standard deep learning components, then transforms those observations into a hidden state which it uses to plan out its next moves, simulating the strategic space of its environment automatically. This makes it easy for the agent to learn a model of the world it is acting in and to figure out how to plan appropriately.&nbsp;&nbsp;&nbsp;&ldquo;There is no direct constraint or requirement for the hidden state to capture all information necessary to reconstruct the original observation, drastically reducing the amount of information the model has to maintain and predict; nor is there any requirement for the hidden state to match the unknown, true state of the environment; nor any other constraints on the semantics of state,&rdquo; the researchers write. &ldquo;Instead, the hidden states are free to represent state in whatever way is relevant to predicting current a…