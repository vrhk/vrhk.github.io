---

layout: post
category: threads
title: "Unit Normalization + Cross-Entropy Loss outperforms Contrastive Loss? [Discussion]"
date: 2020-12-04 07:37:29
link: https://vrhk.co/3lB9QGx
image: https://external-preview.redd.it/XBvxxIPAp_KPRj04dL0If6Riym8wXD-KWN3OYgNZjVw.jpg?width=774&height=269&auto=webp&crop=774:269,smart&s=453114b2b75b078e2030ea4ed05b1d4b3a24e14c
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "I have been playing with the [Keras official example](<https://keras.io/examples/vision/supervised-contrastive-learning/>) of Fully Supervised..."

---

### Unit Normalization + Cross-Entropy Loss outperforms Contrastive Loss? [Discussion]

I have been playing with the [Keras official example](<https://keras.io/examples/vision/supervised-contrastive-learning/>) of Fully Supervised...