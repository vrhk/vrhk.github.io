---

layout: post
category: product
title: "Import AI 215: The Hardware Lottery; micro GPT3; and, the Peace Computer"
date: 2020-09-21 05:56:28
link: https://vrhk.co/360Ukzg
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Care about the future of AI, open source, and scientific publication norms? Join this NeurIPS workshop on Publication Norms :The Partnership on AI, a membership organization that coordinates AI industry, academia, and civil society, is hosting a workshop at NeurIPS this year about publication norms in AI research. The goal of the workshop is to help flesh out different ways to communicate about AI research, along with different strategies for publishing and/or releasing the technical components of developed systems. They&rsquo;ve just published a Call for Papers, so if you have any opinions on the future of publication norms and AI, please send them in.&nbsp;What questions are they interested in? Some of the questions PAI is asking include: &ldquo;What are some of the practical mechanisms for anticipating future risks and mitigating harms caused by AI research? Are such practices actually effective in improving societal outcomes and protecting vulnerable populations? To what extent do they help in bridging the gap between AI researchers and those with other perspectives and expertise, including the populations at risk of harm?&rdquo;&nbsp; Read more: Navigating the Broader Impacts of AI Research (NeurIPS workshop website).&nbsp; Disclaimer: I participate in the Publication Norms working group at PAI, so I have some bias here. I think longtime readers of this newsletter will understand my views &ndash; as we develop more powerful technology, we should invest more resources into mapping out the implications of the technology and communicating this to people who need to know, like policymakers and the general public.Want different publication norms? Here are some changes worth considering:&hellip;And here are the ways they could go wrong&hellip;How could we change publication norms to increase the range of beneficial impacts from AI research and reduce the downsides? That&rsquo;s an idea that the Montreal AI Ethics Institute (MAIEI) has tried to think through in a paper that discusses some of the issues around publication norms and potential changes to the research community.Potential changes to publication norms: So, what changes could we implement to change the course of AI research? Here are some ideas:&ndash; Increase paper page limits to let researchers include negative results in papers.&ndash; Have conferences require &lsquo;broader impacts&rsquo; statements to encourage work in this area.&ndash; Revamp the peer-review process&ndash; Use tools, like benchmarks or the results of third-party expert panels, to provide context about publication decisionsHow could changes to publication norms backfire? There are several ways this kind of shift can go wrong, for example:&ndash; Destroy science: If implemented in an overly restrictive manner, these changes could constrain or halt innovation at the earliest stages of research, closing off potentially useful avenues of research.&ndash; Black market research: It could push some types of perceived-as-dangerous research underground, creating private networks.&ndash; Misplaced accountability: Evaluating the broader impacts of research is challenging, so the institutions that could encourage changes in publication norms might not have the right skillsets.&nbsp;&nbsp; Read more: Report prepared by the Montreal AI Ethics Institute (MAIEI) for Publication Norms for Responsible AI by Partnership on AI (arXiv). ###################################################How good is the new RTX3080 for deep learning? This goodPuget Systems, a custom PC builder company, has evaluated some of the new NVIDIA cards. &ldquo;Initial results with TensorFlow running ResNet50 training looks to be significantly better than the RTX2080Ti,&rdquo; they write. Check out the post for detailed benchmarks on ResNet-50 training in both FP16 and FP32.&nbsp; Read more: RTX3080 TensorFlow and NAMD Performance on Linux (Puget Systems, lab blog).&nbsp;



###################################################The Hardware Lottery &ndash; how hardware dictates aspects of AI development:&hellip;Or, how CPU-led hardware development contributed to to a 40-year delay us being able to efficiently train large-scale neural networks&hellip;Picture this: it&rsquo;s the mid-1980s and a group of researchers announce to the world they&rsquo;ve trained a computer to categorize images using a technology called a &lsquo;neural network&rsquo;. The breakthrough has a range of commercial applications, leading to a dramatic rise in investment in &lsquo;connectionist&rsquo; AI approaches, along with development of hardware to implement the matrix multiplications required to do efficient neural net training. In the 1990s, the technology is turned into production and, though very expensive, finds its way into the world, leading to a flywheel of investment into the tech.&nbsp; Now: that didn&rsquo;t happen then. In fact, the above happened in 2012, when a team from the University of Toronto demonstrated good results on the &lsquo;ImageNet&rsquo;. The reason for their success? They&rsquo;d figured out how to harness graphical processing units (GPUs) to do large-scale parallelized neural net training &ndash; something the traditional CPUs are bad at because of their prioritization of fast, linear  &lsquo;The Hardware Lottery&rsquo;, Google Brain researcher Sara Hooker argues that many of our contemporary AI advances are a product of their hardware environment as well as their software one. But though researchers spend a lot of time on software, they don&rsquo;t pay as much attention as they could to how our hardware substrates dictate what types of research are possible. &ldquo;Machine learning researchers mostly ignore hardware despite the role it plays in determining what ideas succeed,&rdquo; Hooker says, before noting that our underlying hardware dictates our ability to develop certain types of AI, highlighting the neural net example.Are our new computers trapping us? Now, we&rsquo;re entering a new era where researchers are developing chips even more specialized for matrix multiplication than today&rsquo;s GPUs. See: TPUs, and other in-development chips for AI development. Could we be losing out on other types of AI as a consequence of this big bet on a certain type of hardware? Hooker thinks this is possible. For instance, Hooker notes that Capsule Networks &ndash; an architecture that includes &ldquo;novel components like squashing operations and routing by agreement&rdquo; which aren&rsquo;t trivial to optimize for GPUs and TPUs, leading to less investment and attention from researchers.What else could we be spending money on? &ldquo;More risky directions include biological hardware, analog hardware with in-memory computation, neuromorphic computing, optical computing, and quantum computing based approaches,&rdquo; Hooker says.&nbsp; Read more: The Hardware Lottery (arXiv).###################################################Better-than-GPT3 performance with 0.1% the number of parameters:&hellip;Sometimes, small is beautiful, though typically for specific tasks&hellip;This year, OpenAI published research on GPT-3, a class of large language models pre-trained on significant amounts of text data. One of the notable things about GPT-3 was how it did very well on the difficult multi-task SuperGLUE benchmark without SuperGLUE-specific pre-training &ndash; instead, OpenAI loaded SuperGLUE problems into the context window of an already trained GPT-3 model and tried to get it to output the correct answer.&nbsp; GPT-3 did surprisingly well at this, but at a significant cost: GPT3 is, to use a technical term, a honkingly large language model, with the largest version of it coming in at 175 BILLION parameters. This makes it expensive and challenging to run.Shrinking GPT-3-scale capabilities from billions to millions of parameters: Researchers with the Ludwig Maximilian University of Munich have tried to see if they can match or exceed the results of a GPT-3 model, but with someth…"

---

### Import AI 215: The Hardware Lottery; micro GPT3; and, the Peace Computer

Care about the future of AI, open source, and scientific publication norms? Join this NeurIPS workshop on Publication Norms :The Partnership on AI, a membership organization that coordinates AI industry, academia, and civil society, is hosting a workshop at NeurIPS this year about publication norms in AI research. The goal of the workshop is to help flesh out different ways to communicate about AI research, along with different strategies for publishing and/or releasing the technical components of developed systems. They&rsquo;ve just published a Call for Papers, so if you have any opinions on the future of publication norms and AI, please send them in.&nbsp;What questions are they interested in? Some of the questions PAI is asking include: &ldquo;What are some of the practical mechanisms for anticipating future risks and mitigating harms caused by AI research? Are such practices actually effective in improving societal outcomes and protecting vulnerable populations? To what extent do they help in bridging the gap between AI researchers and those with other perspectives and expertise, including the populations at risk of harm?&rdquo;&nbsp; Read more: Navigating the Broader Impacts of AI Research (NeurIPS workshop website).&nbsp; Disclaimer: I participate in the Publication Norms working group at PAI, so I have some bias here. I think longtime readers of this newsletter will understand my views &ndash; as we develop more powerful technology, we should invest more resources into mapping out the implications of the technology and communicating this to people who need to know, like policymakers and the general public.Want different publication norms? Here are some changes worth considering:&hellip;And here are the ways they could go wrong&hellip;How could we change publication norms to increase the range of beneficial impacts from AI research and reduce the downsides? That&rsquo;s an idea that the Montreal AI Ethics Institute (MAIEI) has tried to think through in a paper that discusses some of the issues around publication norms and potential changes to the research community.Potential changes to publication norms: So, what changes could we implement to change the course of AI research? Here are some ideas:&ndash; Increase paper page limits to let researchers include negative results in papers.&ndash; Have conferences require &lsquo;broader impacts&rsquo; statements to encourage work in this area.&ndash; Revamp the peer-review process&ndash; Use tools, like benchmarks or the results of third-party expert panels, to provide context about publication decisionsHow could changes to publication norms backfire? There are several ways this kind of shift can go wrong, for example:&ndash; Destroy science: If implemented in an overly restrictive manner, these changes could constrain or halt innovation at the earliest stages of research, closing off potentially useful avenues of research.&ndash; Black market research: It could push some types of perceived-as-dangerous research underground, creating private networks.&ndash; Misplaced accountability: Evaluating the broader impacts of research is challenging, so the institutions that could encourage changes in publication norms might not have the right skillsets.&nbsp;&nbsp; Read more: Report prepared by the Montreal AI Ethics Institute (MAIEI) for Publication Norms for Responsible AI by Partnership on AI (arXiv). ###################################################How good is the new RTX3080 for deep learning? This goodPuget Systems, a custom PC builder company, has evaluated some of the new NVIDIA cards. &ldquo;Initial results with TensorFlow running ResNet50 training looks to be significantly better than the RTX2080Ti,&rdquo; they write. Check out the post for detailed benchmarks on ResNet-50 training in both FP16 and FP32.&nbsp; Read more: RTX3080 TensorFlow and NAMD Performance on Linux (Puget Systems, lab blog).&nbsp;



###################################################The Hardware Lottery &ndash; how hardware dictates aspects of AI development:&hellip;Or, how CPU-led hardware development contributed to to a 40-year delay us being able to efficiently train large-scale neural networks&hellip;Picture this: it&rsquo;s the mid-1980s and a group of researchers announce to the world they&rsquo;ve trained a computer to categorize images using a technology called a &lsquo;neural network&rsquo;. The breakthrough has a range of commercial applications, leading to a dramatic rise in investment in &lsquo;connectionist&rsquo; AI approaches, along with development of hardware to implement the matrix multiplications required to do efficient neural net training. In the 1990s, the technology is turned into production and, though very expensive, finds its way into the world, leading to a flywheel of investment into the tech.&nbsp; Now: that didn&rsquo;t happen then. In fact, the above happened in 2012, when a team from the University of Toronto demonstrated good results on the &lsquo;ImageNet&rsquo;. The reason for their success? They&rsquo;d figured out how to harness graphical processing units (GPUs) to do large-scale parallelized neural net training &ndash; something the traditional CPUs are bad at because of their prioritization of fast, linear  &lsquo;The Hardware Lottery&rsquo;, Google Brain researcher Sara Hooker argues that many of our contemporary AI advances are a product of their hardware environment as well as their software one. But though researchers spend a lot of time on software, they don&rsquo;t pay as much attention as they could to how our hardware substrates dictate what types of research are possible. &ldquo;Machine learning researchers mostly ignore hardware despite the role it plays in determining what ideas succeed,&rdquo; Hooker says, before noting that our underlying hardware dictates our ability to develop certain types of AI, highlighting the neural net example.Are our new computers trapping us? Now, we&rsquo;re entering a new era where researchers are developing chips even more specialized for matrix multiplication than today&rsquo;s GPUs. See: TPUs, and other in-development chips for AI development. Could we be losing out on other types of AI as a consequence of this big bet on a certain type of hardware? Hooker thinks this is possible. For instance, Hooker notes that Capsule Networks &ndash; an architecture that includes &ldquo;novel components like squashing operations and routing by agreement&rdquo; which aren&rsquo;t trivial to optimize for GPUs and TPUs, leading to less investment and attention from researchers.What else could we be spending money on? &ldquo;More risky directions include biological hardware, analog hardware with in-memory computation, neuromorphic computing, optical computing, and quantum computing based approaches,&rdquo; Hooker says.&nbsp; Read more: The Hardware Lottery (arXiv).###################################################Better-than-GPT3 performance with 0.1% the number of parameters:&hellip;Sometimes, small is beautiful, though typically for specific tasks&hellip;This year, OpenAI published research on GPT-3, a class of large language models pre-trained on significant amounts of text data. One of the notable things about GPT-3 was how it did very well on the difficult multi-task SuperGLUE benchmark without SuperGLUE-specific pre-training &ndash; instead, OpenAI loaded SuperGLUE problems into the context window of an already trained GPT-3 model and tried to get it to output the correct answer.&nbsp; GPT-3 did surprisingly well at this, but at a significant cost: GPT3 is, to use a technical term, a honkingly large language model, with the largest version of it coming in at 175 BILLION parameters. This makes it expensive and challenging to run.Shrinking GPT-3-scale capabilities from billions to millions of parameters: Researchers with the Ludwig Maximilian University of Munich have tried to see if they can match or exceed the results of a GPT-3 model, but with someth…