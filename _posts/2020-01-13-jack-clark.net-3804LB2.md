---

layout: post
category: product
title: "Import AI 180: Analyzing farms with Agriculture Vision; how deep learning is applied to X-ray security scanning; Agility Robots puts its ‘Digit’ bot up for 6-figure sale"
date: 2020-01-13 17:06:35
link: https://vrhk.co/3804LB2
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Deep learning is superseding machine learning in X-ray security imaging:&hellip;But, like most deep learning applications, researchers want better generalization&hellip;Deep learning-based methods have, since 2016, become the dominant approach used in X-ray security imaging research papers, according to a survey paper from researchers at Durham University. It seems likely that many of today&rsquo;s machine learning algorithms will be replaced or superseded by deep learning systems paired with domain knowledge, they indicate. So, what challenges do deep learning practitioners need to work on to further improve the state-of-the-art in X-ray security imaging?
Research directions for smart X-rays: Future directions in X-ray research feel, to me, like they&rsquo;re quite similar to future directions in general image recognition research &ndash; there need to be more datasets, better explorations of generalization, and more work done in unsupervised learning.&nbsp;
Data: Researchers should &ldquo;build large, homogeneous, realistic and publicly available datasets, collected either by (i) manually scanning numerous bags with different objects and orientations in a lab environment or (ii) generating synthetic datasets via contemporary algorithms&rdquo;.&nbsp;
Scanner transfers: It&rsquo;s not clear how well different models transfer between different scanners &ndash; if we figure that out, then we&rsquo;ll be able to better model the economic implications of work here.&nbsp;
Unsupervised learning: One promising line of research is into detecting anomalous items in an unsupervised way. &ldquo;More research on this topic needs to be undertaken to design better reconstruction techniques that thoroughly learn the characteristics of the normality from which the abnormality would be detected,&rdquo; they write.&nbsp;
Material information: Some x-rays attenuate between high and low energies during a scan, which generates different information according to the materials of the object being scanned &ndash; this information could be used to better improve classification and detection performance.&nbsp;
Read more: Towards Automatic Threat Detection: A Survey of Advances of Deep Learning within X-ray Security Imaging (Arxiv).&nbsp;
####################################################
Agility Robots starts selling its bipedal bot:&hellip;But the company only plans to make between 20 and 30 this year&hellip;Robot startup Agility Robotics has started selling its bipedal &lsquo;Digit&rsquo; robot. Digit is about the size of a small adult human and can carry boxes in its arms of up to 40 pounds in weight, according to The Verge. The company&rsquo;s technology has roots in legged locomotion research Oregon State University &ndash; for many years, Agility&rsquo;s bots only had legs, with the arms being a recent addition. Robot costs: Each Digit costs in the &ldquo;low-mid six figures&rdquo;, Agility&rsquo;s CEO told The Verge. &ldquo;When factoring in upkeep and the robot&rsquo;s expected lifespan, Shelton estimates this amounts to an hourly cost of roughly $25. The first production run of Digits is six units, and Agility expects to make only 20 or 30 of the robots in 2020.&nbsp;
Capabilities: The thing is, these robots aren&rsquo;t that capable yet. They&rsquo;ve got a tremendous amount of intelligence coded into them to allow for elegant, rapid walking. But they lack the autonomous capabilities necessary to, say, automatically pick up boxes and navigate through a couple of buildings to a waiting delivery truck (though Ford is conducting research here). You can get more of a sense of Digit&rsquo;s capabilities by looking at the demo of the robot at CES this year, where it transports packages covered with QR codes from a table to a truck.&nbsp;
Why this matters: Digit is a no-bullshit robot: it walks, can pick things up, and is actually going on sale. It, along with for-sale &lsquo;Spot&rsquo; robots from Boston Dynamics represents the cutting-edge in terms of robot mobility. Now we need to see what kinds of economically-useful tasks these robots can do &ndash; and that&rsquo;s a question that&rsquo;s going to be hard to answer, as it is somewhat contingent on the price of the robots, and these prices are dictated by volume production economics, which are themselves determined by overall market demand. Robotics feels like it&rsquo;s still caught in this awkward chicken and egg problem.  &nbsp; Read more: This walking package-delivery robot is now for sale (The Verge).&nbsp;&nbsp;&nbsp;Watch the video (official Agility Robotics YouTube).&nbsp;
####################################################
Agriculture-Vision gives researchers a massive dataset of aerial farm photographs:&hellip;3,432 farms, annotated&hellip;Researchers with UIUC, Intelinair, and the University of Oregon have developed Agriculture-Vision, a large-scale dataset of aerial photographs of farmland, annotated with nine distinct events (e.g., flooding).&nbsp;
Why farm images are hard: Farm images pose challenges to contemporary techniques because they&rsquo;re often very large (e.g., some of the raw images here had dimensions like 10,000 X 3000 pixels), annotating them requires significant domain knowledge, and very few public large-scale datasets exist to help spur research in this area &ndash; until now!
The dataset&hellip; consists of 94,986 aerial images from 3,432 farmlands across the US. The images were collected by drone during growing seasons between 2017 and 2019.&nbsp; Each image consists of RGB and Near-infrared channels, with resolutions as detailed as 10 cm per pixel. Each image is 512 X 512 resolution and can be labeled with nine types of anomaly, like storm damage, nutrient deficiency, weeds, and so on. The labels are unbalanced due to environmental variations, with annotations for drydown, nutrient deficiency and weed clusters overrepresented in the dataset.Why this matters: AI gives us a chance to build a sense&amp;respond system for the entire planet &ndash; and building such a system starts with gathering datasets like Agriculture-Vision. In a few years don&rsquo;t be surprised when large-scale farms use fleets of drones to proactively monitor their fields and automatically identify problems.&nbsp;&nbsp;&nbsp;Read more: Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis (Arxiv).&nbsp;&nbsp;&nbsp;Find out more information about the upcoming Agriculture Vision competition here (official website).&nbsp;
####################################################
Hitachi describes the pain of building real world AI:&hellip;Need an assistant with domain-specific knowledge? Get ready to work extra hard&hellip;Most applied AI papers can be summarized as: the real world is hellish in the following ways; these are our mitigations. Researchers with Hitachi America Ltd. follow in this tradition by writing a paper that discusses the challenges of building a real-world speech-activated virtual assistant.&nbsp;
What they did: For this work, they developed &ldquo;a virtual assistant for suggesting repairs of equipment-related complaints&rdquo; in vehicles. This system is meant to process phrases like &ldquo;coolant reservoir cracked&rdquo; and map that to the relevant things in its internal knowledge base, then tell the user an appropriate answer. This, as with most real-world AI uses, is harder than it looks. To build their system, they create a pipeline that samples words from a domain-specific corpus of manuals, repair records, etc, then uses a set of domain-specific syntactic rules to extract a vocabulary from the text. They use this pipeline to create two things: a knowledge base, populated from the domain-specific corpus; and a neural-attention based tagging model called S2STagger, for annotating new text as it comes in.
Hitachi versus Amazon versus Google: They use a couple of off-the-shelf services (AlexaSkill from Amazon, and DiagFlow from Google) to develop dialog-agents…"

---

### Import AI 180: Analyzing farms with Agriculture Vision; how deep learning is applied to X-ray security scanning; Agility Robots puts its ‘Digit’ bot up for 6-figure sale

Deep learning is superseding machine learning in X-ray security imaging:&hellip;But, like most deep learning applications, researchers want better generalization&hellip;Deep learning-based methods have, since 2016, become the dominant approach used in X-ray security imaging research papers, according to a survey paper from researchers at Durham University. It seems likely that many of today&rsquo;s machine learning algorithms will be replaced or superseded by deep learning systems paired with domain knowledge, they indicate. So, what challenges do deep learning practitioners need to work on to further improve the state-of-the-art in X-ray security imaging?
Research directions for smart X-rays: Future directions in X-ray research feel, to me, like they&rsquo;re quite similar to future directions in general image recognition research &ndash; there need to be more datasets, better explorations of generalization, and more work done in unsupervised learning.&nbsp;
Data: Researchers should &ldquo;build large, homogeneous, realistic and publicly available datasets, collected either by (i) manually scanning numerous bags with different objects and orientations in a lab environment or (ii) generating synthetic datasets via contemporary algorithms&rdquo;.&nbsp;
Scanner transfers: It&rsquo;s not clear how well different models transfer between different scanners &ndash; if we figure that out, then we&rsquo;ll be able to better model the economic implications of work here.&nbsp;
Unsupervised learning: One promising line of research is into detecting anomalous items in an unsupervised way. &ldquo;More research on this topic needs to be undertaken to design better reconstruction techniques that thoroughly learn the characteristics of the normality from which the abnormality would be detected,&rdquo; they write.&nbsp;
Material information: Some x-rays attenuate between high and low energies during a scan, which generates different information according to the materials of the object being scanned &ndash; this information could be used to better improve classification and detection performance.&nbsp;
Read more: Towards Automatic Threat Detection: A Survey of Advances of Deep Learning within X-ray Security Imaging (Arxiv).&nbsp;
####################################################
Agility Robots starts selling its bipedal bot:&hellip;But the company only plans to make between 20 and 30 this year&hellip;Robot startup Agility Robotics has started selling its bipedal &lsquo;Digit&rsquo; robot. Digit is about the size of a small adult human and can carry boxes in its arms of up to 40 pounds in weight, according to The Verge. The company&rsquo;s technology has roots in legged locomotion research Oregon State University &ndash; for many years, Agility&rsquo;s bots only had legs, with the arms being a recent addition. Robot costs: Each Digit costs in the &ldquo;low-mid six figures&rdquo;, Agility&rsquo;s CEO told The Verge. &ldquo;When factoring in upkeep and the robot&rsquo;s expected lifespan, Shelton estimates this amounts to an hourly cost of roughly $25. The first production run of Digits is six units, and Agility expects to make only 20 or 30 of the robots in 2020.&nbsp;
Capabilities: The thing is, these robots aren&rsquo;t that capable yet. They&rsquo;ve got a tremendous amount of intelligence coded into them to allow for elegant, rapid walking. But they lack the autonomous capabilities necessary to, say, automatically pick up boxes and navigate through a couple of buildings to a waiting delivery truck (though Ford is conducting research here). You can get more of a sense of Digit&rsquo;s capabilities by looking at the demo of the robot at CES this year, where it transports packages covered with QR codes from a table to a truck.&nbsp;
Why this matters: Digit is a no-bullshit robot: it walks, can pick things up, and is actually going on sale. It, along with for-sale &lsquo;Spot&rsquo; robots from Boston Dynamics represents the cutting-edge in terms of robot mobility. Now we need to see what kinds of economically-useful tasks these robots can do &ndash; and that&rsquo;s a question that&rsquo;s going to be hard to answer, as it is somewhat contingent on the price of the robots, and these prices are dictated by volume production economics, which are themselves determined by overall market demand. Robotics feels like it&rsquo;s still caught in this awkward chicken and egg problem.  &nbsp; Read more: This walking package-delivery robot is now for sale (The Verge).&nbsp;&nbsp;&nbsp;Watch the video (official Agility Robotics YouTube).&nbsp;
####################################################
Agriculture-Vision gives researchers a massive dataset of aerial farm photographs:&hellip;3,432 farms, annotated&hellip;Researchers with UIUC, Intelinair, and the University of Oregon have developed Agriculture-Vision, a large-scale dataset of aerial photographs of farmland, annotated with nine distinct events (e.g., flooding).&nbsp;
Why farm images are hard: Farm images pose challenges to contemporary techniques because they&rsquo;re often very large (e.g., some of the raw images here had dimensions like 10,000 X 3000 pixels), annotating them requires significant domain knowledge, and very few public large-scale datasets exist to help spur research in this area &ndash; until now!
The dataset&hellip; consists of 94,986 aerial images from 3,432 farmlands across the US. The images were collected by drone during growing seasons between 2017 and 2019.&nbsp; Each image consists of RGB and Near-infrared channels, with resolutions as detailed as 10 cm per pixel. Each image is 512 X 512 resolution and can be labeled with nine types of anomaly, like storm damage, nutrient deficiency, weeds, and so on. The labels are unbalanced due to environmental variations, with annotations for drydown, nutrient deficiency and weed clusters overrepresented in the dataset.Why this matters: AI gives us a chance to build a sense&amp;respond system for the entire planet &ndash; and building such a system starts with gathering datasets like Agriculture-Vision. In a few years don&rsquo;t be surprised when large-scale farms use fleets of drones to proactively monitor their fields and automatically identify problems.&nbsp;&nbsp;&nbsp;Read more: Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis (Arxiv).&nbsp;&nbsp;&nbsp;Find out more information about the upcoming Agriculture Vision competition here (official website).&nbsp;
####################################################
Hitachi describes the pain of building real world AI:&hellip;Need an assistant with domain-specific knowledge? Get ready to work extra hard&hellip;Most applied AI papers can be summarized as: the real world is hellish in the following ways; these are our mitigations. Researchers with Hitachi America Ltd. follow in this tradition by writing a paper that discusses the challenges of building a real-world speech-activated virtual assistant.&nbsp;
What they did: For this work, they developed &ldquo;a virtual assistant for suggesting repairs of equipment-related complaints&rdquo; in vehicles. This system is meant to process phrases like &ldquo;coolant reservoir cracked&rdquo; and map that to the relevant things in its internal knowledge base, then tell the user an appropriate answer. This, as with most real-world AI uses, is harder than it looks. To build their system, they create a pipeline that samples words from a domain-specific corpus of manuals, repair records, etc, then uses a set of domain-specific syntactic rules to extract a vocabulary from the text. They use this pipeline to create two things: a knowledge base, populated from the domain-specific corpus; and a neural-attention based tagging model called S2STagger, for annotating new text as it comes in.
Hitachi versus Amazon versus Google: They use a couple of off-the-shelf services (AlexaSkill from Amazon, and DiagFlow from Google) to develop dialog-agents…