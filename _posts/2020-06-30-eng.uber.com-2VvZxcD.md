---

layout: post
category: engineering
title: "Fiber: Distributed Computing for AI Made  Simple"
date: 2020-06-30 16:35:35
link: https://ubr.to/2VvZxcD
image: https://eng.uber.com/wp-content/uploads/2020/06/fiber-1-e1593497851266.png
domain: eng.uber.com
author: "Uber Engineering Blog"
icon: https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/favicon.png
excerpt: "Project Homepage: GitHub Over the past several years, increasing processing power of computing machines has led to an increase in machine learning advances. More and more, algorithms exploit parallelism and rely on distributed training to process an enormous amount of data. However, the resulting need to increase both data and training impose great challenges on the software that manages and utilizes large-scale computational resources. At Uber, we’ve developed algorithms such as POET, Go-Explore, and GTN that leverage a large amount of computation to train models on neural networks. To enable future generations of large-scale computation for algorithms like these, we"

---

### Fiber: Distributed Computing for AI Made  Simple

Project Homepage: GitHub Over the past several years, increasing processing power of computing machines has led to an increase in machine learning advances. More and more, algorithms exploit parallelism and rely on distributed training to process an enormous amount of data. However, the resulting need to increase both data and training impose great challenges on the software that manages and utilizes large-scale computational resources. At Uber, we’ve developed algorithms such as POET, Go-Explore, and GTN that leverage a large amount of computation to train models on neural networks. To enable future generations of large-scale computation for algorithms like these, we