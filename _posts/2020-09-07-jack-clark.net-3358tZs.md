---

layout: post
category: product
title: "Import AI 213: DeepFakes can lipsync now; plus hiding military gear with adversarial examples."
date: 2020-09-07 18:46:32
link: https://vrhk.co/3358tZs
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Facebook wants people to use differential privacy, so Facebook has made the technology faster:&hellip;Opacus; software that deals with the speed-problem of differential privacy&hellip;Facebook has released Opacus, a software library for training PyTorch models with a privacy-preserving technology called Differential Privacy (DP). The library is fast, integrated with PyTorch (so inherently quite usable), and is being used by one of the main open source ML+ DP projects, OpenMined. Why care about differential privacy: It&rsquo;s easy to develop AI models using open and generic image or text datasets &ndash; think ImageNet, or CommonCrawl &ndash; but if you&rsquo;re trying to develop a more specific AI application, you might need to handle sensitive user data, e.g, data that relates to an individual&rsquo;s medical or credit status, or emails written in a protected context. Today, you need to get a bunch of permissions to deal with this data, but if you could find a way to encrypt it before you saw it you&rsquo;d be able to work with it in a privacy-preserving way. That&rsquo;s where privacy preserving machine learning techniques come in: Opcasus makes it easier for developers to train models using Differential Privacy &ndash; a privacy preserving technique that lets us train over sensitive user data (Apple uses it).&nbsp; &nbsp;  &nbsp; One drawback of differential privacy has been its speed &ndash; Opcasus has improved this part of the problem by being carefully engineered atop PyTorch to lead to a system that is &ldquo;an order of magnitude faster compared with the alternative micro-batch method used in other packages&rdquo;, according to Facebook. &nbsp; Read more: Introducing Opacus: A high-speed library for training PyTorch models with differential privacy (Facebook AI Blog). &nbsp; Get the code for Opacus here (PyTorch, GitHub). &nbsp; Find out more about differential privacy here: Differential Privacy Series Part 1 | DP-SGD Algorithm Explained (Medium, PyTorch). ###################################################Is it a bird? Is it a plane? No, it is a&hellip; SUNFLOWER&hellip;Adversarial patches + military equipment = an &lsquo;uh oh&rsquo; proof of concept&hellip;Some Dutch researchers, including ones affiliated with the Netherlands&rsquo; Ministry of Defense, have applied adversarial examples to military hardware. An adversarial example is a visual distortion you can apply to an image or object that makes it hard for an AI system to classify it. In this research, they add confounding visual elements to satellite images of military hardware (e.g, fighter jets), causing the system (in this case, YOLOv2) to misclassify the entity in question. A proof of concept: This is a proof of concept and not indicative of the real world tractability of the attack (e.g, it&rsquo;s important to know the type of image processing system your adversary is using, or they might not be vulnerable to your adversarial perturbation; multiple overlapping image processing systems could invalidate the attack, etc). But it does provide a further example of adversarial examples being applied in the wild, following the creation of things as varied as adversarial turtles (#67), t-shirts (#171), and patches.  &nbsp; Plus, it&rsquo;s notable to see military-affiliated researchers do this analysis in their own context (here, trying to cause misidentification of aircraft), which can be taken as a proxy for growing military interest in AI security and counter-security techniques.  &nbsp; Read more: Adversarial Patch Camouflage against Aerial Detection (arXiv).###################################################Fake lipsync: deepfakes are about to get an audio component:&hellip;Wav2Lip = automated lip-syncing for synthetic media =.Sound and Vision&hellip;Today, we can generate synthetic images and videos of people speaking, and we can even pair this with a different audio track, but syncing up the audio with the videos in a convincing way is challenging. That&rsquo;s where new research from IIIT Hyderabad and the University of Bath comes in via &lsquo;Wav2Lip&rsquo;, technology that makes it easy to get a person in a synthetic image or video to lip-sync to an audio track. Many applications: A technology like this would make it very cheap to add lip-syncing to things like dubbed movies, video game characters, lectures, generating missing video call segments, and more, the researchers note. How they did it &ndash; a GAN within a GAN: The authors have a clever solution to the problem of generating faces synced to audio &ndash; they use a pre-trained &lsquo;SyncNet&rsquo;-based discriminator model to analyze generated outputs and check if the face is synced to the audio and if it isn&rsquo;t encourage the generation of one that is, along with another pre-trained model to provide a signal if the synthetic face&amp;lip combination looks unnatural. These two networks sit inside of the broader generation process, where the algorithm tries to generate faces matched to audio.  &nbsp; The results are really good, so good that the authors also proposed a new evaluation framework for evaluating synthetically-generated lip-sync programs.  &nbsp; New ways of measuring tech, as the authors do here, are typically a canary for broader tech progress, because when we need to invent new measures it means we&rsquo;ve  &nbsp; a) reached the ceiling of existing datasets/challenges or  &nbsp; b) have got sufficiently good at the task we need to develop a more granular scoring system for it.  &nbsp; Both of these phenomena are indicative of different types of AI progress. New ways of measuring performance on a task also usually yield further research breakthroughs, as researchers are able to use the new testing regimes to generate better information about the problem they&rsquo;re trying to solve. Combined, we should take the contents of this paper as a strong signal that synthetically generated video with lipsyncing to audio is starting to get very good, and we should expect it to continue to improve. My bet is we have &lsquo;seamless&rsquo; integration of the two within a year*. &nbsp; (*Constraints &ndash; portrait-style camera views, across a broad distribution of types of people and types of clothing; some background blur permitted but not enough to be egregious. These are quite qualitative evals, so I&rsquo;ll refine them as the technology develops.Broader impacts and the discussion (or lack of): The authors do note the potential for abuse by these models and say they&rsquo;re releasing the models as open source to &ldquo;encourage efforts in detecting manipulated video content and their misuse&rdquo;. This is analogous to saying &ldquo;by releasing the poison, we&rsquo;re going to encourage efforts to create its antidote&rdquo;. I think it&rsquo;s worth reflecting on whether there are other, less extreme, solutions to the thorny issue of model publication and dissemination. We should also ask how much damage the &lsquo;virus&rsquo; could do before an &lsquo;antidote&rsquo; is available. &nbsp; Read more: A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild (arXiv). &nbsp; Try out the interactive demo here (Wav2Lip site). &nbsp; Find out more about their technique here at the project page. &nbsp; Mess around with a Colab notebook here (Google Colab).###################################################DeepMind makes Google Maps get better:&hellip;Graph Neural Nets = better ETAs in Google Maps&hellip;DeepMind has worked with the team at Google Maps to develop more accurate ETAs, so next time you use your phone to plot a route you can have a (slightly) higher trust in the ETA being accurate. How they did it: DeepMind worked with Google to use a Graph Neural Network to predict route ETAs within geographic sub-sections (called &lsquo;supersegments&rsquo;) of Google&rsquo;s globe-spanning mapping system. &ldquo;Our model treats the local road network as a graph, where each route segmen…"

---

### Import AI 213: DeepFakes can lipsync now; plus hiding military gear with adversarial examples.

Facebook wants people to use differential privacy, so Facebook has made the technology faster:&hellip;Opacus; software that deals with the speed-problem of differential privacy&hellip;Facebook has released Opacus, a software library for training PyTorch models with a privacy-preserving technology called Differential Privacy (DP). The library is fast, integrated with PyTorch (so inherently quite usable), and is being used by one of the main open source ML+ DP projects, OpenMined. Why care about differential privacy: It&rsquo;s easy to develop AI models using open and generic image or text datasets &ndash; think ImageNet, or CommonCrawl &ndash; but if you&rsquo;re trying to develop a more specific AI application, you might need to handle sensitive user data, e.g, data that relates to an individual&rsquo;s medical or credit status, or emails written in a protected context. Today, you need to get a bunch of permissions to deal with this data, but if you could find a way to encrypt it before you saw it you&rsquo;d be able to work with it in a privacy-preserving way. That&rsquo;s where privacy preserving machine learning techniques come in: Opcasus makes it easier for developers to train models using Differential Privacy &ndash; a privacy preserving technique that lets us train over sensitive user data (Apple uses it).&nbsp; &nbsp;  &nbsp; One drawback of differential privacy has been its speed &ndash; Opcasus has improved this part of the problem by being carefully engineered atop PyTorch to lead to a system that is &ldquo;an order of magnitude faster compared with the alternative micro-batch method used in other packages&rdquo;, according to Facebook. &nbsp; Read more: Introducing Opacus: A high-speed library for training PyTorch models with differential privacy (Facebook AI Blog). &nbsp; Get the code for Opacus here (PyTorch, GitHub). &nbsp; Find out more about differential privacy here: Differential Privacy Series Part 1 | DP-SGD Algorithm Explained (Medium, PyTorch). ###################################################Is it a bird? Is it a plane? No, it is a&hellip; SUNFLOWER&hellip;Adversarial patches + military equipment = an &lsquo;uh oh&rsquo; proof of concept&hellip;Some Dutch researchers, including ones affiliated with the Netherlands&rsquo; Ministry of Defense, have applied adversarial examples to military hardware. An adversarial example is a visual distortion you can apply to an image or object that makes it hard for an AI system to classify it. In this research, they add confounding visual elements to satellite images of military hardware (e.g, fighter jets), causing the system (in this case, YOLOv2) to misclassify the entity in question. A proof of concept: This is a proof of concept and not indicative of the real world tractability of the attack (e.g, it&rsquo;s important to know the type of image processing system your adversary is using, or they might not be vulnerable to your adversarial perturbation; multiple overlapping image processing systems could invalidate the attack, etc). But it does provide a further example of adversarial examples being applied in the wild, following the creation of things as varied as adversarial turtles (#67), t-shirts (#171), and patches.  &nbsp; Plus, it&rsquo;s notable to see military-affiliated researchers do this analysis in their own context (here, trying to cause misidentification of aircraft), which can be taken as a proxy for growing military interest in AI security and counter-security techniques.  &nbsp; Read more: Adversarial Patch Camouflage against Aerial Detection (arXiv).###################################################Fake lipsync: deepfakes are about to get an audio component:&hellip;Wav2Lip = automated lip-syncing for synthetic media =.Sound and Vision&hellip;Today, we can generate synthetic images and videos of people speaking, and we can even pair this with a different audio track, but syncing up the audio with the videos in a convincing way is challenging. That&rsquo;s where new research from IIIT Hyderabad and the University of Bath comes in via &lsquo;Wav2Lip&rsquo;, technology that makes it easy to get a person in a synthetic image or video to lip-sync to an audio track. Many applications: A technology like this would make it very cheap to add lip-syncing to things like dubbed movies, video game characters, lectures, generating missing video call segments, and more, the researchers note. How they did it &ndash; a GAN within a GAN: The authors have a clever solution to the problem of generating faces synced to audio &ndash; they use a pre-trained &lsquo;SyncNet&rsquo;-based discriminator model to analyze generated outputs and check if the face is synced to the audio and if it isn&rsquo;t encourage the generation of one that is, along with another pre-trained model to provide a signal if the synthetic face&amp;lip combination looks unnatural. These two networks sit inside of the broader generation process, where the algorithm tries to generate faces matched to audio.  &nbsp; The results are really good, so good that the authors also proposed a new evaluation framework for evaluating synthetically-generated lip-sync programs.  &nbsp; New ways of measuring tech, as the authors do here, are typically a canary for broader tech progress, because when we need to invent new measures it means we&rsquo;ve  &nbsp; a) reached the ceiling of existing datasets/challenges or  &nbsp; b) have got sufficiently good at the task we need to develop a more granular scoring system for it.  &nbsp; Both of these phenomena are indicative of different types of AI progress. New ways of measuring performance on a task also usually yield further research breakthroughs, as researchers are able to use the new testing regimes to generate better information about the problem they&rsquo;re trying to solve. Combined, we should take the contents of this paper as a strong signal that synthetically generated video with lipsyncing to audio is starting to get very good, and we should expect it to continue to improve. My bet is we have &lsquo;seamless&rsquo; integration of the two within a year*. &nbsp; (*Constraints &ndash; portrait-style camera views, across a broad distribution of types of people and types of clothing; some background blur permitted but not enough to be egregious. These are quite qualitative evals, so I&rsquo;ll refine them as the technology develops.Broader impacts and the discussion (or lack of): The authors do note the potential for abuse by these models and say they&rsquo;re releasing the models as open source to &ldquo;encourage efforts in detecting manipulated video content and their misuse&rdquo;. This is analogous to saying &ldquo;by releasing the poison, we&rsquo;re going to encourage efforts to create its antidote&rdquo;. I think it&rsquo;s worth reflecting on whether there are other, less extreme, solutions to the thorny issue of model publication and dissemination. We should also ask how much damage the &lsquo;virus&rsquo; could do before an &lsquo;antidote&rsquo; is available. &nbsp; Read more: A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild (arXiv). &nbsp; Try out the interactive demo here (Wav2Lip site). &nbsp; Find out more about their technique here at the project page. &nbsp; Mess around with a Colab notebook here (Google Colab).###################################################DeepMind makes Google Maps get better:&hellip;Graph Neural Nets = better ETAs in Google Maps&hellip;DeepMind has worked with the team at Google Maps to develop more accurate ETAs, so next time you use your phone to plot a route you can have a (slightly) higher trust in the ETA being accurate. How they did it: DeepMind worked with Google to use a Graph Neural Network to predict route ETAs within geographic sub-sections (called &lsquo;supersegments&rsquo;) of Google&rsquo;s globe-spanning mapping system. &ldquo;Our model treats the local road network as a graph, where each route segmen…