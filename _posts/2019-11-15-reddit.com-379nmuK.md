---

layout: post
category: threads
title: "[P] Nearing BERT's accuracy on Sentiment Analysis with a model 56 times smaller by Knowledge Distillation"
date: 2019-11-15 19:17:29
link: https://vrhk.co/379nmuK
image: https://external-preview.redd.it/ykRB4pGjAjOL3osHFqggoxa8dxKlkwP_ZiiN-YCCsi0.jpg?width=1200&height=628.272251309&auto=webp&s=a6931cda423edf60ac29cb18386b1a2e4eb0b396
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Hello everyone, I recently trained a tiny bidirectional LSTM model to achieve high accuracy on Stanford's SST-2 by using knowledge distillation..."

---

### [P] Nearing BERT's accuracy on Sentiment Analysis with a model 56 times smaller by Knowledge Distillation

Hello everyone, I recently trained a tiny bidirectional LSTM model to achieve high accuracy on Stanford's SST-2 by using knowledge distillation...