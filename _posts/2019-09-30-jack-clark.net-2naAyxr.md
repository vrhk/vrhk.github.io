---

layout: post
category: product
title: "Import AI: 166: Dawn of the misleading ‘sophistbots’; $50k a year for studying long-term impacts of AI; and squeezing an RL drone policy into 3kb"
date: 2019-09-30 19:06:26
link: https://vrhk.co/2naAyxr
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Will powerful AI make the Turing Test obsolete?&hellip;And if it does, what do we do about it?&hellip;The Turing Test &ndash; judging how sophisticated a machine is, by seeing if it can convince a person that it is a human &ndash; looms large in pop culture discussion about AI. What happens if we have systems today that can pass the Turing Test, but which aren&rsquo;t actually that intelligent? That&rsquo;s something that has started to happen recently with systems that a human interfaces with via text chat. Now, new research from Stanford University, Pennsylvania State University, and the University of Toronto, explores how increasingly advanced so-called &lsquo;sophistbots&rsquo; might influence society.
The problems of &lsquo;sophisbots&rsquo;: The researchers imagine what the future of social media might look like, given recent advances in the ability for AI systems to generate synthetic media. In particular, they imagine social media ruled by &ldquo;sophisbots&rdquo;. They foresee a future where these bots are constantly &ldquo;running in the ether of social media or other infrastructure&hellip;not bound by geography, culture or conscience.&rdquo;&nbsp;
So, what do we do? Technical solutions: Machine learning researchers should develop technical tools to help spot machines posing as humans, and should invest in work to detect the telltale signs of AI-generated things, along with systems to track down the provenance of content to be able to guarantee that something is &lsquo;real&rsquo;, and tools to make it easy for regular people to indicate that the content they themselves are putting online is authentic and not bot-generated.&nbsp;&nbsp;&nbsp;Policy approaches: We need to develop &ldquo;public policy, legal, and normative frameworks for managing the malicious applications of technology in conjunction with efforts to refine it,&rdquo; they write. &ldquo;Let us as a technical community commit ourselves to embracing and addressing these challenges as readily as we do the fascinating and exciting new uses of intelligent systems&rdquo;.
Why this matters: How we deal with the future of synthetic content will define the nature of &lsquo;truth&rsquo; in society, which will ultimately define everything else. So, no pressure.&nbsp;&nbsp;&nbsp;Read more: How Relevant is the Turing Test in the Age of Sophisbots (Arxiv).&nbsp;
####################################################
Do Octopuses dream of electric sheep?Apropos of nothing, here is a film of an octopus changing colors while sleeping.&nbsp;&nbsp;&nbsp;View the sleeping octopus here (Twitter).
####################################################
PHD student? Want $50k a year to study the long-term impacts of AI? Read on!&hellip;Check out the Open Philanthropy Project&rsquo;s &lsquo;AI Fellowship&rsquo;&hellip;$50k for up to five years, with possibility of renewal&hellip;Applications are now open for the Open Phil AI Fellowship. This program extends full support to a community of current &amp; incoming PhD students, in any area of AI/ML, who are interested in making the long-term, large-scale impacts of AI a focus of their work.
The details:
Current and incoming PhD students may apply.
Up to 5 years of PhD support with the possibility of renewal for subsequent years
Students with pre-existing funding sources who find the mission and community of the Fellows Program appealing are welcome to apply
Annual support of $40,000 stipend, payment of tuition and fees, and $10,000 for travel, equipment, and other research expenses
Applications are due by October 25, 2019 at 11:59 PM Pacific time
In a note about this fellowship, a representative of the Open Philanthropy Project wrote: &ldquo;We are committed to fostering a culture of inclusion, and encourage individuals with diverse backgrounds and experiences to apply; we especially encourage applications from women and minorities.&rdquo;&nbsp;&nbsp;&nbsp;Find out more about the Fellowship here (Open Philanthropy website).
####################################################
Small drones with big brains: Harvard researchers apply deep RL to a &lsquo;nanodrone&rsquo;:&hellip;No GPS? That won&rsquo;t be a problem soon, once we have smart drones&hellip;One of the best things that the nuclear disaster at Fukushima did for the world was highlight just how lacking contemporary robotics was: we could have avoided a full meltdown if we&rsquo;d been able to get a robot or a drone into the facility. New research from Harvard, Google, Delft University, and the University of Texas at Austin suggests how we might make smart drones that can autonomously navigate in places where they might not have GPS. It&rsquo;s a first step to developing the sorts of systems needed to be able to rapidly map and understand the sites of various disasters, and also &ndash; as with many omni-use AI technologies &ndash; a prerequisite for low-cost, lightweight, weapons systems.&nbsp;
What they&rsquo;ve done: &ldquo;We introduce the first deep reinforcement learning (RL) based source-seeking nano-drone that is fully autonomous,&rdquo; the researchers write. The drone is trained to seek a light source, and uses light sensors to help it triangulate this, as well as an optical flow-based sensor for flight stability. The drone is trained using the Deep Q-Network (DQN) algorithm in a simulator with the objective of closing the distance between itself and a light source.&nbsp;
Shrinking network sizes: After training, they shrink down the resulting network (to 3kb, via quantization) and run it in the real world on a CrazyFlie nanodrone equipped with a CortexM4 chip &ndash; this is pretty impressive stuff, given the relative immaturity of RL for robot operation and the teeny-tiny compute envelope. &ldquo;While we focus exclusively on light-seeking as our application in this paper, we believe that the general methodology we have developed for deep reinforcement learning-based source seeking&hellip; can be readily extended to other (source seeking) applications as well, they write.&nbsp;
How well does it work? The researchers test out the drone in a bunch of different scenarios and average a success rate of 80% across 105 flight tests. In real world tests, the drone is able to deal with a variety of obstacles being introduced, as well as variations in its own position and the position of the lightsource. Now, 80% is a long way from good enough to use in a life or death situation, but it is meaningful enough to make this line of research worth paying attention to.
Why this matters: I think that in the next five years we&rsquo;re going to see a revolution sweep across the drone industry as researchers figure out how to cram increasingly sophisticated, smart capabilities onto drones ranging from the very big to the very small. It&rsquo;s encouraging to see researchers try to develop ultra-efficient approaches that can work on tiny drones with small compute budgets.&nbsp;&nbsp;&nbsp;Read more: Learning to Seek: Autonomous Source Seeking with Deep Reinforcement Learning Onboard a Nano Drone Microcontroller (Arxiv).&nbsp;&nbsp;&nbsp;Get the code for the research here (GitHub).&nbsp;&nbsp;&nbsp;Watch a video of the drone in action here (Harvard Edge Computing, YouTube).
####################################################
First we could use AI to search over text, then images, now: Code?&hellip;Maybe, just maybe, GitHub&rsquo;s &lsquo;CodeSearchNet&rsquo; dataset could help us develop something smarter than &lsquo;combing through StackOverflow&rsquo;&hellip;Today, search tools help us find words and images that are similar to our query, but have very little overlap (e.g, we can ask a search engine for &ldquo;what is the book with the big whale in it?&rdquo; and receive the answer &lsquo;Moby Dick&rsquo;, even though those words don&rsquo;t appear in the original query). Doing the same thing for code is really difficult &ndash; if you search &lsquo;read JSON data&rsquo; and you&rsquo;re unlikely to get nearl…"

---

### Import AI: 166: Dawn of the misleading ‘sophistbots’; $50k a year for studying long-term impacts of AI; and squeezing an RL drone policy into 3kb

Will powerful AI make the Turing Test obsolete?&hellip;And if it does, what do we do about it?&hellip;The Turing Test &ndash; judging how sophisticated a machine is, by seeing if it can convince a person that it is a human &ndash; looms large in pop culture discussion about AI. What happens if we have systems today that can pass the Turing Test, but which aren&rsquo;t actually that intelligent? That&rsquo;s something that has started to happen recently with systems that a human interfaces with via text chat. Now, new research from Stanford University, Pennsylvania State University, and the University of Toronto, explores how increasingly advanced so-called &lsquo;sophistbots&rsquo; might influence society.
The problems of &lsquo;sophisbots&rsquo;: The researchers imagine what the future of social media might look like, given recent advances in the ability for AI systems to generate synthetic media. In particular, they imagine social media ruled by &ldquo;sophisbots&rdquo;. They foresee a future where these bots are constantly &ldquo;running in the ether of social media or other infrastructure&hellip;not bound by geography, culture or conscience.&rdquo;&nbsp;
So, what do we do? Technical solutions: Machine learning researchers should develop technical tools to help spot machines posing as humans, and should invest in work to detect the telltale signs of AI-generated things, along with systems to track down the provenance of content to be able to guarantee that something is &lsquo;real&rsquo;, and tools to make it easy for regular people to indicate that the content they themselves are putting online is authentic and not bot-generated.&nbsp;&nbsp;&nbsp;Policy approaches: We need to develop &ldquo;public policy, legal, and normative frameworks for managing the malicious applications of technology in conjunction with efforts to refine it,&rdquo; they write. &ldquo;Let us as a technical community commit ourselves to embracing and addressing these challenges as readily as we do the fascinating and exciting new uses of intelligent systems&rdquo;.
Why this matters: How we deal with the future of synthetic content will define the nature of &lsquo;truth&rsquo; in society, which will ultimately define everything else. So, no pressure.&nbsp;&nbsp;&nbsp;Read more: How Relevant is the Turing Test in the Age of Sophisbots (Arxiv).&nbsp;
####################################################
Do Octopuses dream of electric sheep?Apropos of nothing, here is a film of an octopus changing colors while sleeping.&nbsp;&nbsp;&nbsp;View the sleeping octopus here (Twitter).
####################################################
PHD student? Want $50k a year to study the long-term impacts of AI? Read on!&hellip;Check out the Open Philanthropy Project&rsquo;s &lsquo;AI Fellowship&rsquo;&hellip;$50k for up to five years, with possibility of renewal&hellip;Applications are now open for the Open Phil AI Fellowship. This program extends full support to a community of current &amp; incoming PhD students, in any area of AI/ML, who are interested in making the long-term, large-scale impacts of AI a focus of their work.
The details:
Current and incoming PhD students may apply.
Up to 5 years of PhD support with the possibility of renewal for subsequent years
Students with pre-existing funding sources who find the mission and community of the Fellows Program appealing are welcome to apply
Annual support of $40,000 stipend, payment of tuition and fees, and $10,000 for travel, equipment, and other research expenses
Applications are due by October 25, 2019 at 11:59 PM Pacific time
In a note about this fellowship, a representative of the Open Philanthropy Project wrote: &ldquo;We are committed to fostering a culture of inclusion, and encourage individuals with diverse backgrounds and experiences to apply; we especially encourage applications from women and minorities.&rdquo;&nbsp;&nbsp;&nbsp;Find out more about the Fellowship here (Open Philanthropy website).
####################################################
Small drones with big brains: Harvard researchers apply deep RL to a &lsquo;nanodrone&rsquo;:&hellip;No GPS? That won&rsquo;t be a problem soon, once we have smart drones&hellip;One of the best things that the nuclear disaster at Fukushima did for the world was highlight just how lacking contemporary robotics was: we could have avoided a full meltdown if we&rsquo;d been able to get a robot or a drone into the facility. New research from Harvard, Google, Delft University, and the University of Texas at Austin suggests how we might make smart drones that can autonomously navigate in places where they might not have GPS. It&rsquo;s a first step to developing the sorts of systems needed to be able to rapidly map and understand the sites of various disasters, and also &ndash; as with many omni-use AI technologies &ndash; a prerequisite for low-cost, lightweight, weapons systems.&nbsp;
What they&rsquo;ve done: &ldquo;We introduce the first deep reinforcement learning (RL) based source-seeking nano-drone that is fully autonomous,&rdquo; the researchers write. The drone is trained to seek a light source, and uses light sensors to help it triangulate this, as well as an optical flow-based sensor for flight stability. The drone is trained using the Deep Q-Network (DQN) algorithm in a simulator with the objective of closing the distance between itself and a light source.&nbsp;
Shrinking network sizes: After training, they shrink down the resulting network (to 3kb, via quantization) and run it in the real world on a CrazyFlie nanodrone equipped with a CortexM4 chip &ndash; this is pretty impressive stuff, given the relative immaturity of RL for robot operation and the teeny-tiny compute envelope. &ldquo;While we focus exclusively on light-seeking as our application in this paper, we believe that the general methodology we have developed for deep reinforcement learning-based source seeking&hellip; can be readily extended to other (source seeking) applications as well, they write.&nbsp;
How well does it work? The researchers test out the drone in a bunch of different scenarios and average a success rate of 80% across 105 flight tests. In real world tests, the drone is able to deal with a variety of obstacles being introduced, as well as variations in its own position and the position of the lightsource. Now, 80% is a long way from good enough to use in a life or death situation, but it is meaningful enough to make this line of research worth paying attention to.
Why this matters: I think that in the next five years we&rsquo;re going to see a revolution sweep across the drone industry as researchers figure out how to cram increasingly sophisticated, smart capabilities onto drones ranging from the very big to the very small. It&rsquo;s encouraging to see researchers try to develop ultra-efficient approaches that can work on tiny drones with small compute budgets.&nbsp;&nbsp;&nbsp;Read more: Learning to Seek: Autonomous Source Seeking with Deep Reinforcement Learning Onboard a Nano Drone Microcontroller (Arxiv).&nbsp;&nbsp;&nbsp;Get the code for the research here (GitHub).&nbsp;&nbsp;&nbsp;Watch a video of the drone in action here (Harvard Edge Computing, YouTube).
####################################################
First we could use AI to search over text, then images, now: Code?&hellip;Maybe, just maybe, GitHub&rsquo;s &lsquo;CodeSearchNet&rsquo; dataset could help us develop something smarter than &lsquo;combing through StackOverflow&rsquo;&hellip;Today, search tools help us find words and images that are similar to our query, but have very little overlap (e.g, we can ask a search engine for &ldquo;what is the book with the big whale in it?&rdquo; and receive the answer &lsquo;Moby Dick&rsquo;, even though those words don&rsquo;t appear in the original query). Doing the same thing for code is really difficult &ndash; if you search &lsquo;read JSON data&rsquo; and you&rsquo;re unlikely to get nearl…