---

layout: post
category: product
title: "Import AI 171: When will robotics have its ImageNet moment?; fooling surveillance AI with an ‘adversarial t-shirt’, and Stanford calls for $12bn a year in funding for a US national endeavor"
date: 2019-11-04 17:16:25
link: https://vrhk.co/2NDJHb9
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "What do we mean when we say a machine can &ldquo;understand&rdquo; something?&hellip;And does it matter if we do or don&rsquo;t know what we mean here?&hellip;AI professor Tom Dietterich has tackled the thorny question of trying to define what it means for a machine to &ldquo;understand&rdquo; something &ndash; by saying maybe this question doesn&rsquo;t matter.&nbsp;
Who cares about understanding? &ldquo;I believe we should pursue advances in the science and technology of AI without engaging in debates about what counts as &ldquo;genuine&rdquo; understanding,&rdquo; he says. &ldquo;I encourage us instead to focus on which system capabilities we should be trying to achieve in the next 5, 10, or 50 years&rdquo;.&nbsp;
Why this matters: One of the joys and problems with AI is how broad a subject it is, but this is also a source of tension &ndash; I think the specific tension comes from the mushing together of a community that runs on an engineering-centric model of progress where researchers compete with eachother to iteratively hill climb on various state-of-the-art leaderboards, and a more philosophical community that wants to take a step back and ask fundamental questions like what it may mean to &ldquo;understand&rdquo; things and whether today&rsquo;s systems exhibit this or not. I think this is a productive tension, but it can sometimes yield arguments or debates that seem like sideshows to the main event of building iteratively more intelligent systems.&nbsp;&nbsp;&nbsp;&ldquo;We must suppress the hype surrounding new advances, and we must objectively measure the ways in which our systems do and do not understand their users, their goals, and the broader world in which they operate,&rdquo; he writes. &ldquo;Let&rsquo;s stop dismissing our successes as &ldquo;fake&rdquo; and not &ldquo;genuine&rdquo;, and lets continue to move forward with honesty and productive self-criticism&rdquo;.&nbsp;&nbsp;&nbsp;Read more: What does it mean for a machine to &ldquo;understand&rdquo;? (Tom Dietterich, Medium).&nbsp;
####################################################
What&rsquo;s the secret to creating a strong American AI ecosystem? $12 billion a year, say Stanford leaders:&hellip;Policy proposal calls for education, research, and entrepreneurial funding&hellip;If the American government wants the USA to lead in AI, then the government should invest $12 billion into AI every year for at least a decade, according to a policy proposal from Fei-Fei Li and John Etchemendy &ndash; directors of Stanford&rsquo;s Human-Centered Artificial Intelligence initiative.
How to spend $12 billion a year: Specifically, the government should invest $7 billion a year into &ldquo;public research to pursue the next generation of AI breakthroughs&rdquo;, along with $3 billion a year into education and $2 billion into funds to support early-stage AI entrepreneurs. To put these numbers into perspective, a NITRD report recently estimated that the federal government budgeted about $1 billion a year in non-defense programs related to AI, so the Stanford proposal is calling for a significant increase in AI spending, however you slice and dice the figures.&nbsp;
Money + principles: Along with this, the directors ask the US government to &ldquo;implement clear, actionable international standards and guidelines for the ethical use of AI&rdquo;. (In fairness to the US government, the government has participated in the creation of the OECD AI principles, which were adopted in 2019 by OECD member countries and other states, including Brazil, Peru, and Romania.)
Why this matters: The 21st century is the era of the industrialization of AI, and the industrialization of AI demands capital in the same way industrialization in the 18th and 19th centuries demanded capital. Therefore, if governments want to lead in AI, they&rsquo;ll need to dramatically increase spending on fundamental AI research as well as initiatives like better AI education. In the words of commentators of sports matches when a team is in a good position at the start of the second half of the game: it&rsquo;s the US&rsquo;s game to lose!Read more: We Need a National Vision for AI (Human-Centered Artificial Intelligence).Read more: The Networking and Information Technology Research &amp; Development Program Supplement to the President&rsquo;s FY2020 Budget (, PDF).&nbsp;
####################################################
Fundamental failures and machine learning:&hellip;Towards a taxonomy of machine failures&hellip;Researchers with the Universita della Svizzera Italiana in Switzerland have put together a taxonomy of some of the common failures seen in AI systems programmed in TensorFlow, PyTorch, and Keras. The difference with this taxonomy is the amount of research that has gone into it: to build it, the researchers analyzed&nbsp; 477 StackOverflow discussions, 271 issues and pull requests (PRs), 311 commits from GitHub repositories, and conducted interviews with 20 researchers and practitioners.&nbsp;
A taxonomy of failure: So, what failures are common in deep learning? There are around five top level categories, 3 of which are divided into subcategories. These are:
Model: The ML model itself is, unsurprisingly, a common source of failures, with developers frequently running into failures that occur at the level of a layer within the network. These include: problems relating to missing or redundant layers, incorrect layer properties (eg, sample size, input/output format, etc), and activation functions.
Training: Training runs are finicky, problem-laden things, and the common failures here including bad hyperparameter selection, misspecified loss functions, bad data splits between training and testing, optimiser problems, bad training data, crappy training procedures (eg, poor memory management during training), and more.&nbsp;
GPU usage: As anyone who has spent hours fiddling around with NVIDIA drivers can attest, GPUs are machines sent from hell to drive AI researchers mad. Faustian boxes, if you will. Have you ever seen someone with multiple PHDs break down after spending half a day trying to de-bug a problem caused by an NVIDIA card&rsquo;s software playing funny games with a Linux distro? I have. (AMD: Please ramp up your AI GPU business faster to provide better competition to NVIDIA here).&nbsp;
API: These problems are what happens when developers use APIs badly, or improperly.&nbsp;
Tensors &amp; Inputs: Misshapen tensors are a frequent problem, as are mis-specified inputs.
Why this matters: For AI to industrialize, AI processes need to become more repeatable and describable, in the same way that artisanal manufacturing processes were transformed into repeatable documented processes via Taylorism. Papers like this create more pressure for standardization within AI, which prefigures industrialization and societal-scale deployments.&nbsp;&nbsp;&nbsp;Read more: Taxonomy of Real Faults in Deep Learning Systems (Arxiv).
####################################################
Want your robot to be friends with people? You might want this dataset:&hellip;JackRabbot dataset comes with benchmarks, more than an hour of data&hellip;Researchers with the Stanford Vision and Learning Laboratory have built JRDB, a robot-collected dataset meant to help researchers develop smarter, more social robots. The dataset consists of tons of video footage recorded by the Stanford-developed &lsquo;JackRabbot&lsquo; robot &ldquo;social navigation robot&rdquo; as it travels around campus, with detailed annotations of all the people it encounters enroute. Ideally, JRDB can help us build robots that can navigate the world without crashing into the people around them. Seems useful!
What&rsquo;s special about the data? JRDB data consists of 54 action sequences with the following data for each sequence: Video streams at 15fps from stereo cylindrical 360-degree cameras; continuous 3D point clouds gathered via 2 velodyne LiDAR scanners; line 3D point cloud…"

---

### Import AI 171: When will robotics have its ImageNet moment?; fooling surveillance AI with an ‘adversarial t-shirt’, and Stanford calls for $12bn a year in funding for a US national endeavor

What do we mean when we say a machine can &ldquo;understand&rdquo; something?&hellip;And does it matter if we do or don&rsquo;t know what we mean here?&hellip;AI professor Tom Dietterich has tackled the thorny question of trying to define what it means for a machine to &ldquo;understand&rdquo; something &ndash; by saying maybe this question doesn&rsquo;t matter.&nbsp;
Who cares about understanding? &ldquo;I believe we should pursue advances in the science and technology of AI without engaging in debates about what counts as &ldquo;genuine&rdquo; understanding,&rdquo; he says. &ldquo;I encourage us instead to focus on which system capabilities we should be trying to achieve in the next 5, 10, or 50 years&rdquo;.&nbsp;
Why this matters: One of the joys and problems with AI is how broad a subject it is, but this is also a source of tension &ndash; I think the specific tension comes from the mushing together of a community that runs on an engineering-centric model of progress where researchers compete with eachother to iteratively hill climb on various state-of-the-art leaderboards, and a more philosophical community that wants to take a step back and ask fundamental questions like what it may mean to &ldquo;understand&rdquo; things and whether today&rsquo;s systems exhibit this or not. I think this is a productive tension, but it can sometimes yield arguments or debates that seem like sideshows to the main event of building iteratively more intelligent systems.&nbsp;&nbsp;&nbsp;&ldquo;We must suppress the hype surrounding new advances, and we must objectively measure the ways in which our systems do and do not understand their users, their goals, and the broader world in which they operate,&rdquo; he writes. &ldquo;Let&rsquo;s stop dismissing our successes as &ldquo;fake&rdquo; and not &ldquo;genuine&rdquo;, and lets continue to move forward with honesty and productive self-criticism&rdquo;.&nbsp;&nbsp;&nbsp;Read more: What does it mean for a machine to &ldquo;understand&rdquo;? (Tom Dietterich, Medium).&nbsp;
####################################################
What&rsquo;s the secret to creating a strong American AI ecosystem? $12 billion a year, say Stanford leaders:&hellip;Policy proposal calls for education, research, and entrepreneurial funding&hellip;If the American government wants the USA to lead in AI, then the government should invest $12 billion into AI every year for at least a decade, according to a policy proposal from Fei-Fei Li and John Etchemendy &ndash; directors of Stanford&rsquo;s Human-Centered Artificial Intelligence initiative.
How to spend $12 billion a year: Specifically, the government should invest $7 billion a year into &ldquo;public research to pursue the next generation of AI breakthroughs&rdquo;, along with $3 billion a year into education and $2 billion into funds to support early-stage AI entrepreneurs. To put these numbers into perspective, a NITRD report recently estimated that the federal government budgeted about $1 billion a year in non-defense programs related to AI, so the Stanford proposal is calling for a significant increase in AI spending, however you slice and dice the figures.&nbsp;
Money + principles: Along with this, the directors ask the US government to &ldquo;implement clear, actionable international standards and guidelines for the ethical use of AI&rdquo;. (In fairness to the US government, the government has participated in the creation of the OECD AI principles, which were adopted in 2019 by OECD member countries and other states, including Brazil, Peru, and Romania.)
Why this matters: The 21st century is the era of the industrialization of AI, and the industrialization of AI demands capital in the same way industrialization in the 18th and 19th centuries demanded capital. Therefore, if governments want to lead in AI, they&rsquo;ll need to dramatically increase spending on fundamental AI research as well as initiatives like better AI education. In the words of commentators of sports matches when a team is in a good position at the start of the second half of the game: it&rsquo;s the US&rsquo;s game to lose!Read more: We Need a National Vision for AI (Human-Centered Artificial Intelligence).Read more: The Networking and Information Technology Research &amp; Development Program Supplement to the President&rsquo;s FY2020 Budget (, PDF).&nbsp;
####################################################
Fundamental failures and machine learning:&hellip;Towards a taxonomy of machine failures&hellip;Researchers with the Universita della Svizzera Italiana in Switzerland have put together a taxonomy of some of the common failures seen in AI systems programmed in TensorFlow, PyTorch, and Keras. The difference with this taxonomy is the amount of research that has gone into it: to build it, the researchers analyzed&nbsp; 477 StackOverflow discussions, 271 issues and pull requests (PRs), 311 commits from GitHub repositories, and conducted interviews with 20 researchers and practitioners.&nbsp;
A taxonomy of failure: So, what failures are common in deep learning? There are around five top level categories, 3 of which are divided into subcategories. These are:
Model: The ML model itself is, unsurprisingly, a common source of failures, with developers frequently running into failures that occur at the level of a layer within the network. These include: problems relating to missing or redundant layers, incorrect layer properties (eg, sample size, input/output format, etc), and activation functions.
Training: Training runs are finicky, problem-laden things, and the common failures here including bad hyperparameter selection, misspecified loss functions, bad data splits between training and testing, optimiser problems, bad training data, crappy training procedures (eg, poor memory management during training), and more.&nbsp;
GPU usage: As anyone who has spent hours fiddling around with NVIDIA drivers can attest, GPUs are machines sent from hell to drive AI researchers mad. Faustian boxes, if you will. Have you ever seen someone with multiple PHDs break down after spending half a day trying to de-bug a problem caused by an NVIDIA card&rsquo;s software playing funny games with a Linux distro? I have. (AMD: Please ramp up your AI GPU business faster to provide better competition to NVIDIA here).&nbsp;
API: These problems are what happens when developers use APIs badly, or improperly.&nbsp;
Tensors &amp; Inputs: Misshapen tensors are a frequent problem, as are mis-specified inputs.
Why this matters: For AI to industrialize, AI processes need to become more repeatable and describable, in the same way that artisanal manufacturing processes were transformed into repeatable documented processes via Taylorism. Papers like this create more pressure for standardization within AI, which prefigures industrialization and societal-scale deployments.&nbsp;&nbsp;&nbsp;Read more: Taxonomy of Real Faults in Deep Learning Systems (Arxiv).
####################################################
Want your robot to be friends with people? You might want this dataset:&hellip;JackRabbot dataset comes with benchmarks, more than an hour of data&hellip;Researchers with the Stanford Vision and Learning Laboratory have built JRDB, a robot-collected dataset meant to help researchers develop smarter, more social robots. The dataset consists of tons of video footage recorded by the Stanford-developed &lsquo;JackRabbot&lsquo; robot &ldquo;social navigation robot&rdquo; as it travels around campus, with detailed annotations of all the people it encounters enroute. Ideally, JRDB can help us build robots that can navigate the world without crashing into the people around them. Seems useful!
What&rsquo;s special about the data? JRDB data consists of 54 action sequences with the following data for each sequence: Video streams at 15fps from stereo cylindrical 360-degree cameras; continuous 3D point clouds gathered via 2 velodyne LiDAR scanners; line 3D point cloud…