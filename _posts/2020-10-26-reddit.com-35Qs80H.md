---

layout: post
category: threads
title: "[D] Paper Explained - Rethinking Attention with Performers (Full Video Analysis)"
date: 2020-10-26 19:13:36
link: https://vrhk.co/35Qs80H
image: https://external-preview.redd.it/0zipeRXXtEvzRYVJU2ysj3aqhUrdxniv_IQwMmIGP4s.jpg?width=480&height=251.308900524&auto=webp&crop=480:251.308900524,smart&s=50e2b85149094a9963fae9f8ccdad4d22dd8a943
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "[<https://youtu.be/xJrKIPwVwGM>](<https://youtu.be/xJrKIPwVwGM>) Transformers have huge memory and compute requirements because they construct an..."

---

### [D] Paper Explained - Rethinking Attention with Performers (Full Video Analysis)

[<https://youtu.be/xJrKIPwVwGM>](<https://youtu.be/xJrKIPwVwGM>) Transformers have huge memory and compute requirements because they construct an...