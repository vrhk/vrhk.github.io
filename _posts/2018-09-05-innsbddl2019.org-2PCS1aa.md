---

layout: post
category: threads
title: "Tutorials"
date: 2018-09-05 09:50:50
link: https://vrhk.co/2PCS1aa
image: https://innsbddl2019.files.wordpress.com/2018/08/oneto.png?fit=200%2C150
domain: innsbddl2019.org
author: "innsbddl2019"
icon: https://innsbddl2019.files.wordpress.com/2017/12/neural-logo-transparent-cropped.png?w=89
excerpt: "Tutorials

Davide Bacciu (University of Pisa),&nbsp;Deep Learning for Graphs
Silvia Chiappa (DeepMind),&nbsp;Luca Oneto (University of Genoa),&nbsp;Fairness in Machine Learning
Claudio Gallicchio (University of Pisa),&nbsp;Simone Scardapane (Sapienza University of Rome),&nbsp;Deep Randomized Neural Networks
V&#283;ra K&#367;rkov&aacute; (Czech Academy of Sciences),&nbsp;Complexity of Shallow and Deep Networks
Danilo P. Mandic,&nbsp;Ilia Kisil, and Giuseppe G. Calvi&nbsp;(Imperial College London),&nbsp;Tensor Decompositions and Applications. Blessing of Dimensionality
German I. Parisi and Stefan Wermter (University of Hamburg),&nbsp;Continual Lifelong Learning with Neural Networks

Deep Learning for Graphs
Davide Bacciu (University of Pisa)

The tutorial will introduce the emerging field of deep learning for graphs and its applications to bioinformatics, chemistry and vision.&nbsp;&nbsp;Dealing with graph data requires learning models capable of adapting to structured samples of varying size and topology, capturing the relevant structural patterns to perform predictive and explorative tasks while maintaining the efficiency and scalability necessary to process large scale networks. The tutorial will first introduce some foundational aspects of learning with structured data samples and will survey some seminal neural network models for graphs. Then it will focus on the most recent advancements in terms of deep learning for network and graph data, including learning structure embeddings, graph convolutions, attentional models and structure generation.&nbsp;&nbsp;The tutorial is targeted to both early career researchers seeking ideas for their doctoral studies as well as to more advanced stage researchers looking to enter into a lively field of deep learning and seeking both foundational knowledge as well as a perspective on current research.
BIOGRAPHY
Davide Bacciu is Assistant Professor at the Computer Science Department, University of Pisa. The core of his research is on Machine Learning (ML) and deep learning models for structured data processing, including sequences, trees and graphs. He is the PI of an Italian National project on ML for structured data. He has been teaching courses of Artificial Intelligence (AI) and ML at undergraduate and graduate levels since 2010. He is the Secretary of the Italian Association for Artificial Intelligence (AI*IA), member of the&nbsp;&nbsp;IEEE CIS Task Force on Deep Learning and Associate Editor of the IEEE Transactions on Neural Networks and Learning Systems.
Fairness in Machine Learning
Silvia Chiappa (DeepMind),&nbsp;Luca Oneto (University of Genoa)
AI systems and products are reaching society at large and in many aspects of everyday life, including healthcare, criminal justice, education, and finance. This phenomenon has been accompanied by an increase in concern about the ethical issues that may rise from the adoption of these technologies. In response to this concern, a new area of machine learning has recently emerged that studies how to address disparate treatment caused by algorithmic errors and bias in the data. The central question is how to ensure that the learned model does not treat subgroups in the population &lsquo;unfairly&rsquo;. While the design of solutions to this issue requires an interdisciplinary effort, fundamental progress can only be achieved through a radical change in the machine learning paradigm.
In this tutorial, we will describe the state of the art on ML fairness as well as discuss currently unexplored areas of research. We will use the framework of graphical models to provide a clear and intuitive formalization and characterization of the subject.
BIOGRAPHY
Silvia Chiappa is a senior research scientist in Machine Learning at DeepMind, where she works on ML fairness and deep probabilistic temporal models. Silvia received a Diploma di Laurea in Mathematics from University of Bologna and a PhD in Statistical Machine Learning from &Eacute;cole Polytechnique F&eacute;d&eacute;rale de Lausanne. Before joining DeepMind, Silvia worked in several Machine Learning and Statistics research groups: The Empirical Inference Department at the Planck Institute for Intelligent Systems,&nbsp; the Machine Intelligence and Perception Group at Microsoft Research Cambridge, and the Statistical Laboratory, University of Cambridge. Silvia&rsquo;s research interests are based around Bayesian and causal reasoning, graphical models, approximate inference, time-series models, and ML fairness.
Luca Oneto was born in Rapallo, Italy in 1986. He received his BSc and MSc in Electronic Engineering at the University of Genoa, Italy respectively in 2008 and 2010. In 2014 he received his PhD from the same university in School of Sciences and Technologies for Knowledge and Information Retrieval with the thesis &ldquo;Learning Based On Empirical Data&rdquo;. In 2017 he obtained the Italian National Scientific Qualification for the role of Associate Professor in Computer Engineering and in 2018 the one in Computer Science.
He is currently an Assistant Professor in Computer Engineering at University of Genoa with particular interests in Statistical Learning Theory and Data Science.
Deep Randomized Neural Networks
Claudio Gallicchio (University of Pisa),&nbsp;Simone Scardapane (Sapienza University of Rome)
Randomized Neural Networks explore the behavior of neural systems where the majority of connections are fixed, either in a stochastic or a deterministic fashion. Typical examples of such systems consist of multi-layered neural network architectures where the connections to the hidden layer(s) are left untrained after initialization.
Limiting the training algorithms to operate on a reduced set of weights inherently characterizes the class of Randomized Neural Networks with a number of intriguing features. Among them, the extreme efficiency of the resulting learning processes is undoubtedly a striking advantage with respect to fully trained architectures. Besides, despite the involved simplifications, randomized neural systems possess remarkable properties both in practice, achieving state-of-the-art results in multiple domains, and theoretically, allowing to analyze intrinsic properties of neural architectures (e.g. before training of the hidden layers&rsquo; connections). In recent years, the study of Randomized Neural Networks has been extended towards deep architectures, opening new research directions to the design of effective yet extremely efficient deep learning models in vectorial as well as in more complex data domains.
This tutorial will cover all the major aspects regarding the design and analysis of Randomized Neural Networks, and some of the key results with respect to their approximation capabilities. In particular, the tutorial will first introduce the fundamentals of randomized neural models in the context of feed-forward networks (i.e., Random Vector Functional Link and equivalent models), convolutional filters, and recurrent systems (i.e., Reservoir Computing networks). Then, it will focus specifically on recent results in the domain of deep randomized systems, and their application to structured domains.
BIOGRAPHY
Claudio Gallicchio is Assistant Professor at the Department of Computer Science, University of Pisa, within the Computational Intelligence &amp; Machine Learning Group (CIML). He is chair of the IEEE CIS Task Force on Reservoir Computing, and member of the IEEE CIS Task Force on Deep Learning. Claudio Gallicchio has co-organized several special sessions on Randomized Neural Networks methodologies in major international conferences, and since 2016 is co-organizer of the Italian Workshop on Machine Learning and Data Mining (). He serves as member of several program committees of conferences and workshops in Machine Learning and Artificial Intelligence. His research interests include Machine Learning, Deep Learning, Randomized Neural Networks, Reservoir Computing, Recurrent and Recursive Neur…"

---

### Tutorials

Tutorials

Davide Bacciu (University of Pisa),&nbsp;Deep Learning for Graphs
Silvia Chiappa (DeepMind),&nbsp;Luca Oneto (University of Genoa),&nbsp;Fairness in Machine Learning
Claudio Gallicchio (University of Pisa),&nbsp;Simone Scardapane (Sapienza University of Rome),&nbsp;Deep Randomized Neural Networks
V&#283;ra K&#367;rkov&aacute; (Czech Academy of Sciences),&nbsp;Complexity of Shallow and Deep Networks
Danilo P. Mandic,&nbsp;Ilia Kisil, and Giuseppe G. Calvi&nbsp;(Imperial College London),&nbsp;Tensor Decompositions and Applications. Blessing of Dimensionality
German I. Parisi and Stefan Wermter (University of Hamburg),&nbsp;Continual Lifelong Learning with Neural Networks

Deep Learning for Graphs
Davide Bacciu (University of Pisa)

The tutorial will introduce the emerging field of deep learning for graphs and its applications to bioinformatics, chemistry and vision.&nbsp;&nbsp;Dealing with graph data requires learning models capable of adapting to structured samples of varying size and topology, capturing the relevant structural patterns to perform predictive and explorative tasks while maintaining the efficiency and scalability necessary to process large scale networks. The tutorial will first introduce some foundational aspects of learning with structured data samples and will survey some seminal neural network models for graphs. Then it will focus on the most recent advancements in terms of deep learning for network and graph data, including learning structure embeddings, graph convolutions, attentional models and structure generation.&nbsp;&nbsp;The tutorial is targeted to both early career researchers seeking ideas for their doctoral studies as well as to more advanced stage researchers looking to enter into a lively field of deep learning and seeking both foundational knowledge as well as a perspective on current research.
BIOGRAPHY
Davide Bacciu is Assistant Professor at the Computer Science Department, University of Pisa. The core of his research is on Machine Learning (ML) and deep learning models for structured data processing, including sequences, trees and graphs. He is the PI of an Italian National project on ML for structured data. He has been teaching courses of Artificial Intelligence (AI) and ML at undergraduate and graduate levels since 2010. He is the Secretary of the Italian Association for Artificial Intelligence (AI*IA), member of the&nbsp;&nbsp;IEEE CIS Task Force on Deep Learning and Associate Editor of the IEEE Transactions on Neural Networks and Learning Systems.
Fairness in Machine Learning
Silvia Chiappa (DeepMind),&nbsp;Luca Oneto (University of Genoa)
AI systems and products are reaching society at large and in many aspects of everyday life, including healthcare, criminal justice, education, and finance. This phenomenon has been accompanied by an increase in concern about the ethical issues that may rise from the adoption of these technologies. In response to this concern, a new area of machine learning has recently emerged that studies how to address disparate treatment caused by algorithmic errors and bias in the data. The central question is how to ensure that the learned model does not treat subgroups in the population &lsquo;unfairly&rsquo;. While the design of solutions to this issue requires an interdisciplinary effort, fundamental progress can only be achieved through a radical change in the machine learning paradigm.
In this tutorial, we will describe the state of the art on ML fairness as well as discuss currently unexplored areas of research. We will use the framework of graphical models to provide a clear and intuitive formalization and characterization of the subject.
BIOGRAPHY
Silvia Chiappa is a senior research scientist in Machine Learning at DeepMind, where she works on ML fairness and deep probabilistic temporal models. Silvia received a Diploma di Laurea in Mathematics from University of Bologna and a PhD in Statistical Machine Learning from &Eacute;cole Polytechnique F&eacute;d&eacute;rale de Lausanne. Before joining DeepMind, Silvia worked in several Machine Learning and Statistics research groups: The Empirical Inference Department at the Planck Institute for Intelligent Systems,&nbsp; the Machine Intelligence and Perception Group at Microsoft Research Cambridge, and the Statistical Laboratory, University of Cambridge. Silvia&rsquo;s research interests are based around Bayesian and causal reasoning, graphical models, approximate inference, time-series models, and ML fairness.
Luca Oneto was born in Rapallo, Italy in 1986. He received his BSc and MSc in Electronic Engineering at the University of Genoa, Italy respectively in 2008 and 2010. In 2014 he received his PhD from the same university in School of Sciences and Technologies for Knowledge and Information Retrieval with the thesis &ldquo;Learning Based On Empirical Data&rdquo;. In 2017 he obtained the Italian National Scientific Qualification for the role of Associate Professor in Computer Engineering and in 2018 the one in Computer Science.
He is currently an Assistant Professor in Computer Engineering at University of Genoa with particular interests in Statistical Learning Theory and Data Science.
Deep Randomized Neural Networks
Claudio Gallicchio (University of Pisa),&nbsp;Simone Scardapane (Sapienza University of Rome)
Randomized Neural Networks explore the behavior of neural systems where the majority of connections are fixed, either in a stochastic or a deterministic fashion. Typical examples of such systems consist of multi-layered neural network architectures where the connections to the hidden layer(s) are left untrained after initialization.
Limiting the training algorithms to operate on a reduced set of weights inherently characterizes the class of Randomized Neural Networks with a number of intriguing features. Among them, the extreme efficiency of the resulting learning processes is undoubtedly a striking advantage with respect to fully trained architectures. Besides, despite the involved simplifications, randomized neural systems possess remarkable properties both in practice, achieving state-of-the-art results in multiple domains, and theoretically, allowing to analyze intrinsic properties of neural architectures (e.g. before training of the hidden layers&rsquo; connections). In recent years, the study of Randomized Neural Networks has been extended towards deep architectures, opening new research directions to the design of effective yet extremely efficient deep learning models in vectorial as well as in more complex data domains.
This tutorial will cover all the major aspects regarding the design and analysis of Randomized Neural Networks, and some of the key results with respect to their approximation capabilities. In particular, the tutorial will first introduce the fundamentals of randomized neural models in the context of feed-forward networks (i.e., Random Vector Functional Link and equivalent models), convolutional filters, and recurrent systems (i.e., Reservoir Computing networks). Then, it will focus specifically on recent results in the domain of deep randomized systems, and their application to structured domains.
BIOGRAPHY
Claudio Gallicchio is Assistant Professor at the Department of Computer Science, University of Pisa, within the Computational Intelligence &amp; Machine Learning Group (CIML). He is chair of the IEEE CIS Task Force on Reservoir Computing, and member of the IEEE CIS Task Force on Deep Learning. Claudio Gallicchio has co-organized several special sessions on Randomized Neural Networks methodologies in major international conferences, and since 2016 is co-organizer of the Italian Workshop on Machine Learning and Data Mining (). He serves as member of several program committees of conferences and workshops in Machine Learning and Artificial Intelligence. His research interests include Machine Learning, Deep Learning, Randomized Neural Networks, Reservoir Computing, Recurrent and Recursive Neur…