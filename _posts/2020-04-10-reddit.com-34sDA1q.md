---

layout: post
category: threads
title: "[R] Poor Man's BERT: Smaller and Faster Transformer Models"
date: 2020-04-10 13:17:32
link: https://vrhk.co/34sDA1q
image: https://external-preview.redd.it/0jAvZb-Ffm-pIIoSRhZmTOtqrUXTelW30_b6hNMj0ZA.jpg?width=420&height=219.895287958&auto=webp&crop=420:219.895287958,smart&s=9712c5b598fb311753ffdce3360ec63f6b3c9ac7
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "**ABSTRACT**: The ongoing neural revolution in Natural Language Processing has recently been dominated by large-scale pre-trained Transformer..."

---

### [R] Poor Man's BERT: Smaller and Faster Transformer Models

**ABSTRACT**: The ongoing neural revolution in Natural Language Processing has recently been dominated by large-scale pre-trained Transformer...