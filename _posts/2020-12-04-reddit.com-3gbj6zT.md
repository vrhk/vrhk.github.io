---

layout: post
category: threads
title: "[D] If Transformers are mean for sequential data, how to apply to speech if we don't have \"tokens\"?"
date: 2020-12-04 14:37:27
link: https://vrhk.co/3gbj6zT
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "We all know Transformer models have great performance on seq2seq problems, but all these models come with and embedding layers, and dealing with..."

---

### [D] If Transformers are mean for sequential data, how to apply to speech if we don't have "tokens"?

We all know Transformer models have great performance on seq2seq problems, but all these models come with and embedding layers, and dealing with...