---

layout: post
category: threads
title: "[R] A simple module consistently outperforms self-attention and Transformer model on main NMT datasets with SoTA performance."
date: 2019-11-24 20:17:28
link: https://vrhk.co/2XGNhWk
image: https://external-preview.redd.it/2qkne32qwo041.png?overlay-align=bottom,left&crop=554:177,smart&overlay-height=15p&overlay=%2Fwatermark%2Ft5_2r3gv.png%3Fs%3D25fde90502025a808e495a452fb2218b991321bd&width=554&height=177&auto=webp&s=7826c8ed7bd7fce211a152dac9c5c84f9c367236
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Posted in r/MachineLearning by u/stopwind • 2 points and 0 comments"

---

### [R] A simple module consistently outperforms self-attention and Transformer model on main NMT datasets with SoTA performance.

Posted in r/MachineLearning by u/stopwind • 2 points and 0 comments