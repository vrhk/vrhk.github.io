---

layout: post
category: product
title: "Import AI 212: Robots are getting smart; humans+machines = trouble, says DHS; a 10k product dataset"
date: 2020-08-31 18:16:31
link: https://vrhk.co/3hLTBoK
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s0.wp.com/i/webclip.png
excerpt: "Faster, robots! Move! Move! Move! Maybe the robot revolution is really imminent?&hellip;DeepMind shows one policy + multi-task training = robust robot RL&hellip;Researchers with Deepmind have used a single algorithm &ndash; Scheduled Auxiliary Control (SAC) &ndash; to get multiple simulated robots and a real robot to learn robust movement behaviors. That&rsquo;s notable compared to the state of the art a few years ago, when you might see a multitude of different algorithms used for a bunch of different robots. DeepMind did this without changing the reward functions across different robots. Their approach can learn to operate new robots in a couple of hours. Learning is easier when you&rsquo;re trying to learn a lot: DeepMind shows that it&rsquo;s more efficient to try and learn multiple skills for a robot at once, rather than learning skills in sequence. In other words, if you&rsquo;re trying to learn to walk forwards and backwards, it&rsquo;s more efficient to learn a little bit of walking forwards and then a little bit of working backwards and alternate till you&rsquo;ve got it down to a science, rather than just trying to learn to walk forward and perfecting that, then learning to move backward.  &nbsp; Hours of savings: DeepMind was able to learn a range of movements on one robot which took about 1590 episodes, netting out to around five hours of work. If they&rsquo;d tried to learn the same skills in a single task setting, they estimate it&rsquo;d take about 3050 episodes, adding another five hours. That&rsquo;s an encouraging sign with regard to both the robustness of SAC and the utility of multi-task learning. Which robots? DeepMind uses simulated and real robots made by HEBI Robotics, a spinout from Carnegie Mellon University. Why this matters: Papers like this give us a sense of how researchers, after probably half a decade of various experiments, are starting to turn robots+RL from a lab-borne curiosity to something that might be repeatable and reliable enough we can further develop techniques and inject smart robots into the world.  &nbsp; Read more: Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion (arXiv). Check out the video (YouTube).Learning a good per-robot policy? That&rsquo;s nice. How about learning one policy that works across a bunch of robots without retraining?&hellip;What if we treat a robot as an environment and its limbs as a multi-agent learning problem?&hellip;Here&rsquo;s a different approach to robotic control, from researchers with Berkeley, Google, and CMU &amp; Facebook AI Research. In One Policy to Control Them All &nbsp;the researchers build a system that lets them train a bunch of different AI agents in parallel, which yields a single flexible policy that generalizes to new (simulated) robots. How it works: They do this by trying to learn control policies that help different joints coordinate with eachother, then they share these controllers across all the motors/limbs of all the agents. &ldquo;Now the policies are fully modular, it&rsquo;s just like lego blocks,&rdquo; says one of the authors in a video about the research.&nbsp; Individual agents are able to learn to move coherently through the incorporation of a message passing approach, where the control policies for different limbs/motors propagate information to nearby limb/motor nodes&nbsp; in a cascade until they&rsquo;ve passed messages through the whole entity, which then tries to pass a prediction message back &ndash; by having the nodes propagate information and the agent try to predict their actions, the authors are able to inject some emergent coherence into the system. Message passing: The message passing approach is similar to how some multi-agent systems are trained, the authors note. This feels quite intuitive &ndash; if we want to train a bunch of agents to solve a task, you need to design a learning approach that means the agents figure out how to coordinate with eachother in an unsupervised way. Here, the same thing is happening when you treat the different limbs/actuators in a robot as a collection of distinct agents in a single world (the robot platform) &ndash; over time, you see them figure out how to coordinate with eachother to achieve an objective, like moving a robot.&nbsp;
Testing (in simulation): In tests on simulated MuJoCo robots, the researchers show their approach can outperform some basic multi-task baselines (similar to the sorts of powerful SAC models trained by DeepMind elsewhere in this issue of Import AI). They also show their approach can generalize to simulated robots different to what they were trained on, highlighting the potential robustness of this technique. (Though note that the policy fails if they dramatically alter the weight/friction of the limbs, or change the size of the creatures). &nbsp; Read more: One Policy to Control Them All: Shared Modular Policies for Agent-Agnostic Control (arXiv).  &nbsp; Find out more: ICML 2020 Oral Talk: One Policy to Control Them All (YouTube).###################################################Don&rsquo;t have a supercomputer to train your big AI model? No problem &ndash; use the Hivemind!&hellip;Folding@Home, but for AI&hellip;The era of large AI models is here, with systems like GPT-3, AlphaStar, and others started to yield interesting things via multi-million dollar training regimes. How can researchers with few financial resources compete with entities that can train large AI systems? One idea is to decentralize, specifically, developing software to make it easy to train large models via a vast sea of distributed computation. &nbsp; That&rsquo;s the idea behind Hivemind, a system to help people &lsquo;run crowdsourced deep learning using compute from volunteers or decentralized participants&rsquo;. Hivemind uses a decentralized Mixture-of-Experts approach, and the authors have tried to build some models using this approach, but note: &ldquo;one can train immensely large neural networks on volunteer hardware. However, reaching the full potential of this idea is a monumental task well outside the restrictions of one publication&rdquo;.Why this matters: Infrastructure &ndash; and who has access to it &ndash; is inherently political. In the 21st century, some forms of political power will accrue to the people that build computational infrastructures and those which can build artefacts on top of it. Systems like Hivemind hint at ways a broader set of people could gain access to large infrastructure, potentially altering who does and doesn&rsquo;t have political power as a consequence. &nbsp; Read more: Learning at Home (GitHub).  &nbsp; Read more: Learning@home: Crowdsourced Training of Large Neural networks using Decentralized Mixture-of-Experts (arXiv).###################################################Beware of how humans react to algorithmic recognitions, says DHS study:&hellip;Facial recognition test shows differences in how humans treat algo vs human recommendations&hellip;Researchers with the Maryland Test Facility (MdTF), a Department of Homeland Security-affiliated laboratory, have investigated how humans work in tandem with machines, when doing a facial recognition task. They tested about 300 volunteers in three groups of a hundred at the task of working out whether two greyscale pictures of people show the same person or a different person. One group got no prior information, while another group got suggested answers &ndash; priors &ndash;&nbsp; for the task provided by a human , and the final group got suggested answers from an AI. The test showed that the presence of a prior, unsurprisingly, improved performance. But humans treated computer-provided priors differently to human-provided priors&hellip;When we trust machines versus when we trust humans: The study shows that &ldquo;volunteers reported distrusting human identification ability more than computer identification ability&rdquo;, though notes that both sources led to similar ov…"

---

### Import AI 212: Robots are getting smart; humans+machines = trouble, says DHS; a 10k product dataset

Faster, robots! Move! Move! Move! Maybe the robot revolution is really imminent?&hellip;DeepMind shows one policy + multi-task training = robust robot RL&hellip;Researchers with Deepmind have used a single algorithm &ndash; Scheduled Auxiliary Control (SAC) &ndash; to get multiple simulated robots and a real robot to learn robust movement behaviors. That&rsquo;s notable compared to the state of the art a few years ago, when you might see a multitude of different algorithms used for a bunch of different robots. DeepMind did this without changing the reward functions across different robots. Their approach can learn to operate new robots in a couple of hours. Learning is easier when you&rsquo;re trying to learn a lot: DeepMind shows that it&rsquo;s more efficient to try and learn multiple skills for a robot at once, rather than learning skills in sequence. In other words, if you&rsquo;re trying to learn to walk forwards and backwards, it&rsquo;s more efficient to learn a little bit of walking forwards and then a little bit of working backwards and alternate till you&rsquo;ve got it down to a science, rather than just trying to learn to walk forward and perfecting that, then learning to move backward.  &nbsp; Hours of savings: DeepMind was able to learn a range of movements on one robot which took about 1590 episodes, netting out to around five hours of work. If they&rsquo;d tried to learn the same skills in a single task setting, they estimate it&rsquo;d take about 3050 episodes, adding another five hours. That&rsquo;s an encouraging sign with regard to both the robustness of SAC and the utility of multi-task learning. Which robots? DeepMind uses simulated and real robots made by HEBI Robotics, a spinout from Carnegie Mellon University. Why this matters: Papers like this give us a sense of how researchers, after probably half a decade of various experiments, are starting to turn robots+RL from a lab-borne curiosity to something that might be repeatable and reliable enough we can further develop techniques and inject smart robots into the world.  &nbsp; Read more: Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion (arXiv). Check out the video (YouTube).Learning a good per-robot policy? That&rsquo;s nice. How about learning one policy that works across a bunch of robots without retraining?&hellip;What if we treat a robot as an environment and its limbs as a multi-agent learning problem?&hellip;Here&rsquo;s a different approach to robotic control, from researchers with Berkeley, Google, and CMU &amp; Facebook AI Research. In One Policy to Control Them All &nbsp;the researchers build a system that lets them train a bunch of different AI agents in parallel, which yields a single flexible policy that generalizes to new (simulated) robots. How it works: They do this by trying to learn control policies that help different joints coordinate with eachother, then they share these controllers across all the motors/limbs of all the agents. &ldquo;Now the policies are fully modular, it&rsquo;s just like lego blocks,&rdquo; says one of the authors in a video about the research.&nbsp; Individual agents are able to learn to move coherently through the incorporation of a message passing approach, where the control policies for different limbs/motors propagate information to nearby limb/motor nodes&nbsp; in a cascade until they&rsquo;ve passed messages through the whole entity, which then tries to pass a prediction message back &ndash; by having the nodes propagate information and the agent try to predict their actions, the authors are able to inject some emergent coherence into the system. Message passing: The message passing approach is similar to how some multi-agent systems are trained, the authors note. This feels quite intuitive &ndash; if we want to train a bunch of agents to solve a task, you need to design a learning approach that means the agents figure out how to coordinate with eachother in an unsupervised way. Here, the same thing is happening when you treat the different limbs/actuators in a robot as a collection of distinct agents in a single world (the robot platform) &ndash; over time, you see them figure out how to coordinate with eachother to achieve an objective, like moving a robot.&nbsp;
Testing (in simulation): In tests on simulated MuJoCo robots, the researchers show their approach can outperform some basic multi-task baselines (similar to the sorts of powerful SAC models trained by DeepMind elsewhere in this issue of Import AI). They also show their approach can generalize to simulated robots different to what they were trained on, highlighting the potential robustness of this technique. (Though note that the policy fails if they dramatically alter the weight/friction of the limbs, or change the size of the creatures). &nbsp; Read more: One Policy to Control Them All: Shared Modular Policies for Agent-Agnostic Control (arXiv).  &nbsp; Find out more: ICML 2020 Oral Talk: One Policy to Control Them All (YouTube).###################################################Don&rsquo;t have a supercomputer to train your big AI model? No problem &ndash; use the Hivemind!&hellip;Folding@Home, but for AI&hellip;The era of large AI models is here, with systems like GPT-3, AlphaStar, and others started to yield interesting things via multi-million dollar training regimes. How can researchers with few financial resources compete with entities that can train large AI systems? One idea is to decentralize, specifically, developing software to make it easy to train large models via a vast sea of distributed computation. &nbsp; That&rsquo;s the idea behind Hivemind, a system to help people &lsquo;run crowdsourced deep learning using compute from volunteers or decentralized participants&rsquo;. Hivemind uses a decentralized Mixture-of-Experts approach, and the authors have tried to build some models using this approach, but note: &ldquo;one can train immensely large neural networks on volunteer hardware. However, reaching the full potential of this idea is a monumental task well outside the restrictions of one publication&rdquo;.Why this matters: Infrastructure &ndash; and who has access to it &ndash; is inherently political. In the 21st century, some forms of political power will accrue to the people that build computational infrastructures and those which can build artefacts on top of it. Systems like Hivemind hint at ways a broader set of people could gain access to large infrastructure, potentially altering who does and doesn&rsquo;t have political power as a consequence. &nbsp; Read more: Learning at Home (GitHub).  &nbsp; Read more: Learning@home: Crowdsourced Training of Large Neural networks using Decentralized Mixture-of-Experts (arXiv).###################################################Beware of how humans react to algorithmic recognitions, says DHS study:&hellip;Facial recognition test shows differences in how humans treat algo vs human recommendations&hellip;Researchers with the Maryland Test Facility (MdTF), a Department of Homeland Security-affiliated laboratory, have investigated how humans work in tandem with machines, when doing a facial recognition task. They tested about 300 volunteers in three groups of a hundred at the task of working out whether two greyscale pictures of people show the same person or a different person. One group got no prior information, while another group got suggested answers &ndash; priors &ndash;&nbsp; for the task provided by a human , and the final group got suggested answers from an AI. The test showed that the presence of a prior, unsurprisingly, improved performance. But humans treated computer-provided priors differently to human-provided priors&hellip;When we trust machines versus when we trust humans: The study shows that &ldquo;volunteers reported distrusting human identification ability more than computer identification ability&rdquo;, though notes that both sources led to similar ov…