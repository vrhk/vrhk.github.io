---

layout: post
category: threads
title: "[D] Why is the maximum input sequence length of BERT is restricted to 512?"
date: 2020-05-06 17:27:37
link: https://vrhk.co/2Wsj8di
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "I read the the Attention Is All You Need paper and assume the reason is due to computational complexity and the ability for the model to learn..."

---

### [D] Why is the maximum input sequence length of BERT is restricted to 512?

I read the the Attention Is All You Need paper and assume the reason is due to computational complexity and the ability for the model to learn...