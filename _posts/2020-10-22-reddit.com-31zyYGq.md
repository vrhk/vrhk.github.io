---

layout: post
category: threads
title: "[D] Can't self-attention layers easily have linear memory complexity in terms of sequence length?"
date: 2020-10-22 20:07:30
link: https://vrhk.co/31zyYGq
image: https://external-preview.redd.it/Cb_PcUygTcaeTkYhbknuR-wtPZFTuzLC2xj03KfWrwk.jpg?width=600&height=314.136125654&auto=webp&crop=600:314.136125654,smart&s=512847f3b280746dabbf5c5b436734fdd9ee0508
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "I'm aware that the usual way of implementing them is to do the following: 1. Given input x\_i for each word (or letter chunks if you are using..."

---

### [D] Can't self-attention layers easily have linear memory complexity in terms of sequence length?

I'm aware that the usual way of implementing them is to do the following: 1. Given input x\_i for each word (or letter chunks if you are using...