---

layout: post
category: threads
title: "[D] Whcih embedding is best for seq2seq?"
date: 2019-10-21 16:12:44
link: https://vrhk.co/2Bt3DYs
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Are there embeddings for seq2seq tasks? I have a very large vocab and I want to feed a fixed-length vector rather than one-hot vector to the input..."

---

### [D] Whcih embedding is best for seq2seq?

Are there embeddings for seq2seq tasks? I have a very large vocab and I want to feed a fixed-length vector rather than one-hot vector to the input...