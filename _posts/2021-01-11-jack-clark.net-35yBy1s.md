---

layout: post
category: product
title: "Import AI 231: US army builds nightvision facial recognition; 800GB of text for training GPT-3 models; fighting COVID with a mask detector"
date: 2021-01-11 17:07:06
link: https://vrhk.co/35yBy1s
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Fighting COVID with a janky mask detector:&hellip;It&rsquo;s getting really, really easy to homebrew surveillance tech&hellip;Researchers with Texas A&amp;M university, the University of Wisconsin-Milwaukee, and the State University of New York at Binghamtom, have built a basic AI model that can detect whether construction site workers are wearing COVID masks or not. The model itself is super basic &ndash; they finetune an object detection model on a mask dataset which they build out of:&ndash; A ~850-image &lsquo;Mask&rsquo; dataset from a site called MakeML.&ndash; A 1,000-image dataset they gather themselves.The authors train a Faster R-CNN Inception ResNet V2 model to test for mask compliance, as well as whether workers are respecting social distancing guidelines, then they test it out on four videos of road maintenance projects in Houston, TX. &rdquo; The output of the four cases indicated an average of more than 90% accuracy in detecting different types of mask wearing in construction workers&rdquo;, they note.Why this matters: Surveillance is becoming a widely available, commodity technology. Papers like this give us a sense of how easy it is getting to homebrew custom surveillance systems. (I also have a theory I published last summer with the &lsquo;CSET&rsquo; thinktank that COVID-19 would drive the rapid development of surveillance technologies, with usage growing faster in nations like China than America. Maybe this paper indicates America is going to use more AI-based surveillance than I anticipated).&nbsp; Read more: An Automatic System to Monitor the Physical Distance and Face Mask Wearing of Construction Workers in COVID-19 Pandemic (arXiv).###################################################Legendary chip designer heads to Canada:&hellip;Jim Keller heads from Tesla to Tenstorrent&hellip;Jim Keller, the guy who designed important chips for AMD, PA Semi, Apple, Tesla, and Intel (with the exception of Intel, this is basically a series of gigantic home runs), has joined AI chip startup Tenstorrent. Tenstorrent includes talent from AMD, NVIDIA, Altera, and more, and with Keller onboard, is definitely worth watching. It&rsquo;ll compete on building chips for ML inference and training with other startups like Graphcore, Cerebras, and others.&nbsp; Read more: Jim Keller Becomes CTO at Tenstorrent: &ldquo;The Most Promising Architecture Out There&rdquo; (AnandTech).



Meanwhile, another chip startup exits bankruptcy:As a reminder that semiconductor startups are insanely, mind-bendingly hard work: Wave Computing recently started going through Chapter 11 bankruptcy proceedings and has restructured itself to transfer some of its IP to Tallwood Technology Partners LLC. Wave Computing had made MIPS architecture chips for AI training and AI inference.&nbsp; Read more: Wave Computing and MIPS Technologies Reach Agreement to Exit Bankruptcy (press release, PR Newswire).Chinese companies pump ~$300 million into chip startup:&hellip;Tencent, others, back Enflame&hellip;Chinese AI chip startup Enflame Technology has raised $278m from investors including Tencent and CITIC. This is notable for a couple of reasons:&ndash; 1) Chiplomacy: The US is currently trying to kill China&rsquo;s nascent chip industry before the nation can develop its own independent technology stack (see: Import AI 181 for more). This has had the rather predictable effect of pouring jetfuel on China&rsquo;s domestic chip industry, as the country redoubles efforts to develop its own domestic champions.&ndash; 2) Vertical integration: Google has TPUs. Amazon has Trainium. Microsoft has some FPGA hybrid. The point is: all the big technology companies are trying to develop their own chips in a vertically oriented manner. Tencent investing in Enflame could signal that the Chinese internet giant is thinking about this more as well. (Tencent also formed a subsidiary in 2020, Baoan Bay Tencent Cloud Computing Company, which seems to be working on developing custom silicon for Tencent).&nbsp; Read more: Tencent invests in Chinese A.I. chip start-up as part of $279 million funding round (CNBC).&nbsp; Find out more about Enflame here (Enflame Tech).###################################################US army builds a thermal facial recognition dataset:&hellip;ARL-VTF means the era of nighttime robot surveillance isn&rsquo;t that far away&hellip;The US army has built a dataset to help it teach machine learning systems to do facial recognition on footage from thermal cameras.The DEVCOM Army Research Laboratory Visible-Thermal Face Dataset (ARL-VTF) was built by researchers from West Virginia University, the DEVCOM Army Research Laboratory, Booz Allen Hamilton, Johns Hopkins University , and the University of Nebraska-Lincoln. ARL-VTF consists of 549,712 images of 395 distinct people, with data in the form of RGB pictures as well as long wave infrared (LWIR). All the footage was taken at a resolution of 640 X 512 at a range of around 2 meters, with the human subjects doing different facial expressions and poses.&nbsp;



Why this matters: &ldquo;Thermal imaging of faces have applications in the military and law enforcement for face recognition in low-light and nighttime environments&rdquo;, the researchers note in the paper. ARL-VTF is an example of how the gains we&rsquo;ve seen in recent years in image recognition are being applied to other challenging identification problems. Look forward to a future where machines search for people in the dark.&nbsp; Read more: A Large-Scale, Time-Synchronized Visible and Thermal Face Dataset (arXiv).



###################################################Is your language model confused and/or biased? Use &lsquo;Ecco&rsquo; to check:&hellip;Python library lets you x-ray models like GPT2&hellip;Ecco is a new open source python library that lets people make language models more interpretable. Specifically, the software lets people analyze input saliency (how important is a word or phrase for the generation of another word or phrase) and neuron activations (what neurons in the model &lsquo;fire&rsquo; in response to what thing) for GPT-based models. Ecco is built on top of Pytorch via Hugging Face&rsquo;s &lsquo;Transformers&rsquo; library and runs in Google Colab.Why this matters: Language models are like big aliens that have arrived on earth and started helping us out with our search engines, fan fiction generation, and so on. But what are these aliens &lsquo;thinking&rsquo; and how do they &lsquo;think&rsquo;? These are the sorts of questions that software tools like Ecco will shed a bit of light on, though the whole field of interpretability likely needs to evolve further for us to fully decode these aliens.&nbsp; Read more: Interfaces for Explaining Transformer Language Models (Jay Alammar, Ecco creator, blog).&nbsp;&nbsp;Get the code here: Ecco (GitHub).&nbsp; Official project website here ().###################################################GPT-3 replicators release 800GB of text:&hellip;Want to build large language models like GPT-3? You&rsquo;ll need data first&hellip;Eleuther AI, a mysterious AI research collective who are trying to replicate (and release as open source) a GPT-3 scale language model, have released &lsquo;The Pile&rsquo;, a dataset of 800GB of text.What&rsquo;s in The Pile: The Pile includes data from PubMed Central, ArXiv, GitHub, the FreeLaw Project, Stack Exchange, the US Patent and Trademark Office, PubMed, Ubuntu IRC, HackerNews, YouTube, PhilPapers, and NIH. It also includes implementations of OpenWebText2 and BooksCorpus2, and wraps in existing datasets like Books3, Project Gutenberg, Open Subtitles, English Wikipedia, DM Mathematics, EuroParl, and the Enron Emails corpus.What does data mean for bias? Commendably, the authors include a discussion of some of the biases inherent to the model by conducting sentiment analysis of certain words and how these manifest in different sub parts of the overall dataset. They also note that fil…"

---

### Import AI 231: US army builds nightvision facial recognition; 800GB of text for training GPT-3 models; fighting COVID with a mask detector

Fighting COVID with a janky mask detector:&hellip;It&rsquo;s getting really, really easy to homebrew surveillance tech&hellip;Researchers with Texas A&amp;M university, the University of Wisconsin-Milwaukee, and the State University of New York at Binghamtom, have built a basic AI model that can detect whether construction site workers are wearing COVID masks or not. The model itself is super basic &ndash; they finetune an object detection model on a mask dataset which they build out of:&ndash; A ~850-image &lsquo;Mask&rsquo; dataset from a site called MakeML.&ndash; A 1,000-image dataset they gather themselves.The authors train a Faster R-CNN Inception ResNet V2 model to test for mask compliance, as well as whether workers are respecting social distancing guidelines, then they test it out on four videos of road maintenance projects in Houston, TX. &rdquo; The output of the four cases indicated an average of more than 90% accuracy in detecting different types of mask wearing in construction workers&rdquo;, they note.Why this matters: Surveillance is becoming a widely available, commodity technology. Papers like this give us a sense of how easy it is getting to homebrew custom surveillance systems. (I also have a theory I published last summer with the &lsquo;CSET&rsquo; thinktank that COVID-19 would drive the rapid development of surveillance technologies, with usage growing faster in nations like China than America. Maybe this paper indicates America is going to use more AI-based surveillance than I anticipated).&nbsp; Read more: An Automatic System to Monitor the Physical Distance and Face Mask Wearing of Construction Workers in COVID-19 Pandemic (arXiv).###################################################Legendary chip designer heads to Canada:&hellip;Jim Keller heads from Tesla to Tenstorrent&hellip;Jim Keller, the guy who designed important chips for AMD, PA Semi, Apple, Tesla, and Intel (with the exception of Intel, this is basically a series of gigantic home runs), has joined AI chip startup Tenstorrent. Tenstorrent includes talent from AMD, NVIDIA, Altera, and more, and with Keller onboard, is definitely worth watching. It&rsquo;ll compete on building chips for ML inference and training with other startups like Graphcore, Cerebras, and others.&nbsp; Read more: Jim Keller Becomes CTO at Tenstorrent: &ldquo;The Most Promising Architecture Out There&rdquo; (AnandTech).



Meanwhile, another chip startup exits bankruptcy:As a reminder that semiconductor startups are insanely, mind-bendingly hard work: Wave Computing recently started going through Chapter 11 bankruptcy proceedings and has restructured itself to transfer some of its IP to Tallwood Technology Partners LLC. Wave Computing had made MIPS architecture chips for AI training and AI inference.&nbsp; Read more: Wave Computing and MIPS Technologies Reach Agreement to Exit Bankruptcy (press release, PR Newswire).Chinese companies pump ~$300 million into chip startup:&hellip;Tencent, others, back Enflame&hellip;Chinese AI chip startup Enflame Technology has raised $278m from investors including Tencent and CITIC. This is notable for a couple of reasons:&ndash; 1) Chiplomacy: The US is currently trying to kill China&rsquo;s nascent chip industry before the nation can develop its own independent technology stack (see: Import AI 181 for more). This has had the rather predictable effect of pouring jetfuel on China&rsquo;s domestic chip industry, as the country redoubles efforts to develop its own domestic champions.&ndash; 2) Vertical integration: Google has TPUs. Amazon has Trainium. Microsoft has some FPGA hybrid. The point is: all the big technology companies are trying to develop their own chips in a vertically oriented manner. Tencent investing in Enflame could signal that the Chinese internet giant is thinking about this more as well. (Tencent also formed a subsidiary in 2020, Baoan Bay Tencent Cloud Computing Company, which seems to be working on developing custom silicon for Tencent).&nbsp; Read more: Tencent invests in Chinese A.I. chip start-up as part of $279 million funding round (CNBC).&nbsp; Find out more about Enflame here (Enflame Tech).###################################################US army builds a thermal facial recognition dataset:&hellip;ARL-VTF means the era of nighttime robot surveillance isn&rsquo;t that far away&hellip;The US army has built a dataset to help it teach machine learning systems to do facial recognition on footage from thermal cameras.The DEVCOM Army Research Laboratory Visible-Thermal Face Dataset (ARL-VTF) was built by researchers from West Virginia University, the DEVCOM Army Research Laboratory, Booz Allen Hamilton, Johns Hopkins University , and the University of Nebraska-Lincoln. ARL-VTF consists of 549,712 images of 395 distinct people, with data in the form of RGB pictures as well as long wave infrared (LWIR). All the footage was taken at a resolution of 640 X 512 at a range of around 2 meters, with the human subjects doing different facial expressions and poses.&nbsp;



Why this matters: &ldquo;Thermal imaging of faces have applications in the military and law enforcement for face recognition in low-light and nighttime environments&rdquo;, the researchers note in the paper. ARL-VTF is an example of how the gains we&rsquo;ve seen in recent years in image recognition are being applied to other challenging identification problems. Look forward to a future where machines search for people in the dark.&nbsp; Read more: A Large-Scale, Time-Synchronized Visible and Thermal Face Dataset (arXiv).



###################################################Is your language model confused and/or biased? Use &lsquo;Ecco&rsquo; to check:&hellip;Python library lets you x-ray models like GPT2&hellip;Ecco is a new open source python library that lets people make language models more interpretable. Specifically, the software lets people analyze input saliency (how important is a word or phrase for the generation of another word or phrase) and neuron activations (what neurons in the model &lsquo;fire&rsquo; in response to what thing) for GPT-based models. Ecco is built on top of Pytorch via Hugging Face&rsquo;s &lsquo;Transformers&rsquo; library and runs in Google Colab.Why this matters: Language models are like big aliens that have arrived on earth and started helping us out with our search engines, fan fiction generation, and so on. But what are these aliens &lsquo;thinking&rsquo; and how do they &lsquo;think&rsquo;? These are the sorts of questions that software tools like Ecco will shed a bit of light on, though the whole field of interpretability likely needs to evolve further for us to fully decode these aliens.&nbsp; Read more: Interfaces for Explaining Transformer Language Models (Jay Alammar, Ecco creator, blog).&nbsp;&nbsp;Get the code here: Ecco (GitHub).&nbsp; Official project website here ().###################################################GPT-3 replicators release 800GB of text:&hellip;Want to build large language models like GPT-3? You&rsquo;ll need data first&hellip;Eleuther AI, a mysterious AI research collective who are trying to replicate (and release as open source) a GPT-3 scale language model, have released &lsquo;The Pile&rsquo;, a dataset of 800GB of text.What&rsquo;s in The Pile: The Pile includes data from PubMed Central, ArXiv, GitHub, the FreeLaw Project, Stack Exchange, the US Patent and Trademark Office, PubMed, Ubuntu IRC, HackerNews, YouTube, PhilPapers, and NIH. It also includes implementations of OpenWebText2 and BooksCorpus2, and wraps in existing datasets like Books3, Project Gutenberg, Open Subtitles, English Wikipedia, DM Mathematics, EuroParl, and the Enron Emails corpus.What does data mean for bias? Commendably, the authors include a discussion of some of the biases inherent to the model by conducting sentiment analysis of certain words and how these manifest in different sub parts of the overall dataset. They also note that fil…