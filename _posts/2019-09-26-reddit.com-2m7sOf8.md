---

layout: post
category: threads
title: "[D] Why do effective activation functions have a bounded derivative?"
date: 2019-09-26 22:22:25
link: https://vrhk.co/2m7sOf8
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Is there a reason why almost every modern activation function in deep learning has a bounded derivative? ReLU, Swish, tanh, sigmoid and other..."

---

### [D] Why do effective activation functions have a bounded derivative?

Is there a reason why almost every modern activation function in deep learning has a bounded derivative? ReLU, Swish, tanh, sigmoid and other...