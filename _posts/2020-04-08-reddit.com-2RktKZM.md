---

layout: post
category: threads
title: "[D] Should small random initialization be used with ReLU?"
date: 2020-04-08 06:17:34
link: https://vrhk.co/2RktKZM
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "w = np.random.randn(n) * sqrt(2.0/n) or w = 0.01 * np.random.randn(n) * sqrt(2.0/n)"

---

### [D] Should small random initialization be used with ReLU?

w = np.random.randn(n) * sqrt(2.0/n) or w = 0.01 * np.random.randn(n) * sqrt(2.0/n)