---

layout: post
category: threads
title: "[P] Optimization for ML post - training models using stochastic proximal point with mini-batches"
date: 2020-06-16 19:17:33
link: https://vrhk.co/37Aj403
image: https://external-preview.redd.it/VKiKvej59Mc4BsDtEuZyZ31ra_kAfQXsueDsrtoxjgQ.jpg?width=611&height=319.895287958&auto=webp&crop=611:319.895287958,smart&s=946dc3a012cc2781a100a2e8e378035538d92c6a
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Training ML models while exploiting the exact losses, seems impossible at first glance, and that's why we usually employ SGD variants - they..."

---

### [P] Optimization for ML post - training models using stochastic proximal point with mini-batches

Training ML models while exploiting the exact losses, seems impossible at first glance, and that's why we usually employ SGD variants - they...