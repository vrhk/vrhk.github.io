---

layout: post
category: threads
title: "[R] Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation. They show that NMT models with manually engineered, fixed (i.e. position-based) attention patterns perform as well as models that learn how to attend."
date: 2020-02-26 01:37:35
link: https://vrhk.co/380c0sm
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Posted in r/MachineLearning by u/hardmaru • 1 point and 1 comment"

---

### [R] Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation. They show that NMT models with manually engineered, fixed (i.e. position-based) attention patterns perform as well as models that learn how to attend.

Posted in r/MachineLearning by u/hardmaru • 1 point and 1 comment