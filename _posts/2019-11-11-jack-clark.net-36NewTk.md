---

layout: post
category: product
title: "Import AI 172: Google codes AI to fix errors in Google’s code; Amazon makes mini-self-driving cars with deepracer; and Microsoft uses GPT-2 to make auto-suggest for coders"
date: 2019-11-11 18:41:33
link: https://vrhk.co/36NewTk
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Microsoft wants to use internet-scale language models to make programmers fitter, happier, and more productive:&hellip;Code + GPT-2 = Auto-complete for programmers&hellip;Microsoft has used recent advances in language understanding to build a smart auto-complete function for programmers. The software company announced its Visual Studio &ldquo;IntelliCode&rdquo; feature at its Microsoft Ignite conference in November. The technology, which is inspired by language models like GPT-2, &ldquo;extracts statistical coding patterns and learns the intricacies of programming languages from GitHub repos to assist developers in their coding,&rdquo; the company says. &ldquo;Based on code context, as you type, IntelliCode uses that semantic information and sourced patterns to predict the most likely completion in-line with your code.&rdquo; Other people have experimented with applying large language models to problems of code prediction, including a startup called TabNine which released a GPT-2-based code completer earlier this summer.&nbsp;
Why this matters: Recent advances in language models are making it easy for us to build big, predictive models for any sort of information that can be framed as a text processing problem. That means in the coming years we&rsquo;re going to develop systems that can predict pleasing sequences of words, code scripts, and (though this is only just beginning to happen) sequences of chemical compounds and other things. As this technology matures, I expect people will start using such prediction tools to augment their own intelligence, pairing human intuition with big internet-scale predictive models for given domains. The cyborgs will soon be among us &ndash; and they&rsquo;ll be helping to do code review!&nbsp;&nbsp;&nbsp;Read more: Re-imagining developer productivity with AI-assisted tools (Microsoft).&nbsp;&nbsp;&nbsp;Try out the feature in Microsoft&rsquo;s latest Visual Studio Preview (official Microsoft webpage).&nbsp;&nbsp;&nbsp;Read more about TabNine&rsquo;s tech &ndash; Autocompletion with deep learning (TabNine blog).
####################################################
So, how do we feel about all this surveillance AI stuff we&rsquo;re developing?&hellip;Reddit thread gives us a view into how the machine learning community thinks about ethical tradeoffs&hellip;AI, or &ndash; more narrowly &ndash; deep learning, is a utility-class technology; it has a vast range of applications and many of these are being explored by developers around the world. So, how do we feel about the fact that some of these applications are focused on surveillance? And how do we feel about the fact that a small number of nation states are enthusiastically adopting AI-based surveillance technologies in the service of surveiling their citizens? That&rsquo;s a question that some parts of the AI research community are beginning to ponder, and a recent thread on Reddit dramatizes this by noting just how many surveillance-oriented applications seem to come from Chinese labs (which makes sense, given that China is probably the world&rsquo;s most significant developer and deployer of surveillance AI systems).&nbsp;
Many reactions, few solutions: In the thread, users of the r/machinelearning subreddit share their thoughts on the issue. Responses range from (paraphrased) it&rsquo;s all science, it&rsquo;s not our job to think about second-order effects to this question is indicative of absurd paranoia about China to yes, China does a lot of this, but what about the US? The volume and diversity of responses gives us a sense of how thorny an issue this is for many ML researchers.&nbsp;
Dual-use technologies: A big issue with surveillance AI is that it has a range of usages, some of which are quite positive. &ldquo;For what it&rsquo;s worth, I work in animal re-identification and the technologies that are applied and perfected in humans are slowly making their way to help monitor endangered animal populations,&rdquo; they write. &ldquo;It is our responsibility to call out unethical practices but also to not lose sight of all the social good that can come from ML research.&rdquo;&nbsp;&nbsp;&nbsp;Read more: ICCV &ndash; 19 &ndash; The state of (some) ethically questionable papers (Reddit/r/machinelearning).
####################################################
Stanford researchers give simulated robots the sensation of touch:&hellip;Sim2real + simulated robots + high-fidelity environments + interaction, oh my!&hellip;Researchers with the Stanford AI Lab have extended their &lsquo;Gibson&rsquo; robot simulation software to support interactive objects, making it possible for researchers to use Gibson to train simulated AI agents to interact with the world around them. Because the Gibson simulator (first covered: Import AI 111) supports high-fidelity graphics, it may be possible to transfer agents trained in Gibson into reality (though that&rsquo;s more likely to be successful for pure visual perception tasks, rather than manipulation).&nbsp;
Faster, Gibson! The researchers have also made Gibson faster &ndash; the first version of Gibson rendered scenes at between 25 and 40 frames per second (FPS) on modern GPUs. That&rsquo;s barely good enough for a standard computer game being played by a human, and wildly inefficient for AI research, where agents are typically so sample efficient that it&rsquo;s much better to have simulators that can run at thousands of FPS. In Interactive Gibson, the researchers implement a high-performance mesh rendering system written in Python and C++, improving FPS to ~1,000FPS at a 256X256 scene resolution &ndash; this is pretty good and should make the platform more attractive to researchers.&nbsp;
Interactive Gibson Benchmark: If you want to test out how well your agents can perform in the new, improved Gibson, you can investigate a benchmark challenge created by the researchers. This challenge augments 106 existing Gibson scenes with 1984 interactable instances of five objects: chairs, desks, doors, sofas, and tables. Because Gibson consists of over 211,000 square meters of simulated indoor space, it&rsquo;s not feasible to have human annotators go through it and work out where to put new objects; instead, the Gibson researchers create an AI-infused object-generation system that scans over the entire dataset and proposes objects it can add to scenes, then checks with humans as to whether its suggestions are appropriate. I think it&rsquo;s interesting how common it is becoming to use ML techniques to semi-automatically enhance ML-oriented datasets.&nbsp;&nbsp;&nbsp;&nbsp;
What does success mean in Gibson? As many AI researchers know, goal specification is always a hard problem when developing AI tasks and challenges. So, how can we assess we&rsquo;re making progress in the Gibson environment? The developers propose a metric called Interactive Navigation Score (INS) that measures a couple of dimensions of the efficiency of an embodied AI agent; specifically, the efficiency (aka, distance traveled) of the paths it discovers to reach its goals, as well as the effort efficiency, which measures how much energy the agent needed to expend to achieve its goal (eg, how much energy it spends moving its own body or manipulating objects in the environment to help it achieve its goal).
The robot agents of Gibson: Having a world you can interact with is pretty pointless if you don&rsquo;t have a body to use to interact with the world, so the Gibson team has also implemented several simulated robots that researchers can use within Gibson.  &nbsp; These robots include:&nbsp;
Two widely-used simulated agents (the Mujoco humanoid and ant bots)
Four wheeled navigation agents (Freight, JackRabbot v1, Husky, Turtlebot v2)
Two mobile manipulators with arms (Fetch, JackRabbot v2)
A quadrocopter/drone (specifically, a Quadrotor)
Why this matters: As I&rsquo;ve written in this newsletter before, the worlds of robotics and of AI are becoming increasingly intermi…"

---

### Import AI 172: Google codes AI to fix errors in Google’s code; Amazon makes mini-self-driving cars with deepracer; and Microsoft uses GPT-2 to make auto-suggest for coders

Microsoft wants to use internet-scale language models to make programmers fitter, happier, and more productive:&hellip;Code + GPT-2 = Auto-complete for programmers&hellip;Microsoft has used recent advances in language understanding to build a smart auto-complete function for programmers. The software company announced its Visual Studio &ldquo;IntelliCode&rdquo; feature at its Microsoft Ignite conference in November. The technology, which is inspired by language models like GPT-2, &ldquo;extracts statistical coding patterns and learns the intricacies of programming languages from GitHub repos to assist developers in their coding,&rdquo; the company says. &ldquo;Based on code context, as you type, IntelliCode uses that semantic information and sourced patterns to predict the most likely completion in-line with your code.&rdquo; Other people have experimented with applying large language models to problems of code prediction, including a startup called TabNine which released a GPT-2-based code completer earlier this summer.&nbsp;
Why this matters: Recent advances in language models are making it easy for us to build big, predictive models for any sort of information that can be framed as a text processing problem. That means in the coming years we&rsquo;re going to develop systems that can predict pleasing sequences of words, code scripts, and (though this is only just beginning to happen) sequences of chemical compounds and other things. As this technology matures, I expect people will start using such prediction tools to augment their own intelligence, pairing human intuition with big internet-scale predictive models for given domains. The cyborgs will soon be among us &ndash; and they&rsquo;ll be helping to do code review!&nbsp;&nbsp;&nbsp;Read more: Re-imagining developer productivity with AI-assisted tools (Microsoft).&nbsp;&nbsp;&nbsp;Try out the feature in Microsoft&rsquo;s latest Visual Studio Preview (official Microsoft webpage).&nbsp;&nbsp;&nbsp;Read more about TabNine&rsquo;s tech &ndash; Autocompletion with deep learning (TabNine blog).
####################################################
So, how do we feel about all this surveillance AI stuff we&rsquo;re developing?&hellip;Reddit thread gives us a view into how the machine learning community thinks about ethical tradeoffs&hellip;AI, or &ndash; more narrowly &ndash; deep learning, is a utility-class technology; it has a vast range of applications and many of these are being explored by developers around the world. So, how do we feel about the fact that some of these applications are focused on surveillance? And how do we feel about the fact that a small number of nation states are enthusiastically adopting AI-based surveillance technologies in the service of surveiling their citizens? That&rsquo;s a question that some parts of the AI research community are beginning to ponder, and a recent thread on Reddit dramatizes this by noting just how many surveillance-oriented applications seem to come from Chinese labs (which makes sense, given that China is probably the world&rsquo;s most significant developer and deployer of surveillance AI systems).&nbsp;
Many reactions, few solutions: In the thread, users of the r/machinelearning subreddit share their thoughts on the issue. Responses range from (paraphrased) it&rsquo;s all science, it&rsquo;s not our job to think about second-order effects to this question is indicative of absurd paranoia about China to yes, China does a lot of this, but what about the US? The volume and diversity of responses gives us a sense of how thorny an issue this is for many ML researchers.&nbsp;
Dual-use technologies: A big issue with surveillance AI is that it has a range of usages, some of which are quite positive. &ldquo;For what it&rsquo;s worth, I work in animal re-identification and the technologies that are applied and perfected in humans are slowly making their way to help monitor endangered animal populations,&rdquo; they write. &ldquo;It is our responsibility to call out unethical practices but also to not lose sight of all the social good that can come from ML research.&rdquo;&nbsp;&nbsp;&nbsp;Read more: ICCV &ndash; 19 &ndash; The state of (some) ethically questionable papers (Reddit/r/machinelearning).
####################################################
Stanford researchers give simulated robots the sensation of touch:&hellip;Sim2real + simulated robots + high-fidelity environments + interaction, oh my!&hellip;Researchers with the Stanford AI Lab have extended their &lsquo;Gibson&rsquo; robot simulation software to support interactive objects, making it possible for researchers to use Gibson to train simulated AI agents to interact with the world around them. Because the Gibson simulator (first covered: Import AI 111) supports high-fidelity graphics, it may be possible to transfer agents trained in Gibson into reality (though that&rsquo;s more likely to be successful for pure visual perception tasks, rather than manipulation).&nbsp;
Faster, Gibson! The researchers have also made Gibson faster &ndash; the first version of Gibson rendered scenes at between 25 and 40 frames per second (FPS) on modern GPUs. That&rsquo;s barely good enough for a standard computer game being played by a human, and wildly inefficient for AI research, where agents are typically so sample efficient that it&rsquo;s much better to have simulators that can run at thousands of FPS. In Interactive Gibson, the researchers implement a high-performance mesh rendering system written in Python and C++, improving FPS to ~1,000FPS at a 256X256 scene resolution &ndash; this is pretty good and should make the platform more attractive to researchers.&nbsp;
Interactive Gibson Benchmark: If you want to test out how well your agents can perform in the new, improved Gibson, you can investigate a benchmark challenge created by the researchers. This challenge augments 106 existing Gibson scenes with 1984 interactable instances of five objects: chairs, desks, doors, sofas, and tables. Because Gibson consists of over 211,000 square meters of simulated indoor space, it&rsquo;s not feasible to have human annotators go through it and work out where to put new objects; instead, the Gibson researchers create an AI-infused object-generation system that scans over the entire dataset and proposes objects it can add to scenes, then checks with humans as to whether its suggestions are appropriate. I think it&rsquo;s interesting how common it is becoming to use ML techniques to semi-automatically enhance ML-oriented datasets.&nbsp;&nbsp;&nbsp;&nbsp;
What does success mean in Gibson? As many AI researchers know, goal specification is always a hard problem when developing AI tasks and challenges. So, how can we assess we&rsquo;re making progress in the Gibson environment? The developers propose a metric called Interactive Navigation Score (INS) that measures a couple of dimensions of the efficiency of an embodied AI agent; specifically, the efficiency (aka, distance traveled) of the paths it discovers to reach its goals, as well as the effort efficiency, which measures how much energy the agent needed to expend to achieve its goal (eg, how much energy it spends moving its own body or manipulating objects in the environment to help it achieve its goal).
The robot agents of Gibson: Having a world you can interact with is pretty pointless if you don&rsquo;t have a body to use to interact with the world, so the Gibson team has also implemented several simulated robots that researchers can use within Gibson.  &nbsp; These robots include:&nbsp;
Two widely-used simulated agents (the Mujoco humanoid and ant bots)
Four wheeled navigation agents (Freight, JackRabbot v1, Husky, Turtlebot v2)
Two mobile manipulators with arms (Fetch, JackRabbot v2)
A quadrocopter/drone (specifically, a Quadrotor)
Why this matters: As I&rsquo;ve written in this newsletter before, the worlds of robotics and of AI are becoming increasingly intermi…