---

layout: post
category: product
title: "Import AI 195: StereoSet tests bias in language models; an AI Index job ad; plus, using NLP to waste phishers’ time"
date: 2020-04-27 17:26:39
link: https://vrhk.co/2SdyCk6
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "NLP Spy vs Spy:&hellip;&rdquo;Panacea&rdquo; cyber-defense platform uses NLP to counter phishing attacks and waste criminals&rsquo; time&hellip;Criminals love email. It&rsquo;s an easy, convenient way to reach people, and it makes it easy to carry out a social engineering attack, where you try and convince someone to open an attachment, or carry out an action, to help you achieve a malicious goal. How can companies protect themselves from these kinds of attacks? One way is to train employees so they understand the threat landscape. Training is nice, but it doesn&rsquo;t help you defend against attackers in an automated way, or figure out information about them. This is why a group of researchers at IHMC, SUNY, UNCC, and Rensselaer Polytechnic Institute, have developed software called Panacea, which uses natural language processing technology to create defenses against social engineering attacks. Defending with Panacea: &ldquo;Panacea&rsquo;s primary use cases are: (1) monitoring a user&rsquo;s inbox to detect SE attacks; and (2) engaging the attacker to gain attributable information about their true identity while preventing attacks from succeeding&rdquo;. If Panacea thinks it has encountered a fraudulent email, then it boots up a load of NLP capabilities to analyze the email and parse out the possible attack type and attack intention, then tries to generate an email in response. The purpose of this email is to try and find out more information about the attacker and also to waste their time. Why this matters: AI is going to become a new kind of ethereal armor for organizations &ndash; we&rsquo;ll use technologies like Panacea to create complex, self-adjusting defensive perimeters, and these systems will display some traits of emergent sophistication as they adjust to (and learn from) their enemies.  &nbsp; Read more: The Panacea Threat Intelligence and Active Defense Platform (arXiv). ####################################################Job posting &ndash; work with me on the AI Index:The AI Index is hiring a project manager! The AI Index is a Stanford initiative to measure, assess, and analyze the progress and impact of artificial intelligence. You&rsquo;ll work with me, members of the Steering Committee of the AI Index, and members of Stanford&rsquo;s Institute for Human-Centered Artificial Intelligence to help produce the annual AI Index report, and think about better and more impactful ways to measure and communicate AI progress. The role would suit someone who loves digging into scientific papers, is good at project management, and has a burning desire to figure out where this technology is going, what it means for civilization, and how to communicate its trajectory to decisionmakers around the world.  &nbsp; If you&rsquo;ve got any questions, feel free to email me about the role! &nbsp; More details about the role here at Stanford&rsquo;s site. ####################################################Can we build language models that possess less bias?&hellip;StereoSet dataset and challenge suggests &lsquo;yes&rsquo;, though who defines bias?&hellip;Language models are funhouse mirrors of reality &ndash; they take the underlying biases inherent in a corpus of information (like an internet-scale text dataset), then magnify them unevenly. What comes out is a pre-trained LM that can generate text, some of which exhibits the biases of the dataset on which it was trained. How can we evaluate the bias of these language models in a disciplined way? That&rsquo;s the idea of new research from MIT, Intel, and the Montreal Institute for Learning Algorithms (MILA), which introduces StereoSet, &ldquo;a large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion&rdquo;. What does StereoSet test for? StereoSet is designed to &ldquo;assess the stereotypical biases of popular pre-trained language models&rdquo;. It does this by gathering a bunch of different &lsquo;target terms&rsquo; (e.g., &ldquo;actor&rdquo;, &ldquo;housekeeper&rdquo;) for four different domains, then creates a batch of tests meant to judge if the language model skews towards stereotypical, anti-stereotypical, or non-stereotyped predictions about these terms. For instance, if a language model consistently says &ldquo;Mexican&rdquo; at the end of a sentence like &ldquo;Our housekeeper is a _____&rdquo;, rather than &ldquo;American&rdquo;, etc, then it could be said to be displaying a stereotype. )OpenAI earlier analyzed its &lsquo;GPT-2&rsquo; model using some bias tests that were philosophically similar to this analytical method).How do we test for Bias? Stereset tests for bias by using three metrics: &ndash; A language modeling score &ndash; this tests how well the system does at basic language modeling tasks. &ndash; A stereotype score &ndash; this tests how much a model &lsquo;prefers&rsquo; a stereotype or anti-stereotype term in a dataset (so a good stereotype score is around 50%, as that means your model doesn&rsquo;t display a clear bias for a given stereotypical term). &ndash; A Idealized context association test (CAT), which combines the language modeling score and stereotype score, which basically reflects how well a model does at language modeling relative to how biased it may be. Who defines bias? To define the stereotypes in StereoSet, the researchers use crowdworkers based in the USA, rented via Amazon Mechanical Turk. They ask these people to construct sentences or phrases that, in their subjective view, relates to stereotypical or anti-stereotypical sentences. This feels&hellip; okay? These people definitely have their own biases, and this whole area feels hard to develop a sense of &lsquo;ground-truth&rsquo; about, as our own interpretations of bias are themselves subjective. This highlights the meta-challenge in bias research &ndash; how biased is your research approach to AI bias?How biased are today&rsquo;s language models? The researchers test out variants of four different language models &ndash; BERT, RoBERTA, XLNET, and GPT2 against StereoSet. In tests, the model which has the highest &lsquo;idealized CAT score&rsquo; (so a fusion of capability and lack of bias) is a small GPT2 model, which gets a score of 73.0; while the least biased model is a ROBERTA-base model, that gets a stereotype score of 50.5, compared to 56.4 for GPT2.
Read more: StereoSet: Measuring stereotypical bias in pretrained language models (arXiv).Check out the StereoSet leaderboard and rankings here (StereoSet official website).####################################################Want to train AI against GameBoy games? Try out PyBoy:&hellip;OpenAI Gym, but for the Gameboy&hellip;PyBoy is a new software package that emulates a gameboy, making it possible for developers to train AI systems against them. &ldquo;PyBoy is loadable as an object in Python,&rdquo; the developers write. &ldquo;This means, it can be initialized from another script, and be controlled and probed by the script&rdquo;.  &nbsp; Get the code for PyBoy from here (GitHub). &nbsp; Read more about the emulator here (PDF).####################################################Why a &lsquo;national security&rsquo; mindset means we&rsquo;ll die of an asteroid:&hellip;Want humanity to survive the next century? Think about &lsquo;existential security&rsquo;&hellip;If you went to Washington DC during the past few years, you could entertain yourself by playing a drinking game called &lsquo;national security blackout&rsquo;. The game works like this: you sit in a room with some liquor in a brown paper bag and listen to some mid-career policy wonks talk about STEM policy; every time you hear the words &ldquo;national security&rdquo; you take a drink. By the end of the conversation you&rsquo;re so drunk you&rsquo;ve got no idea what anyone else is saying, nor do you think you need to listen to them.  &nbsp; Actual policy is eerily similar to this: nations sit around and every time they …"

---

### Import AI 195: StereoSet tests bias in language models; an AI Index job ad; plus, using NLP to waste phishers’ time

NLP Spy vs Spy:&hellip;&rdquo;Panacea&rdquo; cyber-defense platform uses NLP to counter phishing attacks and waste criminals&rsquo; time&hellip;Criminals love email. It&rsquo;s an easy, convenient way to reach people, and it makes it easy to carry out a social engineering attack, where you try and convince someone to open an attachment, or carry out an action, to help you achieve a malicious goal. How can companies protect themselves from these kinds of attacks? One way is to train employees so they understand the threat landscape. Training is nice, but it doesn&rsquo;t help you defend against attackers in an automated way, or figure out information about them. This is why a group of researchers at IHMC, SUNY, UNCC, and Rensselaer Polytechnic Institute, have developed software called Panacea, which uses natural language processing technology to create defenses against social engineering attacks. Defending with Panacea: &ldquo;Panacea&rsquo;s primary use cases are: (1) monitoring a user&rsquo;s inbox to detect SE attacks; and (2) engaging the attacker to gain attributable information about their true identity while preventing attacks from succeeding&rdquo;. If Panacea thinks it has encountered a fraudulent email, then it boots up a load of NLP capabilities to analyze the email and parse out the possible attack type and attack intention, then tries to generate an email in response. The purpose of this email is to try and find out more information about the attacker and also to waste their time. Why this matters: AI is going to become a new kind of ethereal armor for organizations &ndash; we&rsquo;ll use technologies like Panacea to create complex, self-adjusting defensive perimeters, and these systems will display some traits of emergent sophistication as they adjust to (and learn from) their enemies.  &nbsp; Read more: The Panacea Threat Intelligence and Active Defense Platform (arXiv). ####################################################Job posting &ndash; work with me on the AI Index:The AI Index is hiring a project manager! The AI Index is a Stanford initiative to measure, assess, and analyze the progress and impact of artificial intelligence. You&rsquo;ll work with me, members of the Steering Committee of the AI Index, and members of Stanford&rsquo;s Institute for Human-Centered Artificial Intelligence to help produce the annual AI Index report, and think about better and more impactful ways to measure and communicate AI progress. The role would suit someone who loves digging into scientific papers, is good at project management, and has a burning desire to figure out where this technology is going, what it means for civilization, and how to communicate its trajectory to decisionmakers around the world.  &nbsp; If you&rsquo;ve got any questions, feel free to email me about the role! &nbsp; More details about the role here at Stanford&rsquo;s site. ####################################################Can we build language models that possess less bias?&hellip;StereoSet dataset and challenge suggests &lsquo;yes&rsquo;, though who defines bias?&hellip;Language models are funhouse mirrors of reality &ndash; they take the underlying biases inherent in a corpus of information (like an internet-scale text dataset), then magnify them unevenly. What comes out is a pre-trained LM that can generate text, some of which exhibits the biases of the dataset on which it was trained. How can we evaluate the bias of these language models in a disciplined way? That&rsquo;s the idea of new research from MIT, Intel, and the Montreal Institute for Learning Algorithms (MILA), which introduces StereoSet, &ldquo;a large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion&rdquo;. What does StereoSet test for? StereoSet is designed to &ldquo;assess the stereotypical biases of popular pre-trained language models&rdquo;. It does this by gathering a bunch of different &lsquo;target terms&rsquo; (e.g., &ldquo;actor&rdquo;, &ldquo;housekeeper&rdquo;) for four different domains, then creates a batch of tests meant to judge if the language model skews towards stereotypical, anti-stereotypical, or non-stereotyped predictions about these terms. For instance, if a language model consistently says &ldquo;Mexican&rdquo; at the end of a sentence like &ldquo;Our housekeeper is a _____&rdquo;, rather than &ldquo;American&rdquo;, etc, then it could be said to be displaying a stereotype. )OpenAI earlier analyzed its &lsquo;GPT-2&rsquo; model using some bias tests that were philosophically similar to this analytical method).How do we test for Bias? Stereset tests for bias by using three metrics: &ndash; A language modeling score &ndash; this tests how well the system does at basic language modeling tasks. &ndash; A stereotype score &ndash; this tests how much a model &lsquo;prefers&rsquo; a stereotype or anti-stereotype term in a dataset (so a good stereotype score is around 50%, as that means your model doesn&rsquo;t display a clear bias for a given stereotypical term). &ndash; A Idealized context association test (CAT), which combines the language modeling score and stereotype score, which basically reflects how well a model does at language modeling relative to how biased it may be. Who defines bias? To define the stereotypes in StereoSet, the researchers use crowdworkers based in the USA, rented via Amazon Mechanical Turk. They ask these people to construct sentences or phrases that, in their subjective view, relates to stereotypical or anti-stereotypical sentences. This feels&hellip; okay? These people definitely have their own biases, and this whole area feels hard to develop a sense of &lsquo;ground-truth&rsquo; about, as our own interpretations of bias are themselves subjective. This highlights the meta-challenge in bias research &ndash; how biased is your research approach to AI bias?How biased are today&rsquo;s language models? The researchers test out variants of four different language models &ndash; BERT, RoBERTA, XLNET, and GPT2 against StereoSet. In tests, the model which has the highest &lsquo;idealized CAT score&rsquo; (so a fusion of capability and lack of bias) is a small GPT2 model, which gets a score of 73.0; while the least biased model is a ROBERTA-base model, that gets a stereotype score of 50.5, compared to 56.4 for GPT2.
Read more: StereoSet: Measuring stereotypical bias in pretrained language models (arXiv).Check out the StereoSet leaderboard and rankings here (StereoSet official website).####################################################Want to train AI against GameBoy games? Try out PyBoy:&hellip;OpenAI Gym, but for the Gameboy&hellip;PyBoy is a new software package that emulates a gameboy, making it possible for developers to train AI systems against them. &ldquo;PyBoy is loadable as an object in Python,&rdquo; the developers write. &ldquo;This means, it can be initialized from another script, and be controlled and probed by the script&rdquo;.  &nbsp; Get the code for PyBoy from here (GitHub). &nbsp; Read more about the emulator here (PDF).####################################################Why a &lsquo;national security&rsquo; mindset means we&rsquo;ll die of an asteroid:&hellip;Want humanity to survive the next century? Think about &lsquo;existential security&rsquo;&hellip;If you went to Washington DC during the past few years, you could entertain yourself by playing a drinking game called &lsquo;national security blackout&rsquo;. The game works like this: you sit in a room with some liquor in a brown paper bag and listen to some mid-career policy wonks talk about STEM policy; every time you hear the words &ldquo;national security&rdquo; you take a drink. By the end of the conversation you&rsquo;re so drunk you&rsquo;ve got no idea what anyone else is saying, nor do you think you need to listen to them.  &nbsp; Actual policy is eerily similar to this: nations sit around and every time they …