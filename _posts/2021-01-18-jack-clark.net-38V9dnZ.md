---

layout: post
category: product
title: "Import AI 232: Google trains a trillion parameter model; South Korean chatbot blows up; AI doesn’t use as much electricity as you think"
date: 2021-01-18 18:36:39
link: https://vrhk.co/38V9dnZ
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Uh-oh, Parler is about to step on a big &lsquo;ol algorithm rake:&hellip;CEO says algorithms can filter hate speech. Good luck with that!&hellip;Parler, the social network used by far right activists and subsequently pulled offline due to failing to meet T&amp;Cs from a variety of infrastructure services (including Amazon Web Services), has a plan to come back: it&rsquo;s going to use algorithms to filter hate speech on the service. Uh oh!&ldquo;We will be taking more algorithmic approaches to content but doing it to respect people&rsquo;s privacy, too,&rdquo; Parler CEO John Matz told FOX News. &ldquo;Will be having algorithms look at all the content &hellip; to try and predict whether it&rsquo;s a terms-of-service violation so we can adjust quicker and the most egregious things can get taken down&rdquo;.Algorithms != editors: If you want to use algorithms to moderate hate speech, you&rsquo;re going to get into the fun questions that entails. These include:&ndash; Can your algorithms effectively tell the difference between hate speech and satire of hate speech?&ndash; Are you comfortable making judgement calls about the heuristics you will use to give initial biases to these algorithms?&ndash; How do you distinguish between acceptable and unacceptable words and phrases?Why this matters: Parler highlights the challenge of scale combined with contemporary economics &ndash; Parler operate(d) at a scale equivalent to things like large television networks, but did so with a tiny investment into its own humans. Traditional media organizations deal with issues of speech by having an editorial line which gets enforced by thousands of individual journalists and editors making subjective, qualitative decisions. It&rsquo;s imperfect, but put it this way: when you watch Fox, you know what you&rsquo;re getting, and when you watch the BBC, you know what you&rsquo;re getting, and you can intuit the biases of the humans behind the editorial decisions. Now, tiny companies are trying to use algorithms to substitute for this varied multitude of different human perspectives. Will it work? Who knows, but it feels like a risky thing to bet a company&nbsp; on.&nbsp; Read more: Parler CEO says platform will &lsquo;come back strong&rsquo; with changes to keep users safe while respecting free speech (FOX News).###################################################



Google breaks the trillion-parameter ceiling with the Switch Transformer:&hellip;The best part? It seems to be reasonably efficient&hellip;Google has built the Switch Transformer, a more efficient variant of the Transformer. Switch Transformers are designed &ldquo;to maximize the parameter count of a Transformer model in a simple and computationally efficient way&rdquo;. The idea is that you can keep compute constant and cram more parameters into your network and still see performance gains.Does it work: Switch Transformers seem to be more efficient than standard ones; in a bakeoff between a model trained using a few of these &lsquo;Switch&rsquo; layers versus ones that use dense layers (T5-Base and T5-Large), Google shows the Switch is more efficient. The company also experiments with distilling Switch Transformers (which seems to work). They also show significant performance improvements on challenging tasks like GLUE, SQuAD, Winogrande, and ARC, with Switch-based systems outperforming T5 ones consistently.One treeeelion parameters: Google tests out its ideas by training a 395 billion and 1.6 trillion parameter Switch transformer (far in excess of GPT-3, which at 175 billion parameters is the largest (publicly) deployed language model on the planet. These mammoth systems display good performance properties (as one would expect), while also appearing to have some efficiency gains over systems trained solely on standard dense transformers.Why this matters: AI is moving into its industrial era &ndash; big companies are developing far more capable AI systems than in the past. Studies like this give us a sense of the limits of scaling (there don&rsquo;t seem to be many yet) as well as outlining some ways to improve efficiency while scaling. It might seem odd to call this an intrinsically political act, but it kind of is &ndash; right now, a variety of AI systems are being trained on slices of the internet, developed using substantial amounts of capital by a tiny set of people, then deployed widely. We live in interesting times!&nbsp; Read more: Switch Transformers: Scaling to Trilliong Parameter Models with Simple and Efficient Sparsity (arXiv).&nbsp; Check out a thread on Twitter from Google Cloud&rsquo;s Barret Zoph for more (Twitter).&nbsp; Get code related to this paper here (GitHub).###################################################South Korean chatbot blows up in public:&hellip;Luda chatbot gives off-color responses around sex, race&hellip;South Korean startup Scatter Lab has pulled an AI-based chatbot offline after the system started spewing sexist and racist comments in response to user inputs. &ldquo;&rdquo;Yuck, I really hate them,&rdquo; the bot said in response to a question about transgender people,&rdquo; according to Vice.What went wrong: Luda was trained on the chatlogs from &lsquo;Science of Lab&rsquo;, an earlier project developed by Scatter Labs. Based on a skim of a few (Google Translated) Korean documents, it seems like the problem was the underlying generative language model responded to user inputs with responses that varied from the benign to the highly offensive &ndash; this could have been because of the data. Prior to the problems, Scatter Lab said in a press release that &lsquo;Luda&rsquo; was better at conversation than Google&rsquo;s &ldquo;Meena&rdquo; system (about Meena: Import AI 183)).What went EXTREMELY wrong: Scatter Labs is currently under investigation by the Korean Internet &amp; Security Agency (KISA) and the Personal Information Protection Committee, due to using user data to train its chatbot. Scatter Labs had also used this user data in an earlier model published to GitHub (which is currently not available).&nbsp; Read more: AI Chatbot Shut Down After Learning to Talk Like a Racist Asshole (VICE World News).&nbsp; Read Scatter Labs&rsquo; statement about Luda (official website, Korean).&nbsp; Find out more via the official apology FAQ (official website, Korean).&nbsp; Check out the press release where they compare their technology to Google&rsquo;s &lsquo;Meena&rsquo; bot (Artificial Intelligence Times, Korean).###################################################Need help evaluating your NLP model? Try robustness gym:&hellip;Toolkit aims to turn model evaluation from an art to a science&hellip;Language models have got pretty good recently (see: BERT, GPT2, GPT3, Google&rsquo;s above-mentioned Switch Transformer being used for pre-trained models, etc). That means people are beginning to deploy them for a variety of purposes, ranging from classifying text to generating text. But these language models are huge generative models with complex capability surfaces, which means it is challenging to characterize their safety for a given usecase without doing a lot of direct experimentation.&nbsp; As all scientists know, setting up experiments is finicky work, and different labs and companies will have their own approaches to doing experimental design. This makes it hard to develop common standards for evaluating models. Enter: Robustness Gym, software built by people at Stanford, Salesforce, and UNC-Chapel Hill to provide a standard system for testing and evaluating models.What can Robustness Gym do? The software helps people do experimental design, initial evaluations of models across a range of dimensions (safety, different evaluation sets, resilience to various types of &lsquo;attack), and it produces a &lsquo;robustness report&rsquo; for any given model being analyzed. You can get the code for Robustness Gym from GitHub.Does Robustness Gym tell us anything useful? They use the …"

---

### Import AI 232: Google trains a trillion parameter model; South Korean chatbot blows up; AI doesn’t use as much electricity as you think

Uh-oh, Parler is about to step on a big &lsquo;ol algorithm rake:&hellip;CEO says algorithms can filter hate speech. Good luck with that!&hellip;Parler, the social network used by far right activists and subsequently pulled offline due to failing to meet T&amp;Cs from a variety of infrastructure services (including Amazon Web Services), has a plan to come back: it&rsquo;s going to use algorithms to filter hate speech on the service. Uh oh!&ldquo;We will be taking more algorithmic approaches to content but doing it to respect people&rsquo;s privacy, too,&rdquo; Parler CEO John Matz told FOX News. &ldquo;Will be having algorithms look at all the content &hellip; to try and predict whether it&rsquo;s a terms-of-service violation so we can adjust quicker and the most egregious things can get taken down&rdquo;.Algorithms != editors: If you want to use algorithms to moderate hate speech, you&rsquo;re going to get into the fun questions that entails. These include:&ndash; Can your algorithms effectively tell the difference between hate speech and satire of hate speech?&ndash; Are you comfortable making judgement calls about the heuristics you will use to give initial biases to these algorithms?&ndash; How do you distinguish between acceptable and unacceptable words and phrases?Why this matters: Parler highlights the challenge of scale combined with contemporary economics &ndash; Parler operate(d) at a scale equivalent to things like large television networks, but did so with a tiny investment into its own humans. Traditional media organizations deal with issues of speech by having an editorial line which gets enforced by thousands of individual journalists and editors making subjective, qualitative decisions. It&rsquo;s imperfect, but put it this way: when you watch Fox, you know what you&rsquo;re getting, and when you watch the BBC, you know what you&rsquo;re getting, and you can intuit the biases of the humans behind the editorial decisions. Now, tiny companies are trying to use algorithms to substitute for this varied multitude of different human perspectives. Will it work? Who knows, but it feels like a risky thing to bet a company&nbsp; on.&nbsp; Read more: Parler CEO says platform will &lsquo;come back strong&rsquo; with changes to keep users safe while respecting free speech (FOX News).###################################################



Google breaks the trillion-parameter ceiling with the Switch Transformer:&hellip;The best part? It seems to be reasonably efficient&hellip;Google has built the Switch Transformer, a more efficient variant of the Transformer. Switch Transformers are designed &ldquo;to maximize the parameter count of a Transformer model in a simple and computationally efficient way&rdquo;. The idea is that you can keep compute constant and cram more parameters into your network and still see performance gains.Does it work: Switch Transformers seem to be more efficient than standard ones; in a bakeoff between a model trained using a few of these &lsquo;Switch&rsquo; layers versus ones that use dense layers (T5-Base and T5-Large), Google shows the Switch is more efficient. The company also experiments with distilling Switch Transformers (which seems to work). They also show significant performance improvements on challenging tasks like GLUE, SQuAD, Winogrande, and ARC, with Switch-based systems outperforming T5 ones consistently.One treeeelion parameters: Google tests out its ideas by training a 395 billion and 1.6 trillion parameter Switch transformer (far in excess of GPT-3, which at 175 billion parameters is the largest (publicly) deployed language model on the planet. These mammoth systems display good performance properties (as one would expect), while also appearing to have some efficiency gains over systems trained solely on standard dense transformers.Why this matters: AI is moving into its industrial era &ndash; big companies are developing far more capable AI systems than in the past. Studies like this give us a sense of the limits of scaling (there don&rsquo;t seem to be many yet) as well as outlining some ways to improve efficiency while scaling. It might seem odd to call this an intrinsically political act, but it kind of is &ndash; right now, a variety of AI systems are being trained on slices of the internet, developed using substantial amounts of capital by a tiny set of people, then deployed widely. We live in interesting times!&nbsp; Read more: Switch Transformers: Scaling to Trilliong Parameter Models with Simple and Efficient Sparsity (arXiv).&nbsp; Check out a thread on Twitter from Google Cloud&rsquo;s Barret Zoph for more (Twitter).&nbsp; Get code related to this paper here (GitHub).###################################################South Korean chatbot blows up in public:&hellip;Luda chatbot gives off-color responses around sex, race&hellip;South Korean startup Scatter Lab has pulled an AI-based chatbot offline after the system started spewing sexist and racist comments in response to user inputs. &ldquo;&rdquo;Yuck, I really hate them,&rdquo; the bot said in response to a question about transgender people,&rdquo; according to Vice.What went wrong: Luda was trained on the chatlogs from &lsquo;Science of Lab&rsquo;, an earlier project developed by Scatter Labs. Based on a skim of a few (Google Translated) Korean documents, it seems like the problem was the underlying generative language model responded to user inputs with responses that varied from the benign to the highly offensive &ndash; this could have been because of the data. Prior to the problems, Scatter Lab said in a press release that &lsquo;Luda&rsquo; was better at conversation than Google&rsquo;s &ldquo;Meena&rdquo; system (about Meena: Import AI 183)).What went EXTREMELY wrong: Scatter Labs is currently under investigation by the Korean Internet &amp; Security Agency (KISA) and the Personal Information Protection Committee, due to using user data to train its chatbot. Scatter Labs had also used this user data in an earlier model published to GitHub (which is currently not available).&nbsp; Read more: AI Chatbot Shut Down After Learning to Talk Like a Racist Asshole (VICE World News).&nbsp; Read Scatter Labs&rsquo; statement about Luda (official website, Korean).&nbsp; Find out more via the official apology FAQ (official website, Korean).&nbsp; Check out the press release where they compare their technology to Google&rsquo;s &lsquo;Meena&rsquo; bot (Artificial Intelligence Times, Korean).###################################################Need help evaluating your NLP model? Try robustness gym:&hellip;Toolkit aims to turn model evaluation from an art to a science&hellip;Language models have got pretty good recently (see: BERT, GPT2, GPT3, Google&rsquo;s above-mentioned Switch Transformer being used for pre-trained models, etc). That means people are beginning to deploy them for a variety of purposes, ranging from classifying text to generating text. But these language models are huge generative models with complex capability surfaces, which means it is challenging to characterize their safety for a given usecase without doing a lot of direct experimentation.&nbsp; As all scientists know, setting up experiments is finicky work, and different labs and companies will have their own approaches to doing experimental design. This makes it hard to develop common standards for evaluating models. Enter: Robustness Gym, software built by people at Stanford, Salesforce, and UNC-Chapel Hill to provide a standard system for testing and evaluating models.What can Robustness Gym do? The software helps people do experimental design, initial evaluations of models across a range of dimensions (safety, different evaluation sets, resilience to various types of &lsquo;attack), and it produces a &lsquo;robustness report&rsquo; for any given model being analyzed. You can get the code for Robustness Gym from GitHub.Does Robustness Gym tell us anything useful? They use the …