---

layout: post
category: threads
title: "[R] Why do knowledge distillation papers always use KL-divergence as constraints?"
date: 2020-12-16 04:37:27
link: https://vrhk.co/3muuNn2
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "I am studying some literatures on Knowledge distillation. Most of the time, they are targeting classification models, and perform knowledge..."

---

### [R] Why do knowledge distillation papers always use KL-divergence as constraints?

I am studying some literatures on Knowledge distillation. Most of the time, they are targeting classification models, and perform knowledge...