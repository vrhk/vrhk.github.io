---

layout: post
category: product
title: "Import AI 201: the facial recognition rebellion; how Amazon Go sees people; and the past&amp;present of YOLO"
date: 2020-06-16 14:36:38
link: https://vrhk.co/2Y5wH4e
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Could 2020 be the year where facial recognition gets some constraints?&hellip;Amazon, IBM, Microsoft, change facial recognition policies&hellip;This week, IBM said it no longer sells &ldquo;general purpose facial recognition or analysis software&rdquo;, and that it opposes the use of facial recognition for &ldquo;mass surveillance, racial profiling, violations of basic human rights and freedoms&rdquo;. After this, Amazon announced a one-year moratorium on police use of its facial recognition technology, &lsquo;Rekognition&rsquo;, then Microsoft said the next day it would not sell the technology to police departments in the U.S until a federal law exists that regulates the technology.&nbsp; These moves mark a change in mood for Western AI companies, which after years of heady business expansion have started to change the sorts of products they sell according to various pressures. I think the change started a while ago when employee outcry at Google led to the company pausing work on its drone-surveillance &lsquo;Maven&rsquo; projects for the US military. Now, it seems like companies are reconsidering their stances more broadly.&nbsp;
The backstory: Why is this happening? I think one of the main reasons is the intersection of our contemporary political moment with a few years of high-impact research into the biases exhibited by facial recognition systems. The project that started most of this was &lsquo;Gender Shades&lsquo;, which tested a variety of commercial facial recognition systems and found them all to display harmful biases. Dave Gershgorn at Medium has a good overview of this chain of events: How a 2018 Research Paper Led Amazon, Microsoft, and IBM to Curb Their Facial Recognition Programs (Medium, OneZero). Why this matters: &lsquo;Can we control technology?&rsquo; is a theme I write about a lot in for Import AI &ndash; it&rsquo;s a subtle question because usually the answer is some form of &lsquo;well, those people can, but what about them?&lsquo;. Right now, there&rsquo;s a very widespread, evidence-backed view that facial recognition has a bunch of harms, especially when deployed using contemporary (mostly faulty and/or brittle) AI systems. I&rsquo;m curious to see which companies step into the void left by the technology giants&rsquo; vacation &ndash; which companies will arbitrage their reputation for profits? And how might the actions of Amazon, IBM, and Microsoft shift perceptions in other countries, as well? &nbsp; Read more: Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification (PDF). &nbsp; Read more: We are implementing a one-year moratorium on police use of Rekognition (Amazon blog). &nbsp; Read more: IBM CEO&rsquo;s Letter to Congress on Racial Justice Reform (IBM). &nbsp; Read more: Microsoft says it won&rsquo;t sell facial recognition technology to US police departments (CNN).####################################################Who likes big models? Everyone likes big models! Says HuggingFace:&hellip;NLP startup publishes some recipes for training high-performance models&hellip;NLP startup Hugging Face has published research showing people how to train large, high-performance language models. (This post is based on earlier research published by OpenAI, Scaling Laws for Neural Language Models).The takeaways: The HuggingFace post has a couple of takeaways worth highlighting: Big models are surprisingly efficient, and &ldquo;optimizing model size for speed lowers training costs&rdquo;. More interesting to me is the practical mindset of the takeaways, which I think speaks to the broader maturity of the large-scale language model space at this point. Why this matters: Last year, people said NLP was having its &lsquo;ImageNet moment&lsquo;. Well, we know what happened with ImageNet &ndash; following the landmark 2012 results, the field of computer vision evolved to use deep learning-based methods, unleashing a wave of applications on the world. Perhaps that&rsquo;s beginning to happen with NLP now? &nbsp; Read more: How Big Should My Language Model Be? (Hugging Face).. ####################################################Research papers that sound like poetry, edition #1:Deep Neural Network BasedReal-time Kiwi Fruit Flower DetectionIn an Orchard Environment. &ndash; University of Auckland, New Zealand. ArXiv preprint.####################################################The long, strange life of the YOLO object detection software:&hellip; Multiple owners, ethical concerns, ML brand name wars, and so much more!&hellip;YOLO, short for You Only Look Once, is a widely-used software package for object detection using machine learning. Tons of developers use YOLO because it is fast, well documented, and open source. Now, there&rsquo;s a new version of the software &ndash; and the news isn&rsquo;t the version, but who developed it, and why.The original YOLO went through three versions, then in 2019 its creator, a researcher named Joseph Redmon, said they had stopped doing research into computer vision due to worries about its usage and had therefore stopped developing YOLO; a few months later a developer published a new, improved version of YOLO called YOLOv4 (Import AI #196, highlighting how tricky it can be to control technology like this. Now, there&rsquo;s controversy as another developer has stepped in with an end-to-end YOLO implementation in PyTorch that they call YOLOv5 &ndash; there are some controversies about whether this is a true successor to v4 due to some shady benchmarking and marketing methods, but the essential point remains: the original creator stopped due to ethical concerns, and now multiple creators have moved this forward.  &nbsp; It&rsquo;s all a bit reminiscent of (spoiler alert) the ending of the book &lsquo;Fight Club&rsquo;, where the protagonist who had formed some underground &lsquo;fight clubs&rsquo; and an associated cult swore off their creation, wakes up in a medical facility, and discovers that most of the staff are continuing to host and develop &lsquo;Fight Clubs&rsquo; &ndash; the creation has transcended its creator, and can no longer be controlled.  &nbsp; Read: The GitHub comments from YOLOv4 developer AlexeyAB (GitHub). &nbsp; Some context on the controversy: Responding to the Controversy about YOLOv5 (roboflow, blog).&nbsp;
####################################################3D modelling + AI = cheap data for better surveillance:&hellip;Where &lsquo;Amazon Go&rsquo; explores today, the world will end up tomorrow&hellip;Researchers with Amazon Go, the team inside Amazon that builds the technology for its no-cash-required walk in-walk out shops, are trying to generate synthetic images of groups of people, to help them train more robust models for scene mapping and dense depth estimation.  &nbsp; The research outlines &ldquo;a fully controllable approach for generating photorealistic images of humans performing various activities. To the best of our knowledge, this is the first system capable of generating complex human interactions from appearance-based semantic segmentation label maps.&rdquo;How they did it: Like a lot of synthetic data experiments, this work relies on a multi-stage set of actions, where it starts by training a scene parsing model over a mixture of real and synthetic data, then uses this model to automatically label frames from the data distribution that you want to create fake imagery in, then they use a cGAN to generate a realistic image from the data generated by the scene parsing model.  &nbsp; Crossing the reality gap: &rdquo; We emphasize that we cross the domain gap three times,&rdquo; the researchers write. &ldquo;First, we cross from synthetic to real by training a human parsing model on synthetic images and apply it to real images from the target domain. Second, we train a generative model on real images for the opposite task &ndash; to create a realistic image from a synthesized semantic segmentation map. Third, we train a semantic segmenta…"

---

### Import AI 201: the facial recognition rebellion; how Amazon Go sees people; and the past&amp;present of YOLO

Could 2020 be the year where facial recognition gets some constraints?&hellip;Amazon, IBM, Microsoft, change facial recognition policies&hellip;This week, IBM said it no longer sells &ldquo;general purpose facial recognition or analysis software&rdquo;, and that it opposes the use of facial recognition for &ldquo;mass surveillance, racial profiling, violations of basic human rights and freedoms&rdquo;. After this, Amazon announced a one-year moratorium on police use of its facial recognition technology, &lsquo;Rekognition&rsquo;, then Microsoft said the next day it would not sell the technology to police departments in the U.S until a federal law exists that regulates the technology.&nbsp; These moves mark a change in mood for Western AI companies, which after years of heady business expansion have started to change the sorts of products they sell according to various pressures. I think the change started a while ago when employee outcry at Google led to the company pausing work on its drone-surveillance &lsquo;Maven&rsquo; projects for the US military. Now, it seems like companies are reconsidering their stances more broadly.&nbsp;
The backstory: Why is this happening? I think one of the main reasons is the intersection of our contemporary political moment with a few years of high-impact research into the biases exhibited by facial recognition systems. The project that started most of this was &lsquo;Gender Shades&lsquo;, which tested a variety of commercial facial recognition systems and found them all to display harmful biases. Dave Gershgorn at Medium has a good overview of this chain of events: How a 2018 Research Paper Led Amazon, Microsoft, and IBM to Curb Their Facial Recognition Programs (Medium, OneZero). Why this matters: &lsquo;Can we control technology?&rsquo; is a theme I write about a lot in for Import AI &ndash; it&rsquo;s a subtle question because usually the answer is some form of &lsquo;well, those people can, but what about them?&lsquo;. Right now, there&rsquo;s a very widespread, evidence-backed view that facial recognition has a bunch of harms, especially when deployed using contemporary (mostly faulty and/or brittle) AI systems. I&rsquo;m curious to see which companies step into the void left by the technology giants&rsquo; vacation &ndash; which companies will arbitrage their reputation for profits? And how might the actions of Amazon, IBM, and Microsoft shift perceptions in other countries, as well? &nbsp; Read more: Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification (PDF). &nbsp; Read more: We are implementing a one-year moratorium on police use of Rekognition (Amazon blog). &nbsp; Read more: IBM CEO&rsquo;s Letter to Congress on Racial Justice Reform (IBM). &nbsp; Read more: Microsoft says it won&rsquo;t sell facial recognition technology to US police departments (CNN).####################################################Who likes big models? Everyone likes big models! Says HuggingFace:&hellip;NLP startup publishes some recipes for training high-performance models&hellip;NLP startup Hugging Face has published research showing people how to train large, high-performance language models. (This post is based on earlier research published by OpenAI, Scaling Laws for Neural Language Models).The takeaways: The HuggingFace post has a couple of takeaways worth highlighting: Big models are surprisingly efficient, and &ldquo;optimizing model size for speed lowers training costs&rdquo;. More interesting to me is the practical mindset of the takeaways, which I think speaks to the broader maturity of the large-scale language model space at this point. Why this matters: Last year, people said NLP was having its &lsquo;ImageNet moment&lsquo;. Well, we know what happened with ImageNet &ndash; following the landmark 2012 results, the field of computer vision evolved to use deep learning-based methods, unleashing a wave of applications on the world. Perhaps that&rsquo;s beginning to happen with NLP now? &nbsp; Read more: How Big Should My Language Model Be? (Hugging Face).. ####################################################Research papers that sound like poetry, edition #1:Deep Neural Network BasedReal-time Kiwi Fruit Flower DetectionIn an Orchard Environment. &ndash; University of Auckland, New Zealand. ArXiv preprint.####################################################The long, strange life of the YOLO object detection software:&hellip; Multiple owners, ethical concerns, ML brand name wars, and so much more!&hellip;YOLO, short for You Only Look Once, is a widely-used software package for object detection using machine learning. Tons of developers use YOLO because it is fast, well documented, and open source. Now, there&rsquo;s a new version of the software &ndash; and the news isn&rsquo;t the version, but who developed it, and why.The original YOLO went through three versions, then in 2019 its creator, a researcher named Joseph Redmon, said they had stopped doing research into computer vision due to worries about its usage and had therefore stopped developing YOLO; a few months later a developer published a new, improved version of YOLO called YOLOv4 (Import AI #196, highlighting how tricky it can be to control technology like this. Now, there&rsquo;s controversy as another developer has stepped in with an end-to-end YOLO implementation in PyTorch that they call YOLOv5 &ndash; there are some controversies about whether this is a true successor to v4 due to some shady benchmarking and marketing methods, but the essential point remains: the original creator stopped due to ethical concerns, and now multiple creators have moved this forward.  &nbsp; It&rsquo;s all a bit reminiscent of (spoiler alert) the ending of the book &lsquo;Fight Club&rsquo;, where the protagonist who had formed some underground &lsquo;fight clubs&rsquo; and an associated cult swore off their creation, wakes up in a medical facility, and discovers that most of the staff are continuing to host and develop &lsquo;Fight Clubs&rsquo; &ndash; the creation has transcended its creator, and can no longer be controlled.  &nbsp; Read: The GitHub comments from YOLOv4 developer AlexeyAB (GitHub). &nbsp; Some context on the controversy: Responding to the Controversy about YOLOv5 (roboflow, blog).&nbsp;
####################################################3D modelling + AI = cheap data for better surveillance:&hellip;Where &lsquo;Amazon Go&rsquo; explores today, the world will end up tomorrow&hellip;Researchers with Amazon Go, the team inside Amazon that builds the technology for its no-cash-required walk in-walk out shops, are trying to generate synthetic images of groups of people, to help them train more robust models for scene mapping and dense depth estimation.  &nbsp; The research outlines &ldquo;a fully controllable approach for generating photorealistic images of humans performing various activities. To the best of our knowledge, this is the first system capable of generating complex human interactions from appearance-based semantic segmentation label maps.&rdquo;How they did it: Like a lot of synthetic data experiments, this work relies on a multi-stage set of actions, where it starts by training a scene parsing model over a mixture of real and synthetic data, then uses this model to automatically label frames from the data distribution that you want to create fake imagery in, then they use a cGAN to generate a realistic image from the data generated by the scene parsing model.  &nbsp; Crossing the reality gap: &rdquo; We emphasize that we cross the domain gap three times,&rdquo; the researchers write. &ldquo;First, we cross from synthetic to real by training a human parsing model on synthetic images and apply it to real images from the target domain. Second, we train a generative model on real images for the opposite task &ndash; to create a realistic image from a synthesized semantic segmentation map. Third, we train a semantic segmenta…