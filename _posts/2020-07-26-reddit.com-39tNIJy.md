---

layout: post
category: threads
title: "[D] Breaking the Quadratic Attention Bottleneck in Transformers?"
date: 2020-07-26 01:17:33
link: https://vrhk.co/39tNIJy
image: https://external-preview.redd.it/pQMYenOPcBbGgxrsWrkaNMSDA-UeepTUyCKe17Y3CxM.jpg?width=1095&height=573.298429319&auto=webp&crop=1095:573.298429319,smart&s=38e70c6769272ba986882bfdc58219e23fbaba33
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "One of the most frustrating limitations of GPT-3 is the context window: 2048 BPEs runs out fast when you start prompt programming something hard,..."

---

### [D] Breaking the Quadratic Attention Bottleneck in Transformers?

One of the most frustrating limitations of GPT-3 is the context window: 2048 BPEs runs out fast when you start prompt programming something hard,...