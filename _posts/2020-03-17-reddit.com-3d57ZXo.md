---

layout: post
category: threads
title: "[R] Efficient Content-Based Sparse Attention with Routing Transformers. Great performance on Wikitext-103, enwik8, and image generation task on ImageNet-64, while reducing overall complexity of attention from O(n² d) to O(n√n d)"
date: 2020-03-17 08:17:35
link: https://vrhk.co/3d57ZXo
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Posted in r/MachineLearning by u/hardmaru • 1 point and 1 comment"

---

### [R] Efficient Content-Based Sparse Attention with Routing Transformers. Great performance on Wikitext-103, enwik8, and image generation task on ImageNet-64, while reducing overall complexity of attention from O(n² d) to O(n√n d)

Posted in r/MachineLearning by u/hardmaru • 1 point and 1 comment