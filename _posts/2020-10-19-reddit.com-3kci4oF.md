---

layout: post
category: threads
title: "[D] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Paper Explained)"
date: 2020-10-19 16:07:32
link: https://vrhk.co/3kci4oF
image: https://external-preview.redd.it/shMNpJQ-0JPJUXCEG__6RSp4QlMpW3q63t0jevy-QBw.jpg?width=480&height=251.308900524&auto=webp&crop=480:251.308900524,smart&s=7cd342f898c070110b5e4dead0e2da9338373c5c
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "[<https://youtu.be/j9toSIRf4RI>](<https://youtu.be/j9toSIRf4RI>) I spent last Saturday to revisit the BERT original paper and made a video to note..."

---

### [D] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Paper Explained)

[<https://youtu.be/j9toSIRf4RI>](<https://youtu.be/j9toSIRf4RI>) I spent last Saturday to revisit the BERT original paper and made a video to note...