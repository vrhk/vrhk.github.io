---

layout: post
category: threads
title: "[D] How does the ULM subword tokenization avoid just splitting every word into single characters?"
date: 2020-07-01 01:17:32
link: https://vrhk.co/2BQxUnF
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "According to the [paper](<https://arxiv.org/pdf/1804.10959.pdf>): &gt; The probability of a subword sequence X = (x_1, x_2...) is formulated as the..."

---

### [D] How does the ULM subword tokenization avoid just splitting every word into single characters?

According to the [paper](<https://arxiv.org/pdf/1804.10959.pdf>): &gt; The probability of a subword sequence X = (x_1, x_2...) is formulated as the...