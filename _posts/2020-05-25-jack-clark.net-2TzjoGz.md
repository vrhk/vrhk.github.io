---

layout: post
category: product
title: "Import AI 199: Drone cinematographer; spotting toxic content with 4chan word embeddings; plus, a million text annotations help cars see"
date: 2020-05-25 18:06:38
link: https://vrhk.co/2TzjoGz
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Get ready for the droneswarm cinematographer(s):&hellip;But be prepared to wait awhile; we&rsquo;re in the Wright Brothers era&hellip;Today, people use drones to help film tricky things in a variety of cinematic settings. These drones are typically human-piloted, though there are the beginnings of some mobile drones that can autonomously follow people for sport purposes (e.g, Skydio). How might cinema change as people begin to use drones to film more and more complex shots? That&rsquo;s an idea inherent to new research from the University of Seville, which outlines &ldquo;a multi-UAV approach for autonomous cinematography planning, aimed at filming outdoor events such as cycling or boat races&rdquo;.The proposed system gives a human director software that they can use to lay out specific shots &ndash; e.g, drones flying to certain locations, or following people across a landscape &ndash; then the software figures out how to coordinate multiple drones to pull of the shot. This is a complex problem, since drones typically have short battery lives, and are themselves machines. The researchers use a graph-based solution to the problem that can find optimal solutions for single drones and approximate solutions for multi-drones scenarios. &ldquo;We focus on high-level planning. This means how to distribute filming tasks among the team members,&rdquo; they write. They run the drones through a couple of basic in-the-wild experiments, involving collectively filming a single object from multiple angles, as well as filming a cyclist and relaying the shot from one drone to the other. The latter experiment has an 8 second gap, as the drones need to create space for eachother for safety reasons, which means there&rsquo;s not a perf overlap during the filming handover. Why this matters: This research is very early &ndash; as the video shows &ndash; but drones are a burgeoning consumer product, and this research is backed up a by an EU-wide project named &lsquo;MULTIDRONE&lsquo; which is pouring money into increasing drone capabilities in this area.  &nbsp; Read more: Autonomous Planning for Multiple Aerial Cinematographers (arXiv).&nbsp; &nbsp; Video: Multi-drone cinematographers are coming, but they&rsquo;re a long way off (YouTube).####################################################Want to give your machines a sense of fashion? Try MMFashion:&hellip;Free software includes pre-trained models for specific fashion-analysis tasks&hellip;Researchers with the Chinese University of Hong Kong have released a new version of MMFashion, an open source toolbox for using AI to analyze images for clothing and other fashion-related attributes. MMFashion v0.4: The software is implemented in Pytorch and ships with pre-trained models for specific fashion-related tasks. The latest version of the software has the following capabilities:&ndash; Fashion attribute prediction &ndash; predicts attributes of clothing, eg, a print, t-shirt, etc.&ndash; Fashion recognition and retrieval &ndash; determines if two images belong from the same clothing line.&ndash; Fashion Landmark Detection &ndash; detect neckline, hemline, cuffs, etc.&ndash; Fashion Parsing and Segmentation &ndash; detect and segment clothing / fashion objects.&ndash; Fashion Compatibility and Recommendation &ndash; recommend items.Model Zoo: You can see the list of models MMFashion currently ships with here, along with their performance on baseline tasks. Why this matters: I think we&rsquo;re on the verge of being able to build large-scale &lsquo;culture detectors&rsquo; &ndash; systems that automatically analyze a given set of people for various traits, like the clothing they&rsquo;re wearing, or their individual tastes (and how they change over time). Software like MMFashion feels like a very early step towards these systems, and I can imagine retailers increasingly using AI techniques to both understand what clothes people are wearing, as well as figure out how to recommend more visually similar clothes to them.  &nbsp; Get the code here (mmfashion Github). &nbsp; Read more: MMFashion: An Open-Source Toolbox for Visual Fashion Analysis (arXiv).####################################################
Spotting toxic content with 4chan and 8chan embeddings:&hellip;Bottling up websites with word embeddings&hellip;Word embeddings are kind of amazing &ndash; they&rsquo;re a way you can develop a semantic fingerprint of a corpus of text, letting you understand how different words relate to one another in it. So it might seem like a strange idea to use word embeddings to bottle up the offensive shitposting on 4chan&rsquo;s &lsquo;/pol&rsquo; board &ndash; message boards notorious for their unregulated, frequently offensive speech, and association with acts of violent extremism (e.g, the Christchurch shooting). Yet that&rsquo;s what a team of researchers from AI startup Textgain have done. The idea, they say, is people can use the word embedding filter to help them build datasets of potentially offensive words, or to detect them (via being deployed in toxicity filters of some kind). The dataset: To build the embedding model, the researchers gathered around 30 million posts from the /pol subforum on 4chan and 8chan, with 90% of the corpus coming from 4chan and 10% from 8chan. The underlying dataset is available on request, they write. Things that make you go &lsquo;eugh&rsquo;: The (short) research paper is worth a read for understanding how the thing works in practice. Though, be warned, the examples used include testing out toxicity detection with the n-word and &lsquo;cuck&rsquo;. However, it gives us a sense of how this technology can be put to work.  &nbsp; Read more: 4chan &amp; 8chan embeddings (arXiv). &nbsp; Get the embeddings in binary and raw format from here (textgain official website).####################################################Want to make your own weird robot texts? Try out this free &lsquo;aitextgen&rsquo; software:&hellip;Plus, finetune GPT-2 in your browser via a Google colab&hellip;AI developer Max Woolf has spent months building free software to make it easy for people to mess around with generating text via GPT-2 language models. This week, he updated the open source software to make it faster and easier to setup. And best of all, he has released a Colab notebook that handles all the fiddly parts of training and finetuning simple GPT-2 text models: try it out now and brew up your own custom language model!Why this matters: Easy tools encourage experimentation, and experimentation (sometimes) yields invention. &nbsp;  &nbsp; Get the code (aitextgen, GitHub) &nbsp; Want to train it in your browser? Use a Google colab here (Google colab). &nbsp; Read the docs here (aitextgen docs website).####################################################
Want self-driving cars that can read signs? The RoadText-1K dataset might help:&hellip;Bringing us (incrementally) closer to the era of robots that can see and read&hellip;Self-driving cars need to be able to read; a new dataset from the International Institute of Information Technology in Hyderabad, India, and the Autonomous University of Barcelona, might teach them how. RoadText-1K: The RoadText-1K dataset consists of 1000 videos that are around 10 seconds long each. Each video is from the BDD100K dataset, which is made up of video taken from the driver&rsquo;s perspective of cars as they travel around the US. BDD is from the Berkeley Deep Drive project, which sees car companies and the eponymous university collaborate on open research for self-driving cars. &nbsp; Each frame in each video in RoadText-1K has been annotated with bounding boxes around the objects containing text, giving researchers a dataset full of numberplates, street signs, road signs, and more. In total, the dataset contains 1,280,613 instances of text across 300,000 frames. Why this matters: Slowly and steadily, we&rsquo;re making the world around us legible to computer vision. Much of this work is …"

---

### Import AI 199: Drone cinematographer; spotting toxic content with 4chan word embeddings; plus, a million text annotations help cars see

Get ready for the droneswarm cinematographer(s):&hellip;But be prepared to wait awhile; we&rsquo;re in the Wright Brothers era&hellip;Today, people use drones to help film tricky things in a variety of cinematic settings. These drones are typically human-piloted, though there are the beginnings of some mobile drones that can autonomously follow people for sport purposes (e.g, Skydio). How might cinema change as people begin to use drones to film more and more complex shots? That&rsquo;s an idea inherent to new research from the University of Seville, which outlines &ldquo;a multi-UAV approach for autonomous cinematography planning, aimed at filming outdoor events such as cycling or boat races&rdquo;.The proposed system gives a human director software that they can use to lay out specific shots &ndash; e.g, drones flying to certain locations, or following people across a landscape &ndash; then the software figures out how to coordinate multiple drones to pull of the shot. This is a complex problem, since drones typically have short battery lives, and are themselves machines. The researchers use a graph-based solution to the problem that can find optimal solutions for single drones and approximate solutions for multi-drones scenarios. &ldquo;We focus on high-level planning. This means how to distribute filming tasks among the team members,&rdquo; they write. They run the drones through a couple of basic in-the-wild experiments, involving collectively filming a single object from multiple angles, as well as filming a cyclist and relaying the shot from one drone to the other. The latter experiment has an 8 second gap, as the drones need to create space for eachother for safety reasons, which means there&rsquo;s not a perf overlap during the filming handover. Why this matters: This research is very early &ndash; as the video shows &ndash; but drones are a burgeoning consumer product, and this research is backed up a by an EU-wide project named &lsquo;MULTIDRONE&lsquo; which is pouring money into increasing drone capabilities in this area.  &nbsp; Read more: Autonomous Planning for Multiple Aerial Cinematographers (arXiv).&nbsp; &nbsp; Video: Multi-drone cinematographers are coming, but they&rsquo;re a long way off (YouTube).####################################################Want to give your machines a sense of fashion? Try MMFashion:&hellip;Free software includes pre-trained models for specific fashion-analysis tasks&hellip;Researchers with the Chinese University of Hong Kong have released a new version of MMFashion, an open source toolbox for using AI to analyze images for clothing and other fashion-related attributes. MMFashion v0.4: The software is implemented in Pytorch and ships with pre-trained models for specific fashion-related tasks. The latest version of the software has the following capabilities:&ndash; Fashion attribute prediction &ndash; predicts attributes of clothing, eg, a print, t-shirt, etc.&ndash; Fashion recognition and retrieval &ndash; determines if two images belong from the same clothing line.&ndash; Fashion Landmark Detection &ndash; detect neckline, hemline, cuffs, etc.&ndash; Fashion Parsing and Segmentation &ndash; detect and segment clothing / fashion objects.&ndash; Fashion Compatibility and Recommendation &ndash; recommend items.Model Zoo: You can see the list of models MMFashion currently ships with here, along with their performance on baseline tasks. Why this matters: I think we&rsquo;re on the verge of being able to build large-scale &lsquo;culture detectors&rsquo; &ndash; systems that automatically analyze a given set of people for various traits, like the clothing they&rsquo;re wearing, or their individual tastes (and how they change over time). Software like MMFashion feels like a very early step towards these systems, and I can imagine retailers increasingly using AI techniques to both understand what clothes people are wearing, as well as figure out how to recommend more visually similar clothes to them.  &nbsp; Get the code here (mmfashion Github). &nbsp; Read more: MMFashion: An Open-Source Toolbox for Visual Fashion Analysis (arXiv).####################################################
Spotting toxic content with 4chan and 8chan embeddings:&hellip;Bottling up websites with word embeddings&hellip;Word embeddings are kind of amazing &ndash; they&rsquo;re a way you can develop a semantic fingerprint of a corpus of text, letting you understand how different words relate to one another in it. So it might seem like a strange idea to use word embeddings to bottle up the offensive shitposting on 4chan&rsquo;s &lsquo;/pol&rsquo; board &ndash; message boards notorious for their unregulated, frequently offensive speech, and association with acts of violent extremism (e.g, the Christchurch shooting). Yet that&rsquo;s what a team of researchers from AI startup Textgain have done. The idea, they say, is people can use the word embedding filter to help them build datasets of potentially offensive words, or to detect them (via being deployed in toxicity filters of some kind). The dataset: To build the embedding model, the researchers gathered around 30 million posts from the /pol subforum on 4chan and 8chan, with 90% of the corpus coming from 4chan and 10% from 8chan. The underlying dataset is available on request, they write. Things that make you go &lsquo;eugh&rsquo;: The (short) research paper is worth a read for understanding how the thing works in practice. Though, be warned, the examples used include testing out toxicity detection with the n-word and &lsquo;cuck&rsquo;. However, it gives us a sense of how this technology can be put to work.  &nbsp; Read more: 4chan &amp; 8chan embeddings (arXiv). &nbsp; Get the embeddings in binary and raw format from here (textgain official website).####################################################Want to make your own weird robot texts? Try out this free &lsquo;aitextgen&rsquo; software:&hellip;Plus, finetune GPT-2 in your browser via a Google colab&hellip;AI developer Max Woolf has spent months building free software to make it easy for people to mess around with generating text via GPT-2 language models. This week, he updated the open source software to make it faster and easier to setup. And best of all, he has released a Colab notebook that handles all the fiddly parts of training and finetuning simple GPT-2 text models: try it out now and brew up your own custom language model!Why this matters: Easy tools encourage experimentation, and experimentation (sometimes) yields invention. &nbsp;  &nbsp; Get the code (aitextgen, GitHub) &nbsp; Want to train it in your browser? Use a Google colab here (Google colab). &nbsp; Read the docs here (aitextgen docs website).####################################################
Want self-driving cars that can read signs? The RoadText-1K dataset might help:&hellip;Bringing us (incrementally) closer to the era of robots that can see and read&hellip;Self-driving cars need to be able to read; a new dataset from the International Institute of Information Technology in Hyderabad, India, and the Autonomous University of Barcelona, might teach them how. RoadText-1K: The RoadText-1K dataset consists of 1000 videos that are around 10 seconds long each. Each video is from the BDD100K dataset, which is made up of video taken from the driver&rsquo;s perspective of cars as they travel around the US. BDD is from the Berkeley Deep Drive project, which sees car companies and the eponymous university collaborate on open research for self-driving cars. &nbsp; Each frame in each video in RoadText-1K has been annotated with bounding boxes around the objects containing text, giving researchers a dataset full of numberplates, street signs, road signs, and more. In total, the dataset contains 1,280,613 instances of text across 300,000 frames. Why this matters: Slowly and steadily, we&rsquo;re making the world around us legible to computer vision. Much of this work is …