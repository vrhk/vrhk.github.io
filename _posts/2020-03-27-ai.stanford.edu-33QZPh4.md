---

layout: post
category: research
title: "BERT, ELMo, &amp; GPT-2: How contextual are contextualized word representations?"
date: 2020-03-27 12:50:57
link: https://stanford.io/33QZPh4
image: http://ai.stanford.edu/blog/assets/img/posts/2020-03-24-contextual/teaser.png
domain: ai.stanford.edu
author: "SAIL Blog"
icon: http://ai.stanford.edu/blog/assets/img/favicon-32x32.png
excerpt: "Incorporating context into word embeddings - as exemplified by BERT, ELMo, and GPT-2 - has proven to be a watershed idea in NLP. Replacing static vectors (e.g., word2vec) with contextualized word representations has led to significant improvements on virtually every NLP task."

---

### BERT, ELMo, &amp; GPT-2: How contextual are contextualized word representations?

Incorporating context into word embeddings - as exemplified by BERT, ELMo, and GPT-2 - has proven to be a watershed idea in NLP. Replacing static vectors (e.g., word2vec) with contextualized word representations has led to significant improvements on virtually every NLP task.