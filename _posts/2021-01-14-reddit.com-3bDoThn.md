---

layout: post
category: threads
title: "[D] In self-attention, does adding positional encoding to ReLU features make sense¿"
date: 2021-01-14 13:37:32
link: https://vrhk.co/3bDoThn
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Given the ReLU features can make the positional encoding [0, 1] seem negligible. Would sigmoid make a better ground for adding positional encoding?"

---

### [D] In self-attention, does adding positional encoding to ReLU features make sense¿

Given the ReLU features can make the positional encoding [0, 1] seem negligible. Would sigmoid make a better ground for adding positional encoding?