---

layout: post
category: threads
title: "[D] Simpler alternatives to multihead self-attention"
date: 2020-10-18 07:47:29
link: https://vrhk.co/3dD2UGk
image: https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=400&height=209.42408377&auto=webp&crop=400:209.42408377,smart&s=ebc3789682ec67e4c4dcb021811183d3fe0cb92e
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "I've seen attention and more specifically transformer block being used in *a lot* of problems lately, and I am honestly quite confused as to why..."

---

### [D] Simpler alternatives to multihead self-attention

I've seen attention and more specifically transformer block being used in *a lot* of problems lately, and I am honestly quite confused as to why...