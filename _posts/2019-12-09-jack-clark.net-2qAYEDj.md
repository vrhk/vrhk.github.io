---

layout: post
category: product
title: "Import AI 176: Are language models full of hot air? Test them on BLiMP; Facebook notches up Hanabi win; plus, TensorFlow woes."
date: 2019-12-09 15:16:20
link: https://vrhk.co/2qAYEDj
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "First, poker. Now: Hanabi! Facebook notches up another AI game-playing win:&hellip;The secret? Better search&hellip;Facebook AI researchers have developed an AI system that gets superhuman performance in Hanabi, a collaborative card game that requires successful players to &ldquo;understand the beliefs and intentions of other players, because they can&rsquo;t see the same cards their teammates see and can only share very limited hints with eachother&rdquo;. The Facebook-developed system is based on Pluribus, the CMU/Facebook-developed machine that defeated players in six-player no-limit Hold&rsquo;em earlier this year.&nbsp;
Why Hanabi? In February, researchers with Google and DeepMind published a paper arguing that the card game &lsquo;Hanabi&rsquo; should be treated as a milestone in AI research, following the success of contemporary systems in domains like Go, Atari, and Poker. Hanabi is a useful milestone because along with requiring reasoning with partial information, the game also &ldquo;elevates reasoning about the beliefs and intentions of other agents to the foreground,&rdquo; wrote the researchers at the time.&nbsp;
How they did it: The Facebook-developed system relies on what the company calls multi-agent search to work. Multi-agent search works roughly like this: agent a) looks at the state of the gameworld and tries to work out the optimal move to make using Monte Carlo rollouts to do the calculation; agent b) looks at the move agent a) made and uses that to infer what cards agent a) had, then uses this knowledge to inform the strategy agent b) picks; agent a) then looks at moves made by agent b) and studies b)&rsquo;s prior moves to estimate what cards b) has and what cards b) thinks agent a) has, then uses this to generate a strategy.&nbsp;&nbsp;&nbsp;This is a bloody expensive procedure, as it involves a ballooning quantity of calculations as the complexity fo the game increases; the Facebook researchers default to a computationally-cheaper but less effective single-agent search procedure for most of the time, only using multi-agent search periodically.&nbsp;
Why this matters: We&rsquo;re not interested in this system because it can play Hanabi &ndash; we&rsquo;re interested in understanding general approaches to improving the performance of AI systems that interact with other systems in messy, strategic contexts. The takeaway from Facebook&rsquo;s research is that if you have access to enough information to be able to simulate the game state, then you can layer on search strategies on top of big blobs of neural inference layers, and use this to create somewhat generic, strategic agents. &ldquo;Adding search to RL can dramatically improve performance beyond what can be achieved through RL alone,&rdquo; they write. &ldquo;We have now shown that this approach can work in cooperative environments as well.&rdquo;&nbsp;&nbsp;&nbsp;Read more: Building AI that can master complex cooperative games with hidden information (Facebook AI research blog).&nbsp;&nbsp;&nbsp;Read more: The Hanabi Challenge: A New Frontier for AI Research (Arxiv).&nbsp;&nbsp;&nbsp;Find out details about Pluribus (Facebook, Carnegie Mellon build first AI that beats pros in 6-player poker (Arxiv).&nbsp;
####################################################
Why TensorFlow is a bit of a pain to use:&hellip;Frustrated developer lists out TF&rsquo;s weaknesses &ndash; and they&rsquo;re not wrong&hellip;Google&rsquo;s TensorFlow software is kind of confusing and hard to use, and one Tumblr user has written up some thoughts on why. The tl;dr: TensorFlow has become a bit bloated in terms of its codebase, and Google continually keeps releasing new abstractions and features for the software that make it harder to understand and use.&nbsp;&nbsp;&nbsp;&ldquo;You know what it reminds me of, in some ways? With the profusion of backwards-incompatible wheel-reinventing features, and the hard-won platform-specific knowledge you just know will be out of date in two years?&nbsp; Microsoft Office,&rdquo; they write.&nbsp;
Why this matters: As AI industrializes, more and more people are going to use the software to develop AI, and the software that captures the greatest number of developers will likely become the Linux-esque basis for a new computational reality. Therefore, it&rsquo;s interesting to contrast the complaints people have about TensorFlow with the general enthusiasm about PyTorch, a Facebook-developed AI software framework that is easier to use and more flexible than TensorFlow.&nbsp;&nbsp;&nbsp;Read about problems with TensorFlow here (trees are harlequins, words are harlequins, GitHub).
####################################################
Enter the GPT-2 Dungeon:&hellip;You&rsquo;re in a forest. To your north is a sentient bar of soap. Where do you go?&hellip;AI advances are going to change gaming &ndash; both the mechanics of games, and also how narratives work in games. Already, we&rsquo;re seeing people use reinforcement learning approaches to create game agents capable of more capable, fluid, movement than their predecessors. Now, with the recent advances in natural language processing, we&rsquo;re seeing people use pre-trained language models in a variety of creative writing applications. One of the most evocative ones is AI Dungeon, a text-adventure game which uses a pre-trained 1.5bn GPT-2 language model to guide how the game unfolds.&nbsp;
What&rsquo;s interesting about this: AI Dungeon lets us take the role of a player in an 80s-style text adventure game &ndash; except, the game doesn&rsquo;t depend on a baroque series of hand-written narrative sections, joined together by fill-in-the-blanks madlib cards and controlled by a set of pre-defined keywords. Instead, it relies on a blob neural stuff that has been trained on a percentage of the internet, and this blob of neural stuff is used to animate the game world, interpreting player commands and generating new narrative sections. We&rsquo;re not in Kansas anymore, folks!&nbsp;
How it plays: The fun thing about AI Dungeon is its inherent flexibility, and most games devolve into an exercise of trying to break GPT-2 in the most amusing way (or at least, that&rsquo;s how I play it!). During an adventure I went on, I was a rogue named &lsquo;Vert&rsquo; and I repeatedly commanded my character to travel through time, but the game adapted to this pretty well, keeping track of changes in buildings as I went through time (some crumbled, some grew). At one point all the humans besides me disappeared, but that seems like the sort of risk you run when time traveling. It&rsquo;s compelling stuff and, while still a bit brittle, can be quite fun when it works.
Why this matters: As the AI research community develops larger and more sophisticated generative models, we can expect the outputs of these models to be plugged into a variety of creative endeavors, ranging from music to gaming to poetry to playwriting. GPT-2 has shown up in all of these so far, and my intuition is in 2020 onwards we&rsquo;ll see the emergence of a range of AI-infused paintbrushes for a variety of different mediums. I can&rsquo;t wait to see what an AI Dungeon might look like in 2020&hellip; or 2021!&nbsp;&nbsp;&nbsp;Play the AI Dungeon now ().
####################################################
Mustafa swaps DeepMind for Google:&hellip;DeepMind co-founder moves on to focus on applied AI projects&hellip;Mustafa Suleyman, the co-founder of DeepMind, has left the company. However, he&rsquo;s not going far &ndash; Mustafa will take on a new role at Google, part of the Alphabet Inc. mothership to which DeepMind is tethered. At Google, Mustafa will work on applied initiatives.
Why this matters: At DeepMind, Mustafa was frequently seen advocating for the development of socially beneficial applications of the firm&rsquo;s technology, most notably in healthcare. He was also a fixture on the international AI policy circuit, turning up at various illustrious meeting ro…"

---

### Import AI 176: Are language models full of hot air? Test them on BLiMP; Facebook notches up Hanabi win; plus, TensorFlow woes.

First, poker. Now: Hanabi! Facebook notches up another AI game-playing win:&hellip;The secret? Better search&hellip;Facebook AI researchers have developed an AI system that gets superhuman performance in Hanabi, a collaborative card game that requires successful players to &ldquo;understand the beliefs and intentions of other players, because they can&rsquo;t see the same cards their teammates see and can only share very limited hints with eachother&rdquo;. The Facebook-developed system is based on Pluribus, the CMU/Facebook-developed machine that defeated players in six-player no-limit Hold&rsquo;em earlier this year.&nbsp;
Why Hanabi? In February, researchers with Google and DeepMind published a paper arguing that the card game &lsquo;Hanabi&rsquo; should be treated as a milestone in AI research, following the success of contemporary systems in domains like Go, Atari, and Poker. Hanabi is a useful milestone because along with requiring reasoning with partial information, the game also &ldquo;elevates reasoning about the beliefs and intentions of other agents to the foreground,&rdquo; wrote the researchers at the time.&nbsp;
How they did it: The Facebook-developed system relies on what the company calls multi-agent search to work. Multi-agent search works roughly like this: agent a) looks at the state of the gameworld and tries to work out the optimal move to make using Monte Carlo rollouts to do the calculation; agent b) looks at the move agent a) made and uses that to infer what cards agent a) had, then uses this knowledge to inform the strategy agent b) picks; agent a) then looks at moves made by agent b) and studies b)&rsquo;s prior moves to estimate what cards b) has and what cards b) thinks agent a) has, then uses this to generate a strategy.&nbsp;&nbsp;&nbsp;This is a bloody expensive procedure, as it involves a ballooning quantity of calculations as the complexity fo the game increases; the Facebook researchers default to a computationally-cheaper but less effective single-agent search procedure for most of the time, only using multi-agent search periodically.&nbsp;
Why this matters: We&rsquo;re not interested in this system because it can play Hanabi &ndash; we&rsquo;re interested in understanding general approaches to improving the performance of AI systems that interact with other systems in messy, strategic contexts. The takeaway from Facebook&rsquo;s research is that if you have access to enough information to be able to simulate the game state, then you can layer on search strategies on top of big blobs of neural inference layers, and use this to create somewhat generic, strategic agents. &ldquo;Adding search to RL can dramatically improve performance beyond what can be achieved through RL alone,&rdquo; they write. &ldquo;We have now shown that this approach can work in cooperative environments as well.&rdquo;&nbsp;&nbsp;&nbsp;Read more: Building AI that can master complex cooperative games with hidden information (Facebook AI research blog).&nbsp;&nbsp;&nbsp;Read more: The Hanabi Challenge: A New Frontier for AI Research (Arxiv).&nbsp;&nbsp;&nbsp;Find out details about Pluribus (Facebook, Carnegie Mellon build first AI that beats pros in 6-player poker (Arxiv).&nbsp;
####################################################
Why TensorFlow is a bit of a pain to use:&hellip;Frustrated developer lists out TF&rsquo;s weaknesses &ndash; and they&rsquo;re not wrong&hellip;Google&rsquo;s TensorFlow software is kind of confusing and hard to use, and one Tumblr user has written up some thoughts on why. The tl;dr: TensorFlow has become a bit bloated in terms of its codebase, and Google continually keeps releasing new abstractions and features for the software that make it harder to understand and use.&nbsp;&nbsp;&nbsp;&ldquo;You know what it reminds me of, in some ways? With the profusion of backwards-incompatible wheel-reinventing features, and the hard-won platform-specific knowledge you just know will be out of date in two years?&nbsp; Microsoft Office,&rdquo; they write.&nbsp;
Why this matters: As AI industrializes, more and more people are going to use the software to develop AI, and the software that captures the greatest number of developers will likely become the Linux-esque basis for a new computational reality. Therefore, it&rsquo;s interesting to contrast the complaints people have about TensorFlow with the general enthusiasm about PyTorch, a Facebook-developed AI software framework that is easier to use and more flexible than TensorFlow.&nbsp;&nbsp;&nbsp;Read about problems with TensorFlow here (trees are harlequins, words are harlequins, GitHub).
####################################################
Enter the GPT-2 Dungeon:&hellip;You&rsquo;re in a forest. To your north is a sentient bar of soap. Where do you go?&hellip;AI advances are going to change gaming &ndash; both the mechanics of games, and also how narratives work in games. Already, we&rsquo;re seeing people use reinforcement learning approaches to create game agents capable of more capable, fluid, movement than their predecessors. Now, with the recent advances in natural language processing, we&rsquo;re seeing people use pre-trained language models in a variety of creative writing applications. One of the most evocative ones is AI Dungeon, a text-adventure game which uses a pre-trained 1.5bn GPT-2 language model to guide how the game unfolds.&nbsp;
What&rsquo;s interesting about this: AI Dungeon lets us take the role of a player in an 80s-style text adventure game &ndash; except, the game doesn&rsquo;t depend on a baroque series of hand-written narrative sections, joined together by fill-in-the-blanks madlib cards and controlled by a set of pre-defined keywords. Instead, it relies on a blob neural stuff that has been trained on a percentage of the internet, and this blob of neural stuff is used to animate the game world, interpreting player commands and generating new narrative sections. We&rsquo;re not in Kansas anymore, folks!&nbsp;
How it plays: The fun thing about AI Dungeon is its inherent flexibility, and most games devolve into an exercise of trying to break GPT-2 in the most amusing way (or at least, that&rsquo;s how I play it!). During an adventure I went on, I was a rogue named &lsquo;Vert&rsquo; and I repeatedly commanded my character to travel through time, but the game adapted to this pretty well, keeping track of changes in buildings as I went through time (some crumbled, some grew). At one point all the humans besides me disappeared, but that seems like the sort of risk you run when time traveling. It&rsquo;s compelling stuff and, while still a bit brittle, can be quite fun when it works.
Why this matters: As the AI research community develops larger and more sophisticated generative models, we can expect the outputs of these models to be plugged into a variety of creative endeavors, ranging from music to gaming to poetry to playwriting. GPT-2 has shown up in all of these so far, and my intuition is in 2020 onwards we&rsquo;ll see the emergence of a range of AI-infused paintbrushes for a variety of different mediums. I can&rsquo;t wait to see what an AI Dungeon might look like in 2020&hellip; or 2021!&nbsp;&nbsp;&nbsp;Play the AI Dungeon now ().
####################################################
Mustafa swaps DeepMind for Google:&hellip;DeepMind co-founder moves on to focus on applied AI projects&hellip;Mustafa Suleyman, the co-founder of DeepMind, has left the company. However, he&rsquo;s not going far &ndash; Mustafa will take on a new role at Google, part of the Alphabet Inc. mothership to which DeepMind is tethered. At Google, Mustafa will work on applied initiatives.
Why this matters: At DeepMind, Mustafa was frequently seen advocating for the development of socially beneficial applications of the firm&rsquo;s technology, most notably in healthcare. He was also a fixture on the international AI policy circuit, turning up at various illustrious meeting ro…