---

layout: post
category: product
title: "Import AI 193: Facebook simulates itself; compete to make more efficient NLP; face in-painting gets better"
date: 2020-04-14 17:26:40
link: https://vrhk.co/2XBGlMc
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Facebook simulates its users:&hellip;What&rsquo;s the difference between the world&rsquo;s largest social network and Westworld? Less than you might imagine&hellip;Facebook wants to better understand itself, so it has filled its site with (invisible) synthetically-created user accounts to help it understand itself. The users range in sophistication from basic entities that simply explore the site, to more complex machine learning-based ones that sometimes work together to simulate &lsquo;social&rsquo; interactions on the website. Facebook calls this a Web-Enabled Simulation (WES) approach and says &ldquo;the primary way in which WES builds on existing testing approaches lies in the way it models behaviour. Traditional testing focuses on system behaviour rather than user behaviour, whereas WES focuses on the interactions between users mediated by the system.&rdquo; Making fake users with reinforcement learning: Facebook uses reinforcement learning techniques to train bots to carry out sophisticated behaviors, like using RL to simulate scammer bots that target rule-based &lsquo;candidate targets&rsquo;.  &nbsp; What else does Facebook simulate? Facebook is also using this approach to simulate bad actors, search for bad content, identify mechanisms that impede bad actors, find weaknesses in its privacy system, identify bots that are trying to slurp up user data, and more. Deliciously evocative quote: This quote from the paper reads like the opening of a sci-fi short story: &ldquo;Bots must be suitably isolated from real users to ensure that the simulation, although executed on real platform code, does not lead to unexpected interactions between bots and real users&rdquo;. Why this matters: WES turns Facebook into two distinct places &ndash; the &lsquo;real&rsquo; world populated by human users, and the shadowy WES world whose entities are fake but designed to become increasingly indistinguishable from the real. When discussing some of the advantages of a WES approach, the researchers write &ldquo;we simply adjust the mechanism through which bots interact with the underlying platform in order to model the proposed restrictions. The mechanism can thus model a possible future version of the platform,&rdquo; they write.  &nbsp; WES is also a baroque artefact in itself, full of recursion and strangeness. The system &ldquo;is not only a simulation of hundreds of millions of lines of code; it is a software system that runs on top of those very same lines of code,&rdquo; Facebook writes.  &nbsp; One of the implications of this is that as Facebook&rsquo;s WES system gets better, we can imagine Facebook testing out more and more features in WES-land before porting them into the real Facebook &ndash; and as the AI systems get more sophisticated it&rsquo;ll be interesting to see how far Facebook can take this.  &nbsp; Read more: WES: Agent-based User Interaction Simulation on Real Infrastructure (Facebook Research).####################################################Make inferences and don&rsquo;t boil the ocean with the SustaiNLP competition:&hellip;You&rsquo;ve heard of powerful models. What about efficient ones?&hellip;In recent years, AI labs have been training increasingly large machine learning models in areas like language (e.g., GPT-2, Megatron), reinforcement learning (Dota 2, AlphaStar), and more. These models typically display significant advances in capabilities, but usually at the cost of resource consumption &ndash; they&rsquo;re literally very big models, requiring significant amounts of infrastructure to train on, and sometimes quite a lot of infrastructure to run inference on. A new competition at EMNLP2020 aims to &ldquo;promote the development of effective, energy-efficient models for difficult natural language understanding tasks&rdquo;, by testing out the efficiency of model inferences.&nbsp;
The challenge: The challenge, held within the SustaiNLP workshop, will see AI researchers compete with eachother to see who can develop the most energy-efficient model that does well on the well-established SuperGLUE benchmark. Participants will use the experiment impact tracker (get the code from its GitHub here) to measure the energy consumption of their models use during inference.Why this matters: Training these systems is expensive, but it&rsquo;s likely the significant real-world energy consumption of models will happen mostly at inference, since over time we can expect more and more models to be deployed into the world and more and more systems to depend on their inferences. Competitions like this will give us a sense of how energy-intensive that world is, and will produce metrics that can help us figure out paths to more energy-efficient futures.  &nbsp; Read more: SustaiNLP official website.####################################################Microsoft tests the limits of multilingual models with XGLUE:&hellip;Sure, your system can solve tasks in other languages. But can it generate phrases in them as well?&hellip;The recent success of large-scale neural machine translation models has caused researchers to develop harder and more diverse tests to probe the capabilities of these systems. A couple of weeks ago, researchers from CMU, DeepMind, and Google showed off XTREME (Import AI 191), a system to test out machine translation systems on nine tasks across 40 languages. Now, Microsoft has released XGLUE, a similarly motivated large-scale testing suite, but with a twist: XGLUE will also test how well multilingual language systems can generate text in different languages, along with testing on various understanding tasks. Multi-lingual generations: XGLUE&rsquo;s two aforementioned generative tasks include:&ndash; Question Generation (QG): Generate a natural language question for a given passage of text. &ndash; News Title Generation (NTG): Generate a headline for a given news story.Why this matters; Between XTREME and XGLUE, we&rsquo;ve got two new suites for testing out the capabilities of large-scale multilingual translation systems. I hope we&rsquo;ll use these to identify the weaknesses of current models, and if enough researchers test out against both task suites we&rsquo;ll inevitably see a new multi-lingual evaluation system get created by splicing the hard parts of both together. Soon, idioms like &lsquo;it&rsquo;s all Greek to me&rsquo; won&rsquo;t be so easy to say for neural agents.  &nbsp; Read more: XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation (arXiv). ####################################################Face in-painting keeps getting better:&hellip;Generative models give us smart paintbrushes that can fill-in reality&hellip;Researchers with South China University of Technology, Stevens Institute of Technology, and the&nbsp; UBTECH Sydney AI Centre have built a system that can perform &ldquo;high fidelity face completion&rdquo;, which means you can give it a photograph of a face where you&rsquo;ve partially occluded some parts, and it&rsquo;ll generate the bits of the face that are hidden. How they did it: The system uses a dual spatial attention (DSA) model that combines foreground self-attention and foreground-background cross-attention modules &ndash; this basically means the system learns a couple of attention patterns over images during training and reconstruction, which makes it better at generating the missing parts of images. In tests, their system does well quantitatively when compared to other methods, and gets close to ground truth (though note: it&rsquo;d be a terrible idea to use systems like this to &lsquo;fill in&rsquo; images and assume the resulting faces correspond to ground truth &ndash; that&rsquo;s how you end up with a police force arresting people because they look like the generations of an AI model. Why this matters: I think technologies like this point to a future where we have &lsquo;anything restoration&rsquo; &ndash; got an old movie with weird compression artefac…"

---

### Import AI 193: Facebook simulates itself; compete to make more efficient NLP; face in-painting gets better

Facebook simulates its users:&hellip;What&rsquo;s the difference between the world&rsquo;s largest social network and Westworld? Less than you might imagine&hellip;Facebook wants to better understand itself, so it has filled its site with (invisible) synthetically-created user accounts to help it understand itself. The users range in sophistication from basic entities that simply explore the site, to more complex machine learning-based ones that sometimes work together to simulate &lsquo;social&rsquo; interactions on the website. Facebook calls this a Web-Enabled Simulation (WES) approach and says &ldquo;the primary way in which WES builds on existing testing approaches lies in the way it models behaviour. Traditional testing focuses on system behaviour rather than user behaviour, whereas WES focuses on the interactions between users mediated by the system.&rdquo; Making fake users with reinforcement learning: Facebook uses reinforcement learning techniques to train bots to carry out sophisticated behaviors, like using RL to simulate scammer bots that target rule-based &lsquo;candidate targets&rsquo;.  &nbsp; What else does Facebook simulate? Facebook is also using this approach to simulate bad actors, search for bad content, identify mechanisms that impede bad actors, find weaknesses in its privacy system, identify bots that are trying to slurp up user data, and more. Deliciously evocative quote: This quote from the paper reads like the opening of a sci-fi short story: &ldquo;Bots must be suitably isolated from real users to ensure that the simulation, although executed on real platform code, does not lead to unexpected interactions between bots and real users&rdquo;. Why this matters: WES turns Facebook into two distinct places &ndash; the &lsquo;real&rsquo; world populated by human users, and the shadowy WES world whose entities are fake but designed to become increasingly indistinguishable from the real. When discussing some of the advantages of a WES approach, the researchers write &ldquo;we simply adjust the mechanism through which bots interact with the underlying platform in order to model the proposed restrictions. The mechanism can thus model a possible future version of the platform,&rdquo; they write.  &nbsp; WES is also a baroque artefact in itself, full of recursion and strangeness. The system &ldquo;is not only a simulation of hundreds of millions of lines of code; it is a software system that runs on top of those very same lines of code,&rdquo; Facebook writes.  &nbsp; One of the implications of this is that as Facebook&rsquo;s WES system gets better, we can imagine Facebook testing out more and more features in WES-land before porting them into the real Facebook &ndash; and as the AI systems get more sophisticated it&rsquo;ll be interesting to see how far Facebook can take this.  &nbsp; Read more: WES: Agent-based User Interaction Simulation on Real Infrastructure (Facebook Research).####################################################Make inferences and don&rsquo;t boil the ocean with the SustaiNLP competition:&hellip;You&rsquo;ve heard of powerful models. What about efficient ones?&hellip;In recent years, AI labs have been training increasingly large machine learning models in areas like language (e.g., GPT-2, Megatron), reinforcement learning (Dota 2, AlphaStar), and more. These models typically display significant advances in capabilities, but usually at the cost of resource consumption &ndash; they&rsquo;re literally very big models, requiring significant amounts of infrastructure to train on, and sometimes quite a lot of infrastructure to run inference on. A new competition at EMNLP2020 aims to &ldquo;promote the development of effective, energy-efficient models for difficult natural language understanding tasks&rdquo;, by testing out the efficiency of model inferences.&nbsp;
The challenge: The challenge, held within the SustaiNLP workshop, will see AI researchers compete with eachother to see who can develop the most energy-efficient model that does well on the well-established SuperGLUE benchmark. Participants will use the experiment impact tracker (get the code from its GitHub here) to measure the energy consumption of their models use during inference.Why this matters: Training these systems is expensive, but it&rsquo;s likely the significant real-world energy consumption of models will happen mostly at inference, since over time we can expect more and more models to be deployed into the world and more and more systems to depend on their inferences. Competitions like this will give us a sense of how energy-intensive that world is, and will produce metrics that can help us figure out paths to more energy-efficient futures.  &nbsp; Read more: SustaiNLP official website.####################################################Microsoft tests the limits of multilingual models with XGLUE:&hellip;Sure, your system can solve tasks in other languages. But can it generate phrases in them as well?&hellip;The recent success of large-scale neural machine translation models has caused researchers to develop harder and more diverse tests to probe the capabilities of these systems. A couple of weeks ago, researchers from CMU, DeepMind, and Google showed off XTREME (Import AI 191), a system to test out machine translation systems on nine tasks across 40 languages. Now, Microsoft has released XGLUE, a similarly motivated large-scale testing suite, but with a twist: XGLUE will also test how well multilingual language systems can generate text in different languages, along with testing on various understanding tasks. Multi-lingual generations: XGLUE&rsquo;s two aforementioned generative tasks include:&ndash; Question Generation (QG): Generate a natural language question for a given passage of text. &ndash; News Title Generation (NTG): Generate a headline for a given news story.Why this matters; Between XTREME and XGLUE, we&rsquo;ve got two new suites for testing out the capabilities of large-scale multilingual translation systems. I hope we&rsquo;ll use these to identify the weaknesses of current models, and if enough researchers test out against both task suites we&rsquo;ll inevitably see a new multi-lingual evaluation system get created by splicing the hard parts of both together. Soon, idioms like &lsquo;it&rsquo;s all Greek to me&rsquo; won&rsquo;t be so easy to say for neural agents.  &nbsp; Read more: XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation (arXiv). ####################################################Face in-painting keeps getting better:&hellip;Generative models give us smart paintbrushes that can fill-in reality&hellip;Researchers with South China University of Technology, Stevens Institute of Technology, and the&nbsp; UBTECH Sydney AI Centre have built a system that can perform &ldquo;high fidelity face completion&rdquo;, which means you can give it a photograph of a face where you&rsquo;ve partially occluded some parts, and it&rsquo;ll generate the bits of the face that are hidden. How they did it: The system uses a dual spatial attention (DSA) model that combines foreground self-attention and foreground-background cross-attention modules &ndash; this basically means the system learns a couple of attention patterns over images during training and reconstruction, which makes it better at generating the missing parts of images. In tests, their system does well quantitatively when compared to other methods, and gets close to ground truth (though note: it&rsquo;d be a terrible idea to use systems like this to &lsquo;fill in&rsquo; images and assume the resulting faces correspond to ground truth &ndash; that&rsquo;s how you end up with a police force arresting people because they look like the generations of an AI model. Why this matters: I think technologies like this point to a future where we have &lsquo;anything restoration&rsquo; &ndash; got an old movie with weird compression artefac…