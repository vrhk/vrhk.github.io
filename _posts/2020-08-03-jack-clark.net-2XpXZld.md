---

layout: post
category: product
title: "Import AI 208: Google trains a vision system in &lt;1 minute; Gender+AI = bad; Ubisoft improves character animation with ML"
date: 2020-08-03 18:46:27
link: https://vrhk.co/2XpXZld
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Genderify: Uh oh &ndash; AI &amp; Bias &amp; Gender&hellip;AI + Gender Identification = No, this is a bad idea&hellip;Last week, an AI startup called Genderify launched on Product Hunt and within days shut down its website, deleted its twitter account (though it still has a microsite on Product Hunt). What happened? The short answer is the service tried to predict gender from names and titles. After it launched, many users demonstrated a series of embarrassing failures of the system, which neatly illustrated why a complex topic like gender is not one that should be approached with a machine learning blunderbuss. Why this matters: Using automated tools to infer the gender of someone is a bad idea &ndash; that&rsquo;s partially why the global standard is to ask the user/civilian/employee to self-identify their gender out of a menu of options when gender information is preferred. Think of how complex human names are and how often you&rsquo;ve heard a person&rsquo;s name and mis-gendered them in your head. Now add to that the fact that people with certain gendered names may consciously use a different pronoun to the one suggested by the name. Does an AI system have any good way, today, to guess this stuff with decent accuracy? No it doesn&rsquo;t! So if you build tools to do gender classification, you&rsquo;re basically committing yourself to getting a non-trivial % of your classifications wrong.  &nbsp; (There are some use cases where this may be somewhat okay, like doing an automated scan of all the names of all the faculty in a country and using that to provide some very basic data on the potential gender difference. I expect there to be relatively few use cases like this, though, and tools like Genderify are unlikely to be that helpful.) &nbsp; Read more: Service that uses AI to identify gender based on names looks incredibly biased (The Verge).
###################################################Things are getting faster &ndash; Google sets new MLPerf AI training record:&hellip;..TPU pods go brrrrr&hellip;.Google has set performance records in six out of eight MLPerf benchmarks, defining the new (public) frontier of large-scale compute performance. MLPerf is a benchmark that masures how long it takes to train popular AI components like residual nets, a mask r-cnn, transformer, a BERT model, and more. Multi-year progress: These results show the time it takes Google to train a ResNet50 network to convergence against ImageNet, giving us performance for a widely used, fairly standard AI task:&ndash; 0.47 seconds: July 2020, MLPerf 0.7.&ndash; 1.28 minutes: June 2019, MLPerf 0.6.&ndash; 7.1 minutes: May 2018, MLPerf 0.5.&ndash; Hours &ndash; it used to take hours to train this stuff, back in 2017, even at the frontier. Things have sped up a lot.Why this matters: We&rsquo;re getting way better at training large-scale networks faster. This makes it easier for developers to iterate while exploring different architectures. It also makes it easier to rapidly retrain networks on new data. Competitions like MLPerf illustrate the breakneck pace of AI progress &ndash; a consequence of years of multibillion-dollar capital expenditures by large technology companies, coupled with the nurturing of vast research labs and investment in frontier processors. All of this translates to a general speedup of the cadence of AI development, which means we should expect to be more surprised by progress in the future.  &nbsp; Read more: Google breaks AI performance records in MLPerf with world&rsquo;s fastest training supercomputer (Google blog).###################################################Trouble in NLParadise &ndash; our benchmarks aren&rsquo;t very good:&hellip;How are our existing benchmarks insufficient? Let me count the ways&hellip;Are benchmarks used to analyze language systems telling us the truth? That&rsquo;s the gist of a new paper from researchers with Element AI and Stanford university, which picks apart the &lsquo;ecological validity&rsquo; of how we test language user interfaces (personal assistants, customer support bots, etc). &lsquo;Ecological validity&rsquo; is a concept from psychology that &ldquo;is a special case of external validity, specifying the degree to which findings generalize to naturally occurring scenarios&rdquo;.Five problems with modern language benchmarks:The researchers identify five major problems with the ways that we develop and evaluate advanced language systems today. These are: &ndash; Synthetic language: Projects like BabyAI, which seek to generate simple instructions using an environment with a restricted or otherwise synthetic dictionary. This means that as you try to increase the complexity of the instructions you express, you can reduce the overall intelligibility of the system. &ldquo;Especially for larger domains it becomes increasingly difficult and tedious to ensure the readability of all questions or instructions&rdquo;.&ndash; Artificial tasks: Many research benchmarks &ldquo;do not correspond to or even resemble a practically relevant LUI setting&rdquo;. Self explanatory. &ndash; Not working with potential users of the system: For example, the visual question answering (VQA) competition teaches computers to caption images. However, &ldquo;although the visual question answering task was at least partly inspired by the need to help the visually impaired, questions were not collected from blind people. Instead, human subjects with 20/20 vision were primed to ask questions that would stump a smart robot&rdquo;. &nbsp; Another example of this is the SQuAD dataset, which was &ldquo;collected by having human annotators generate questions about Wikipedia articles&hellip; these crowdworkers had no information need, which makes it unclear if the resulting questions match the ones from users looking for this information&rdquo;. &ndash; Scripts and priming: Some tests rely on scripts that constrain the type of human-computer interaction, e.g by being customized for a specific context like making reservations. Using scripts like this can trap the systems into preconceived notions of operation that might not work well, and subjects that generate the underlying data might be primed by what the computer says to respond in a similar style (&ldquo;For example, instead of saying &lsquo;I need a place to dine at in the south that serves chinese&rsquo;, most people would probably say &ldquo;Chinese restaurant&rdquo; or &ldquo;Chinese food&rdquo; &ndash; Single-turn interfaces: Most meaningful dialog interactions involve several exchanges in a conversation, rather than just one. Building benchmarks that solely consist of single questions and responses, or other single turns of dialog, might be artificially limiting, and could create systems that don&rsquo;t generalize well. So, what should people do? The language system community should build benchmarks that have what people could call &lsquo;ecological validity&rsquo;, which means they&rsquo;re built with the end users in mind, in complex environments that don&rsquo;t generate contrived data. The example the authors give &ldquo;are the development of LUI benchmarks for popular video game environments like Minecraft or for platforms that bundle user services on the Internet of Things &hellip;[or] work on LUIs tht enable citizens to easily acces statistical information published by governments.&rdquo;  &nbsp; Read more: Towards Ecologically Valid Research on Language User Interfaces (arXiv). ###################################################Boston Dynamics + vehicle manufacturing:&hellip;Rise of the yellow, surprisingly cute machines&hellip;Boston Dynamics&rsquo; &lsquo;Spot&rsquo; quadruped is being used by the Ford Motor Company to surveil and laser-scan its manufacturing facilities, helping it continuously map the space. This is one of the first in-the-wild uses of the Spot robots I&rsquo;ve seen. Check out the video to see the robot and some of the discussion by its handler about what …"

---

### Import AI 208: Google trains a vision system in &lt;1 minute; Gender+AI = bad; Ubisoft improves character animation with ML

Genderify: Uh oh &ndash; AI &amp; Bias &amp; Gender&hellip;AI + Gender Identification = No, this is a bad idea&hellip;Last week, an AI startup called Genderify launched on Product Hunt and within days shut down its website, deleted its twitter account (though it still has a microsite on Product Hunt). What happened? The short answer is the service tried to predict gender from names and titles. After it launched, many users demonstrated a series of embarrassing failures of the system, which neatly illustrated why a complex topic like gender is not one that should be approached with a machine learning blunderbuss. Why this matters: Using automated tools to infer the gender of someone is a bad idea &ndash; that&rsquo;s partially why the global standard is to ask the user/civilian/employee to self-identify their gender out of a menu of options when gender information is preferred. Think of how complex human names are and how often you&rsquo;ve heard a person&rsquo;s name and mis-gendered them in your head. Now add to that the fact that people with certain gendered names may consciously use a different pronoun to the one suggested by the name. Does an AI system have any good way, today, to guess this stuff with decent accuracy? No it doesn&rsquo;t! So if you build tools to do gender classification, you&rsquo;re basically committing yourself to getting a non-trivial % of your classifications wrong.  &nbsp; (There are some use cases where this may be somewhat okay, like doing an automated scan of all the names of all the faculty in a country and using that to provide some very basic data on the potential gender difference. I expect there to be relatively few use cases like this, though, and tools like Genderify are unlikely to be that helpful.) &nbsp; Read more: Service that uses AI to identify gender based on names looks incredibly biased (The Verge).
###################################################Things are getting faster &ndash; Google sets new MLPerf AI training record:&hellip;..TPU pods go brrrrr&hellip;.Google has set performance records in six out of eight MLPerf benchmarks, defining the new (public) frontier of large-scale compute performance. MLPerf is a benchmark that masures how long it takes to train popular AI components like residual nets, a mask r-cnn, transformer, a BERT model, and more. Multi-year progress: These results show the time it takes Google to train a ResNet50 network to convergence against ImageNet, giving us performance for a widely used, fairly standard AI task:&ndash; 0.47 seconds: July 2020, MLPerf 0.7.&ndash; 1.28 minutes: June 2019, MLPerf 0.6.&ndash; 7.1 minutes: May 2018, MLPerf 0.5.&ndash; Hours &ndash; it used to take hours to train this stuff, back in 2017, even at the frontier. Things have sped up a lot.Why this matters: We&rsquo;re getting way better at training large-scale networks faster. This makes it easier for developers to iterate while exploring different architectures. It also makes it easier to rapidly retrain networks on new data. Competitions like MLPerf illustrate the breakneck pace of AI progress &ndash; a consequence of years of multibillion-dollar capital expenditures by large technology companies, coupled with the nurturing of vast research labs and investment in frontier processors. All of this translates to a general speedup of the cadence of AI development, which means we should expect to be more surprised by progress in the future.  &nbsp; Read more: Google breaks AI performance records in MLPerf with world&rsquo;s fastest training supercomputer (Google blog).###################################################Trouble in NLParadise &ndash; our benchmarks aren&rsquo;t very good:&hellip;How are our existing benchmarks insufficient? Let me count the ways&hellip;Are benchmarks used to analyze language systems telling us the truth? That&rsquo;s the gist of a new paper from researchers with Element AI and Stanford university, which picks apart the &lsquo;ecological validity&rsquo; of how we test language user interfaces (personal assistants, customer support bots, etc). &lsquo;Ecological validity&rsquo; is a concept from psychology that &ldquo;is a special case of external validity, specifying the degree to which findings generalize to naturally occurring scenarios&rdquo;.Five problems with modern language benchmarks:The researchers identify five major problems with the ways that we develop and evaluate advanced language systems today. These are: &ndash; Synthetic language: Projects like BabyAI, which seek to generate simple instructions using an environment with a restricted or otherwise synthetic dictionary. This means that as you try to increase the complexity of the instructions you express, you can reduce the overall intelligibility of the system. &ldquo;Especially for larger domains it becomes increasingly difficult and tedious to ensure the readability of all questions or instructions&rdquo;.&ndash; Artificial tasks: Many research benchmarks &ldquo;do not correspond to or even resemble a practically relevant LUI setting&rdquo;. Self explanatory. &ndash; Not working with potential users of the system: For example, the visual question answering (VQA) competition teaches computers to caption images. However, &ldquo;although the visual question answering task was at least partly inspired by the need to help the visually impaired, questions were not collected from blind people. Instead, human subjects with 20/20 vision were primed to ask questions that would stump a smart robot&rdquo;. &nbsp; Another example of this is the SQuAD dataset, which was &ldquo;collected by having human annotators generate questions about Wikipedia articles&hellip; these crowdworkers had no information need, which makes it unclear if the resulting questions match the ones from users looking for this information&rdquo;. &ndash; Scripts and priming: Some tests rely on scripts that constrain the type of human-computer interaction, e.g by being customized for a specific context like making reservations. Using scripts like this can trap the systems into preconceived notions of operation that might not work well, and subjects that generate the underlying data might be primed by what the computer says to respond in a similar style (&ldquo;For example, instead of saying &lsquo;I need a place to dine at in the south that serves chinese&rsquo;, most people would probably say &ldquo;Chinese restaurant&rdquo; or &ldquo;Chinese food&rdquo; &ndash; Single-turn interfaces: Most meaningful dialog interactions involve several exchanges in a conversation, rather than just one. Building benchmarks that solely consist of single questions and responses, or other single turns of dialog, might be artificially limiting, and could create systems that don&rsquo;t generalize well. So, what should people do? The language system community should build benchmarks that have what people could call &lsquo;ecological validity&rsquo;, which means they&rsquo;re built with the end users in mind, in complex environments that don&rsquo;t generate contrived data. The example the authors give &ldquo;are the development of LUI benchmarks for popular video game environments like Minecraft or for platforms that bundle user services on the Internet of Things &hellip;[or] work on LUIs tht enable citizens to easily acces statistical information published by governments.&rdquo;  &nbsp; Read more: Towards Ecologically Valid Research on Language User Interfaces (arXiv). ###################################################Boston Dynamics + vehicle manufacturing:&hellip;Rise of the yellow, surprisingly cute machines&hellip;Boston Dynamics&rsquo; &lsquo;Spot&rsquo; quadruped is being used by the Ford Motor Company to surveil and laser-scan its manufacturing facilities, helping it continuously map the space. This is one of the first in-the-wild uses of the Spot robots I&rsquo;ve seen. Check out the video to see the robot and some of the discussion by its handler about what …