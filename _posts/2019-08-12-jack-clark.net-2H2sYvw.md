---

layout: post
category: product
title: "Import AI: 159: Characterizing attacks on AI systems; teaching AI systems to subvert ML security systems; and what happens when AI regenerates actors"
date: 2019-08-12 19:31:28
link: https://vrhk.co/2H2sYvw
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Can you outsmart a machine learning malware detector?&hellip;Enter the MLSEC competition to find out&hellip;Today, many antivirus companies use machine learning models to try and spot malware &ndash; a new competition wants to challenge people to design malware payloads that evade these machine learning classifiers. The Machine Learning Static Evasion Competition (MLSEC) was announced at the &lsquo;Defcon&rsquo; security conference this week.&nbsp;
White box attack: &ldquo;The competition will demonstrate a white box attack, wherein participants will have access to each model&rsquo;s parameters and source code,&rdquo; the organizers write. &ldquo;Points will be awarded to participants based on how many samples bypass each machine learning model. In particular, for each functional modified malware sample, one point is awarded for each ML model that it bypasses.&rdquo;
Registrants only: Participants can access functional malicious software binaries, so entrances will need to register before they can download the malware samples.&nbsp;
Why this matters: Security is a cat &amp; mouse game between attackers and defenders, and machine learning systems are already helping us create more adaptive, general forms of security defense and offense. Competitions like MLSEC will generate valuable evidence about the relative strengths and weaknesses of ML-based security systems, helping us forecast how these systems might influence society.&nbsp;&nbsp;&nbsp;Register, then check out the code (official competition GitHub, hosted by Endgame Security).&nbsp;&nbsp;&nbsp;Read more: MLSEC overview (official competition website).&nbsp;
####################################################
Need a new Gym for your AI agent? Try getting it to open a door:&hellip;DoorGym teaches robots how to open a near-infinite number of simulated doors&hellip;If any contemporary robots were to become sentient and seek to destroy humanity, then one of the smartest things people could do to protect themselves would be to climb up some stairs and go into a room and shut the door behind them. That&rsquo;s because today&rsquo;s robots have a really hard time doing simple physical things like climbing stairs or opening doors. New research from Panasonic Beta, the startup Totemic, and the University of California at Berkeley tries to change this with &lsquo;DoorGym&rsquo;, software to help researchers teach simulated robots to open doors. DoorGym is &ldquo;intended to be a first step to move reinforcement learning from toy environments towards useful atomic skills that can be composed and extended towards a broader goal&rdquo;.&nbsp;
Enter the Randomized Door-World Generator!: DoorGym uses the &lsquo;Mujoco&rsquo; robotics simulator to generate a selection of doors with different handles (ranging from easy doorknobs based around pulling, to more complex ones that involve grasping), and then uses a technique called domain randomization to generate tens of thousands of different door simulations, varying things like the appearance and physics characteristics of the robot, door, doorknob, door frame, and wall. This highlights how domain randomization lets researchers trade compute for data &ndash; instead of needing to gather data of lots of different doors in the world, DoorGym just uses computers to automatically generate different types of door. DoorGym also ships with a simulated Berkeley &lsquo;BLUE&rsquo; low-cost robot arm.&nbsp;
Door opening baselines: In tests, the researchers test two popular RL algorithms, PPO and SAC, on three tasks within DoorGym. The tests show that Proximal Policy Optimization (PPO) obtains far higher scores than SAC, though SAC has slightly better early exploration properties. This is a somewhat interesting result &ndash; PPO, an OpenAI-developed RL algorithm, came out a couple of years ago and has since become a defacto standard for RL research, partially because it&rsquo;s a relatively simply algorithm with relatively few parameters; this may add some legitimacy to the idea that simple algorithms that scale-up will will tend to be successful.&nbsp;
The future of DOORS: In the future, the researchers will expand the number of baselines they test on, &ldquo;as well as incorporating more complicated tasks such as a broader range of doorknobs, locked doors, door knob generalization, and multi-agent scenarios&rdquo;.&nbsp;
Why this matters: Systems like DoorGym are an indication of the rapid maturity of research at the intersection of AI and robotics. If systems like this become standard testbeds for RL algorithms, it could ultimately lead to the creation of more intelligent and capable robot arms, which could potentially have significant effects on economic impact of robot-based automation.&nbsp;&nbsp;&nbsp;Read more: DoorGym: A Scalable Door Opening Environment And Baseline Agent (Arxiv).
####################################################
Is that a car or a spy robot? Why not both?&hellip;Tesla S mod turns any car into a surveillance system&hellip;An enterprising software engineer has developed a DIY computer called the &lsquo;Surveillance Detection Scout&rsquo; that can turn any Tesla Model S or Model 3 into a roving surveillance vehicle. The mod taps into the Tesla&rsquo;s dash and rearview cameras, then uses open source image recognition software to analyze license plates and faces that the Tesla sees, so the software can warn the car owner if it is being followed. &ldquo;When the car is parked, it can tracky nearby faces to see which ones repeatedly appear,&rdquo; Wired magazine writes. &ldquo;The intent is to offer a warning that someone might be preparing to steal the car, tamper with it or break into the driver&rsquo;s nearby home&rdquo;.&nbsp;
Why this matters: The future is rich people putting DIY software and computers into their machines, giving them enhanced cognitive capabilities relative to other people. Just wait till we optimize thrust/weight for small drones, and wealthy people start getting surrounded by literal &lsquo;thought clouds&rsquo;.&nbsp;&nbsp;&nbsp;Read more: This Tesla Mod Turns a Model S into a Mobile &lsquo;Surveillance Station&rsquo; (Wired).
####################################################
Facebook approaches human-level performance on the tough &lsquo;SuperGLUE&rsquo; benchmark:&hellip;What happens when AI progress outpaces the complexity of our benchmarks?&hellip;Recently, language AI systems have started to get really good. This is mostly due to a vast number of organizations developing language modeling approaches based on unsupervised pre-training &ndash; basically, training large language models with simple objectives on vast amounts of data. Such systems &ndash; BERT, GPT-2, ULMFiT, etc &ndash; have revolutionized parts of NLP, obtaining new state-of-the-art scores on a variety of benchmarks, and generating credibly interesting synthetic text.&nbsp;
Now, researchers from Facebook have shown just how powerful these new systems are with RoBERTa, a replication of Google&rsquo;s BERT system that is trained for longer with more careful hyperparameter selection. RoBERTa obtains new state-of-the-art scores on a bunch of benchmarks, including GLUE, RACE, and SQuAD. Most significantly, the researchers announced on Friday that RoBERTa was now the top entry on the &lsquo;SuperGLUE&rsquo; language challenge. That&rsquo;s significant because SuperGLUE was published this year as a significantly harder version of GLUE &nbsp;&ndash; the multi-task language benchmark that preceded it. It&rsquo;s notable that RoBERTa shows a 15 absolute percentage point improvement over the initial top SuperGLUE entry, and RoBERTa&rsquo;s score of 84.6% is relatively close to human baselines of 89.8.&nbsp;
Why this matters: Multi-task benchmarks like SuperGLUE are one of the best ways we have of judging where we are in terms of AI development, so it&rsquo;s significant if our ability to beat such benchmarks outpaces our ability to create them. As one of Supe…"

---

### Import AI: 159: Characterizing attacks on AI systems; teaching AI systems to subvert ML security systems; and what happens when AI regenerates actors

Can you outsmart a machine learning malware detector?&hellip;Enter the MLSEC competition to find out&hellip;Today, many antivirus companies use machine learning models to try and spot malware &ndash; a new competition wants to challenge people to design malware payloads that evade these machine learning classifiers. The Machine Learning Static Evasion Competition (MLSEC) was announced at the &lsquo;Defcon&rsquo; security conference this week.&nbsp;
White box attack: &ldquo;The competition will demonstrate a white box attack, wherein participants will have access to each model&rsquo;s parameters and source code,&rdquo; the organizers write. &ldquo;Points will be awarded to participants based on how many samples bypass each machine learning model. In particular, for each functional modified malware sample, one point is awarded for each ML model that it bypasses.&rdquo;
Registrants only: Participants can access functional malicious software binaries, so entrances will need to register before they can download the malware samples.&nbsp;
Why this matters: Security is a cat &amp; mouse game between attackers and defenders, and machine learning systems are already helping us create more adaptive, general forms of security defense and offense. Competitions like MLSEC will generate valuable evidence about the relative strengths and weaknesses of ML-based security systems, helping us forecast how these systems might influence society.&nbsp;&nbsp;&nbsp;Register, then check out the code (official competition GitHub, hosted by Endgame Security).&nbsp;&nbsp;&nbsp;Read more: MLSEC overview (official competition website).&nbsp;
####################################################
Need a new Gym for your AI agent? Try getting it to open a door:&hellip;DoorGym teaches robots how to open a near-infinite number of simulated doors&hellip;If any contemporary robots were to become sentient and seek to destroy humanity, then one of the smartest things people could do to protect themselves would be to climb up some stairs and go into a room and shut the door behind them. That&rsquo;s because today&rsquo;s robots have a really hard time doing simple physical things like climbing stairs or opening doors. New research from Panasonic Beta, the startup Totemic, and the University of California at Berkeley tries to change this with &lsquo;DoorGym&rsquo;, software to help researchers teach simulated robots to open doors. DoorGym is &ldquo;intended to be a first step to move reinforcement learning from toy environments towards useful atomic skills that can be composed and extended towards a broader goal&rdquo;.&nbsp;
Enter the Randomized Door-World Generator!: DoorGym uses the &lsquo;Mujoco&rsquo; robotics simulator to generate a selection of doors with different handles (ranging from easy doorknobs based around pulling, to more complex ones that involve grasping), and then uses a technique called domain randomization to generate tens of thousands of different door simulations, varying things like the appearance and physics characteristics of the robot, door, doorknob, door frame, and wall. This highlights how domain randomization lets researchers trade compute for data &ndash; instead of needing to gather data of lots of different doors in the world, DoorGym just uses computers to automatically generate different types of door. DoorGym also ships with a simulated Berkeley &lsquo;BLUE&rsquo; low-cost robot arm.&nbsp;
Door opening baselines: In tests, the researchers test two popular RL algorithms, PPO and SAC, on three tasks within DoorGym. The tests show that Proximal Policy Optimization (PPO) obtains far higher scores than SAC, though SAC has slightly better early exploration properties. This is a somewhat interesting result &ndash; PPO, an OpenAI-developed RL algorithm, came out a couple of years ago and has since become a defacto standard for RL research, partially because it&rsquo;s a relatively simply algorithm with relatively few parameters; this may add some legitimacy to the idea that simple algorithms that scale-up will will tend to be successful.&nbsp;
The future of DOORS: In the future, the researchers will expand the number of baselines they test on, &ldquo;as well as incorporating more complicated tasks such as a broader range of doorknobs, locked doors, door knob generalization, and multi-agent scenarios&rdquo;.&nbsp;
Why this matters: Systems like DoorGym are an indication of the rapid maturity of research at the intersection of AI and robotics. If systems like this become standard testbeds for RL algorithms, it could ultimately lead to the creation of more intelligent and capable robot arms, which could potentially have significant effects on economic impact of robot-based automation.&nbsp;&nbsp;&nbsp;Read more: DoorGym: A Scalable Door Opening Environment And Baseline Agent (Arxiv).
####################################################
Is that a car or a spy robot? Why not both?&hellip;Tesla S mod turns any car into a surveillance system&hellip;An enterprising software engineer has developed a DIY computer called the &lsquo;Surveillance Detection Scout&rsquo; that can turn any Tesla Model S or Model 3 into a roving surveillance vehicle. The mod taps into the Tesla&rsquo;s dash and rearview cameras, then uses open source image recognition software to analyze license plates and faces that the Tesla sees, so the software can warn the car owner if it is being followed. &ldquo;When the car is parked, it can tracky nearby faces to see which ones repeatedly appear,&rdquo; Wired magazine writes. &ldquo;The intent is to offer a warning that someone might be preparing to steal the car, tamper with it or break into the driver&rsquo;s nearby home&rdquo;.&nbsp;
Why this matters: The future is rich people putting DIY software and computers into their machines, giving them enhanced cognitive capabilities relative to other people. Just wait till we optimize thrust/weight for small drones, and wealthy people start getting surrounded by literal &lsquo;thought clouds&rsquo;.&nbsp;&nbsp;&nbsp;Read more: This Tesla Mod Turns a Model S into a Mobile &lsquo;Surveillance Station&rsquo; (Wired).
####################################################
Facebook approaches human-level performance on the tough &lsquo;SuperGLUE&rsquo; benchmark:&hellip;What happens when AI progress outpaces the complexity of our benchmarks?&hellip;Recently, language AI systems have started to get really good. This is mostly due to a vast number of organizations developing language modeling approaches based on unsupervised pre-training &ndash; basically, training large language models with simple objectives on vast amounts of data. Such systems &ndash; BERT, GPT-2, ULMFiT, etc &ndash; have revolutionized parts of NLP, obtaining new state-of-the-art scores on a variety of benchmarks, and generating credibly interesting synthetic text.&nbsp;
Now, researchers from Facebook have shown just how powerful these new systems are with RoBERTa, a replication of Google&rsquo;s BERT system that is trained for longer with more careful hyperparameter selection. RoBERTa obtains new state-of-the-art scores on a bunch of benchmarks, including GLUE, RACE, and SQuAD. Most significantly, the researchers announced on Friday that RoBERTa was now the top entry on the &lsquo;SuperGLUE&rsquo; language challenge. That&rsquo;s significant because SuperGLUE was published this year as a significantly harder version of GLUE &nbsp;&ndash; the multi-task language benchmark that preceded it. It&rsquo;s notable that RoBERTa shows a 15 absolute percentage point improvement over the initial top SuperGLUE entry, and RoBERTa&rsquo;s score of 84.6% is relatively close to human baselines of 89.8.&nbsp;
Why this matters: Multi-task benchmarks like SuperGLUE are one of the best ways we have of judging where we are in terms of AI development, so it&rsquo;s significant if our ability to beat such benchmarks outpaces our ability to create them. As one of Supe…