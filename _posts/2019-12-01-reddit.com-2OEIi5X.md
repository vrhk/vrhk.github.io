---

layout: post
category: threads
title: "[D] multi-head attention regularization"
date: 2019-12-01 03:17:25
link: https://vrhk.co/2OEIi5X
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "There are papers related to making multi-head attention in transformer look at different parts of inputs to reduce redundancy. do you try any? if..."

---

### [D] multi-head attention regularization

There are papers related to making multi-head attention in transformer look at different parts of inputs to reduce redundancy. do you try any? if...