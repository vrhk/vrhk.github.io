---

layout: post
category: product
title: "Import AI 205: Generative models &amp; clones; Hikvision cameras get smarter; and a full stack deep learning course."
date: 2020-07-13 16:46:32
link: https://vrhk.co/3fl4sox
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "160,000 teenagers get graded by a machine: &hellip;International Baccalaureate organization does a terrible thing&hellip;Due to COVID, the International Baccalaureate educational foundation is going to predict grades for students based on their prior bodies of work, rather than giving them a score as an outcome of taking a test. 166,000 students will be affected by the experiment. This has already gone badly wrong and will continue to do so.A terrible idea: This is a terrible idea. It&rsquo;s almost a made-to-order example of how you shouldn&rsquo;t deploy an AI system. To make that clear, let&rsquo;s outline what is going on:&ndash; Deploy an untested model against a large population&ndash; Have this model make predictions that will have a massive influence over the target populations&rsquo; lives&ndash; Have no plan for how to prevent your model learning to discriminate against students based on Gender, Race, etc. A bad idea, executed mindlessly: Imagine being a teenager and having some opaque algorithm make a fundamental decision about your future educational career. Now imagine that this system&rsquo;s prediction feels wrong to you. How do you live with that? I&rsquo;m not sure &ndash; but people are going to need to. One UK teacher told the Financial Times that the automated grading has already created &ldquo;really appalling injustices&ldquo;. &nbsp; Read more about this here: 160k+ high schools students will only graduate if a statistical model allows them to (Ishan Dikshit, personal blog). &nbsp; Read more about the effects of the algorithm: Students and teachers hit at International Baccalaureate grading (Financial Times).
####################################################
Import (A)Idea &ndash; The Ethics of Cloning in the Generative Model Age:Here&rsquo;s a fun mental experiment: humans have recently discovered a technology that lets them cheaply and easily clone people to carry out tasks. Putting aside the ethical issues of cloning, it feels like most people would be comfortable with people being cloned to do highly specific, physical tasks for which they&rsquo;re a demonstrable expert &ndash; think, 1,000 Michelin-grade chefs, or 1,000 supremely talented jewelery artisans, or 1,000 people working in construction. Now consider what happens if we cloned people to do tasks oriented around influencing mass culture &ndash; how might media be changed by the presence of 1,000 Andy Warhols or 1,000 Edward Bernays, or 1,000 Georgia O&rsquo;Keeffe? And now, for the extra confounding factor, imagine that only a tiny number of entities on the planet have the ability to clone people, cloning is an imperfect process, and the &lsquo;clones&rsquo; are about as interpretable as the people they were cloned from &ndash; aka, basically uninterpretable. Now simply swap out the word &lsquo;clones&rsquo; with &lsquo;generative models&rsquo;, and you might see what I mean. Today, large-scale generative models in text, image, and video are making it easy for (some) organizations to clone a swathe of culture, create an entity that reflects that culture outward, and then deploy that entity into a variety of different contexts. I think this is somewhat analogous to the ethics inherent to choosing to clone a person; once the person we clone starts doing more tasks that have a greater bearing on society, we might ask the cloners what values this person has and what process we used to decide that they were the right person to clone. I think the answers to both of these questions are low-resolution today and a challenge for AI researchers will be to figure out satisfying, detailed answers to these questions. The future of human culture will be the interplay between these AI artefacts that clone, warp, and reflect culture, and the humans who will likely create cultural products in response to the outputs of the &lsquo;clones&rsquo;.&nbsp;
####################################################Want AI systems that can better cope with adversarial examples? Tweak your activation function:&hellip;Is there an easier way to deal with confounding images?&hellip;Adversarial examples, aka optical illusions for computer systems (which could also come in the form of confusing audio or other datastreams), are a problem; the fact our AI-driven classification systems can get broken relatively easy makes it harder to trust the technology and also increases its potential harms. Now, a team of researchers with Google have published a paper about something that people have long desired &ndash; a simple way to tweak models during training so that they&rsquo;re more resilient to adversarial examples. What they did: The Google researchers have proposed to swap out the ReLU activation function for something they call &lsquo;smooth adversarial training&rsquo; (SAT) in the backward pass during neural net training. By doing this, they&rsquo;re able to train systems that are more resilient to adversarial examples, while exhibiting no fall in typical performance as well (a desirable, rare feature).The key statistic: &ldquo;Compared to standard adversarial training, SAT improves adversarial robustness for &ldquo;free&rdquo;, i.e., no drop in accuracy and no increase in computational cost. For example, without introducing additional computations, SAT significantly enhances ResNet-50&rsquo;s robustness from 33.0% to 42.3%, while also improving accuracy by 0.9% on ImageNet.&rdquo; In further tests, they show that SAT&rsquo;s benefits continue to hold even in larger-scale training regimes &ndash; this is encouraging, as it&rsquo;s often the case that new inventions break at large scales. They also show that they&rsquo;re able to train &lsquo;EfficientNet&rsquo; models using SAT instead of ReLU, and continue to see good performance. Why this matters: It&rsquo;s (relatively) general inventions like this that can sometimes have the largest effect. Keep your eyes out for future papers that propose using SAT instead of ReLU or another activation function.  &nbsp; Read more: Smooth Adversarial Training (arXiv). ####################################################Anduril gets $200 million for its military AI vision:&hellip;Want to understand the future of defenseAI? Look at what startups do&hellip;Startups are worth tracking because they&rsquo;re usually founded by people with idiosyncratic visions of the future &ndash; and if they become successful, that vision has a greater chance of coming true. That&rsquo;s why AI-defense-tech startup Anduril getting $200 million in new funding is notable, because if the startup sees further success and executes on its vision, then the U.S government and other countries will acquire increasingly power AI capabilities, using them to police their borders and make various strategic (and, eventually, kinetic) decisions using AI tools. What is Anduril? Anduril has raised $241 million in venture capital funding since it was founded in 2017, according to Crunchbase. Its&nbsp; key staff include include Palmer Luckey, the DIY VR headset wunderkind who sold Oculus to Facebook for $2bn; Trae Stephens, a partner at Peter Thiel&rsquo;s &lsquo;Founders Fund&rsquo; venture capital firm and early Palantir employee; and Chris Brose, the former staff director of the Senate Armed Services Committee.What does Anduril want and why does this matter? Anduril&rsquo;s vision of the future is embodied by the tech it builds today, which includes AI-infused sentry towers that can be deployed to autonomously scan and survey areas (like national borders), the &lsquo;Ghost sUAS&lsquo; autonomous helicopter; and its &lsquo;Anvil sUAS&lsquo; an anti-drone weapon. The more successful companies Anduril are, the more likely it is that our future will consist of &lsquo;invisible&rsquo; walsall made up of numerous smart machines, working together for purposes set by those who can pay.  &nbsp; Read more: Anduril Raises $200 Million To Fund Ambitious Plans To Build A Defense Tech Giant (Forbes). &nbsp; Read …"

---

### Import AI 205: Generative models &amp; clones; Hikvision cameras get smarter; and a full stack deep learning course.

160,000 teenagers get graded by a machine: &hellip;International Baccalaureate organization does a terrible thing&hellip;Due to COVID, the International Baccalaureate educational foundation is going to predict grades for students based on their prior bodies of work, rather than giving them a score as an outcome of taking a test. 166,000 students will be affected by the experiment. This has already gone badly wrong and will continue to do so.A terrible idea: This is a terrible idea. It&rsquo;s almost a made-to-order example of how you shouldn&rsquo;t deploy an AI system. To make that clear, let&rsquo;s outline what is going on:&ndash; Deploy an untested model against a large population&ndash; Have this model make predictions that will have a massive influence over the target populations&rsquo; lives&ndash; Have no plan for how to prevent your model learning to discriminate against students based on Gender, Race, etc. A bad idea, executed mindlessly: Imagine being a teenager and having some opaque algorithm make a fundamental decision about your future educational career. Now imagine that this system&rsquo;s prediction feels wrong to you. How do you live with that? I&rsquo;m not sure &ndash; but people are going to need to. One UK teacher told the Financial Times that the automated grading has already created &ldquo;really appalling injustices&ldquo;. &nbsp; Read more about this here: 160k+ high schools students will only graduate if a statistical model allows them to (Ishan Dikshit, personal blog). &nbsp; Read more about the effects of the algorithm: Students and teachers hit at International Baccalaureate grading (Financial Times).
####################################################
Import (A)Idea &ndash; The Ethics of Cloning in the Generative Model Age:Here&rsquo;s a fun mental experiment: humans have recently discovered a technology that lets them cheaply and easily clone people to carry out tasks. Putting aside the ethical issues of cloning, it feels like most people would be comfortable with people being cloned to do highly specific, physical tasks for which they&rsquo;re a demonstrable expert &ndash; think, 1,000 Michelin-grade chefs, or 1,000 supremely talented jewelery artisans, or 1,000 people working in construction. Now consider what happens if we cloned people to do tasks oriented around influencing mass culture &ndash; how might media be changed by the presence of 1,000 Andy Warhols or 1,000 Edward Bernays, or 1,000 Georgia O&rsquo;Keeffe? And now, for the extra confounding factor, imagine that only a tiny number of entities on the planet have the ability to clone people, cloning is an imperfect process, and the &lsquo;clones&rsquo; are about as interpretable as the people they were cloned from &ndash; aka, basically uninterpretable. Now simply swap out the word &lsquo;clones&rsquo; with &lsquo;generative models&rsquo;, and you might see what I mean. Today, large-scale generative models in text, image, and video are making it easy for (some) organizations to clone a swathe of culture, create an entity that reflects that culture outward, and then deploy that entity into a variety of different contexts. I think this is somewhat analogous to the ethics inherent to choosing to clone a person; once the person we clone starts doing more tasks that have a greater bearing on society, we might ask the cloners what values this person has and what process we used to decide that they were the right person to clone. I think the answers to both of these questions are low-resolution today and a challenge for AI researchers will be to figure out satisfying, detailed answers to these questions. The future of human culture will be the interplay between these AI artefacts that clone, warp, and reflect culture, and the humans who will likely create cultural products in response to the outputs of the &lsquo;clones&rsquo;.&nbsp;
####################################################Want AI systems that can better cope with adversarial examples? Tweak your activation function:&hellip;Is there an easier way to deal with confounding images?&hellip;Adversarial examples, aka optical illusions for computer systems (which could also come in the form of confusing audio or other datastreams), are a problem; the fact our AI-driven classification systems can get broken relatively easy makes it harder to trust the technology and also increases its potential harms. Now, a team of researchers with Google have published a paper about something that people have long desired &ndash; a simple way to tweak models during training so that they&rsquo;re more resilient to adversarial examples. What they did: The Google researchers have proposed to swap out the ReLU activation function for something they call &lsquo;smooth adversarial training&rsquo; (SAT) in the backward pass during neural net training. By doing this, they&rsquo;re able to train systems that are more resilient to adversarial examples, while exhibiting no fall in typical performance as well (a desirable, rare feature).The key statistic: &ldquo;Compared to standard adversarial training, SAT improves adversarial robustness for &ldquo;free&rdquo;, i.e., no drop in accuracy and no increase in computational cost. For example, without introducing additional computations, SAT significantly enhances ResNet-50&rsquo;s robustness from 33.0% to 42.3%, while also improving accuracy by 0.9% on ImageNet.&rdquo; In further tests, they show that SAT&rsquo;s benefits continue to hold even in larger-scale training regimes &ndash; this is encouraging, as it&rsquo;s often the case that new inventions break at large scales. They also show that they&rsquo;re able to train &lsquo;EfficientNet&rsquo; models using SAT instead of ReLU, and continue to see good performance. Why this matters: It&rsquo;s (relatively) general inventions like this that can sometimes have the largest effect. Keep your eyes out for future papers that propose using SAT instead of ReLU or another activation function.  &nbsp; Read more: Smooth Adversarial Training (arXiv). ####################################################Anduril gets $200 million for its military AI vision:&hellip;Want to understand the future of defenseAI? Look at what startups do&hellip;Startups are worth tracking because they&rsquo;re usually founded by people with idiosyncratic visions of the future &ndash; and if they become successful, that vision has a greater chance of coming true. That&rsquo;s why AI-defense-tech startup Anduril getting $200 million in new funding is notable, because if the startup sees further success and executes on its vision, then the U.S government and other countries will acquire increasingly power AI capabilities, using them to police their borders and make various strategic (and, eventually, kinetic) decisions using AI tools. What is Anduril? Anduril has raised $241 million in venture capital funding since it was founded in 2017, according to Crunchbase. Its&nbsp; key staff include include Palmer Luckey, the DIY VR headset wunderkind who sold Oculus to Facebook for $2bn; Trae Stephens, a partner at Peter Thiel&rsquo;s &lsquo;Founders Fund&rsquo; venture capital firm and early Palantir employee; and Chris Brose, the former staff director of the Senate Armed Services Committee.What does Anduril want and why does this matter? Anduril&rsquo;s vision of the future is embodied by the tech it builds today, which includes AI-infused sentry towers that can be deployed to autonomously scan and survey areas (like national borders), the &lsquo;Ghost sUAS&lsquo; autonomous helicopter; and its &lsquo;Anvil sUAS&lsquo; an anti-drone weapon. The more successful companies Anduril are, the more likely it is that our future will consist of &lsquo;invisible&rsquo; walsall made up of numerous smart machines, working together for purposes set by those who can pay.  &nbsp; Read more: Anduril Raises $200 Million To Fund Ambitious Plans To Build A Defense Tech Giant (Forbes). &nbsp; Read …