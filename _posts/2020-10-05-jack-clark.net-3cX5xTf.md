---

layout: post
category: product
title: "Import AI 217: Deepfaked congressmen and deepfaked kids; steering GPT3 with GeDi; Amazon’s robots versus its humans"
date: 2020-10-05 04:26:30
link: https://vrhk.co/3cX5xTf
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Amazon funds AI research center at Columbia:Amazon is giving Columbia University $1 million&nbsp; a year for a new research center, for the next five years. Investments like this typically function as:a) a downpayment on future graduates, which Amazon will likely gain some privileged recruiting opportunities toward.b) a PR/Policy branding play, so when people say &lsquo;hey why are you hiring everyone away from academia&rsquo;, Amazon can point to thisWhy this matters: Amazon is one of the quieter big tech companies with regard to its AI research; initiatives like the Columbia grant could be a signal Amazon is going to become more public about its efforts here.&nbsp; Read more: Columbia Engineering and Amazon Announce Creation of New York AI Research Center (Columbia University blog).&nbsp;



###################################################Salesforce makes it easier to steer GPT3:&hellip;Don&rsquo;t say that! No, not that either. That? Yes! Say that!..Salesforce has updated the code for GeDi to make it work better with GPT3. GeDi, short for Generative Discriminator, is a technique to make it easier to steer the outputs of large language models towards specific types of generations. One use of GeDi is to intervene on model outputs that could display harmful or significant biases about a certain set of people.Why this matters: GeDi is an example of how researchers are beginning to build plug-in tools, techniques, and augmentations, that can be attached to existing pre-trained models (e.g, GPT3) to provide more precise control over them. I expect we&rsquo;ll see many more interventions like GeDi in the future.&nbsp; Read more: GeDi: Generative Discriminator Guided Sequence Generation (arXiv).Get the code &ndash; including the GPT3 support (Salesforce, GitHub).###################################################Twitter: One solution to AI bias? Use less AI!&hellip;Company changes strategy following auto-cropping snafu&hellip;Last month, people realized that Twitter had implemented an automatic cropping algorithm for images on the social network that seemed to have some aspects of algorithmic bias &ndash; specifically, under certain conditions the system would reliably automatically show Twitter uses pictures of white people rather than black people (when given a choice). Twitter tested its auto-cropping system for bias in 2018 when it rolled it out (though crucially, didn&rsquo;t actually publicize its bias tests), but nonetheless it seemed to fail in the wild.What went wrong? Twitter doesn&rsquo;t know: &ldquo;While our analyses to date haven&rsquo;t shown racial or gender bias, we recognize that the way we automatically crop photos means there is a potential for harm. We should&rsquo;ve done a better job of anticipating this possibility when we were first designing and building this product&rdquo;, it says.The solution? Less ML: Twitter&rsquo;s solution to this problem is to use less ML and to give its users more control over how their images appear. &ldquo;Going forward, we are committed to following the &ldquo;what you see is what you get&rdquo; principles of design, meaning quite simply: the photo you see in the Tweet composer is what it will look like in the Tweet,&rdquo; they say.&nbsp; Read more: Transparency around image cropping and changes to come (Twitter blog).###################################################Robosuite: A simulation framework for robot learning:Researchers with Stanford have built and released Robosuite, robot simulation and benchmark software based on MuJoCo. Robosuite includes simulated robots from a variety of manufacturers, including: Baxter, UR5e, Kinova3, Jaco, IIWA, Sawyer, and Panda.Tasks: The software includes several pre-integrated tasks, which researchers can test their robots against. These include:Block Lifting; block stacking; pick-and-place; nut assembly; door opening; table wiping; two arm lifting; two arm peg-in-hole; and a two arm handover.&nbsp; Read more: robosuite: A Modular Simulation Framework and Benchmark for Robot Learning (arXiv).&nbsp; Get the code for robotsuite here (ARISE Initiative, GitHub).&nbsp; More details at the official website ().###################################################US political campaign makes a deepfaked version of congressman Matt Gaetz:&hellip;A no good, very dangerous, asinine use of money, time, and attention&hellip;Phil Ehr, a US House candidate running in Florida, has put together a campaign ad where a synthetic Matt Gaetz &ldquo;says&rdquo; Q-anon sucks, Barack Obama is cool, and he&rsquo;s voting for Joe Biden. Then Phil warns viewers that they just saw an example of &ldquo;deep fake technology&rdquo;, telling them &ldquo;if our campaign can make a video like this, imagine what Putin is doing right now?&rdquo;This is the opposite of helpful: It fills up the information space with misinformation, lowers trust in media, and &ndash; at least for me subjectively &ndash; makes me think the people helping Phil run his campaign are falling foul of the AI techno-fetishism that pervades some aspects of US policymaking. &ldquo;Democrats should not normalize manipulated media in political campaigns,&rdquo; says Alex Stamos, former top security person at Facebook and Yahoo..&nbsp; Check out the disinformation here (Phil Ehr, Twitter).Campaign reanimates a gun violence victim to promote voting:Here&rsquo;s another example of very dubious uses of deepfake technology: campaign group Change the Ref uses some video synthesis technologies to resurrect one of the dead victims from the Parkland school shooting, so they can implore people to vote in the US this November. This has many of the same issues as Phil Ehr&rsquo;s use of video synthesis, and highlights how quickly this stuff is percolating into reality.&lsquo;Technomancy&rsquo;: On Twitter, some people have referred to this kind of reanimation-of-the-dead as a form of necromancy; within a few hours, some people started using the term &lsquo;technomancy&rsquo; which feels like a fitting term for this.&nbsp; Watch the video here (Change the Ref, Twitter).###################################################Report: Amazon&rsquo;s robots create safety issues by increasing speed that humans need to work:&hellip;Faster, human &ndash; work, work, work!&hellip;Picture this: your business has two types of physically-embodied worker &ndash; robots and humans. Every year, you invest money into improving the performance of your robots, and (relatively) less in your people. What happens if your robots get surprisingly capable surprisingly quickly, while your people remain mostly the same? The answer: not good things for the people. At Amazon, increased automation in warehouses seems to lead to a greater rate of injury of the human workers, according to reporting from Reveal News.Amazon&rsquo;s fulfillment centers that contain a lot of robots have a significantly higher human injury rate than those that don&rsquo;t, according to Reveal. These injuries are happening because, as the robots have got better, Amazon has raised its expectations for how much work its humans need to do. The humans, agents in capitalism as they are, then cut corners and sacrifice their own safety to keep up with the machines (and therefore, keep their jobs).&nbsp; &nbsp;&nbsp;&ldquo;The robots were too efficient. They could bring items so quickly that the productivity expectations for workers more than doubled, according to a former senior operations manager who saw the transformation. And they kept climbing. At the most common kind of warehouse, workers called pickers &ndash; who previously had to grab and scan about 100 items an hour &ndash; were expected to hit rates of up to 400 an hour at robotic fulfillment centers,&rdquo; Reveal says.&nbsp; &nbsp;Read more: How Amazon hit its safety crisis (Reveal News).###################################################&nbsp;What does AI progress look like?&hellip;State of AI Report 2020 tries to measure and assess the …"

---

### Import AI 217: Deepfaked congressmen and deepfaked kids; steering GPT3 with GeDi; Amazon’s robots versus its humans

Amazon funds AI research center at Columbia:Amazon is giving Columbia University $1 million&nbsp; a year for a new research center, for the next five years. Investments like this typically function as:a) a downpayment on future graduates, which Amazon will likely gain some privileged recruiting opportunities toward.b) a PR/Policy branding play, so when people say &lsquo;hey why are you hiring everyone away from academia&rsquo;, Amazon can point to thisWhy this matters: Amazon is one of the quieter big tech companies with regard to its AI research; initiatives like the Columbia grant could be a signal Amazon is going to become more public about its efforts here.&nbsp; Read more: Columbia Engineering and Amazon Announce Creation of New York AI Research Center (Columbia University blog).&nbsp;



###################################################Salesforce makes it easier to steer GPT3:&hellip;Don&rsquo;t say that! No, not that either. That? Yes! Say that!..Salesforce has updated the code for GeDi to make it work better with GPT3. GeDi, short for Generative Discriminator, is a technique to make it easier to steer the outputs of large language models towards specific types of generations. One use of GeDi is to intervene on model outputs that could display harmful or significant biases about a certain set of people.Why this matters: GeDi is an example of how researchers are beginning to build plug-in tools, techniques, and augmentations, that can be attached to existing pre-trained models (e.g, GPT3) to provide more precise control over them. I expect we&rsquo;ll see many more interventions like GeDi in the future.&nbsp; Read more: GeDi: Generative Discriminator Guided Sequence Generation (arXiv).Get the code &ndash; including the GPT3 support (Salesforce, GitHub).###################################################Twitter: One solution to AI bias? Use less AI!&hellip;Company changes strategy following auto-cropping snafu&hellip;Last month, people realized that Twitter had implemented an automatic cropping algorithm for images on the social network that seemed to have some aspects of algorithmic bias &ndash; specifically, under certain conditions the system would reliably automatically show Twitter uses pictures of white people rather than black people (when given a choice). Twitter tested its auto-cropping system for bias in 2018 when it rolled it out (though crucially, didn&rsquo;t actually publicize its bias tests), but nonetheless it seemed to fail in the wild.What went wrong? Twitter doesn&rsquo;t know: &ldquo;While our analyses to date haven&rsquo;t shown racial or gender bias, we recognize that the way we automatically crop photos means there is a potential for harm. We should&rsquo;ve done a better job of anticipating this possibility when we were first designing and building this product&rdquo;, it says.The solution? Less ML: Twitter&rsquo;s solution to this problem is to use less ML and to give its users more control over how their images appear. &ldquo;Going forward, we are committed to following the &ldquo;what you see is what you get&rdquo; principles of design, meaning quite simply: the photo you see in the Tweet composer is what it will look like in the Tweet,&rdquo; they say.&nbsp; Read more: Transparency around image cropping and changes to come (Twitter blog).###################################################Robosuite: A simulation framework for robot learning:Researchers with Stanford have built and released Robosuite, robot simulation and benchmark software based on MuJoCo. Robosuite includes simulated robots from a variety of manufacturers, including: Baxter, UR5e, Kinova3, Jaco, IIWA, Sawyer, and Panda.Tasks: The software includes several pre-integrated tasks, which researchers can test their robots against. These include:Block Lifting; block stacking; pick-and-place; nut assembly; door opening; table wiping; two arm lifting; two arm peg-in-hole; and a two arm handover.&nbsp; Read more: robosuite: A Modular Simulation Framework and Benchmark for Robot Learning (arXiv).&nbsp; Get the code for robotsuite here (ARISE Initiative, GitHub).&nbsp; More details at the official website ().###################################################US political campaign makes a deepfaked version of congressman Matt Gaetz:&hellip;A no good, very dangerous, asinine use of money, time, and attention&hellip;Phil Ehr, a US House candidate running in Florida, has put together a campaign ad where a synthetic Matt Gaetz &ldquo;says&rdquo; Q-anon sucks, Barack Obama is cool, and he&rsquo;s voting for Joe Biden. Then Phil warns viewers that they just saw an example of &ldquo;deep fake technology&rdquo;, telling them &ldquo;if our campaign can make a video like this, imagine what Putin is doing right now?&rdquo;This is the opposite of helpful: It fills up the information space with misinformation, lowers trust in media, and &ndash; at least for me subjectively &ndash; makes me think the people helping Phil run his campaign are falling foul of the AI techno-fetishism that pervades some aspects of US policymaking. &ldquo;Democrats should not normalize manipulated media in political campaigns,&rdquo; says Alex Stamos, former top security person at Facebook and Yahoo..&nbsp; Check out the disinformation here (Phil Ehr, Twitter).Campaign reanimates a gun violence victim to promote voting:Here&rsquo;s another example of very dubious uses of deepfake technology: campaign group Change the Ref uses some video synthesis technologies to resurrect one of the dead victims from the Parkland school shooting, so they can implore people to vote in the US this November. This has many of the same issues as Phil Ehr&rsquo;s use of video synthesis, and highlights how quickly this stuff is percolating into reality.&lsquo;Technomancy&rsquo;: On Twitter, some people have referred to this kind of reanimation-of-the-dead as a form of necromancy; within a few hours, some people started using the term &lsquo;technomancy&rsquo; which feels like a fitting term for this.&nbsp; Watch the video here (Change the Ref, Twitter).###################################################Report: Amazon&rsquo;s robots create safety issues by increasing speed that humans need to work:&hellip;Faster, human &ndash; work, work, work!&hellip;Picture this: your business has two types of physically-embodied worker &ndash; robots and humans. Every year, you invest money into improving the performance of your robots, and (relatively) less in your people. What happens if your robots get surprisingly capable surprisingly quickly, while your people remain mostly the same? The answer: not good things for the people. At Amazon, increased automation in warehouses seems to lead to a greater rate of injury of the human workers, according to reporting from Reveal News.Amazon&rsquo;s fulfillment centers that contain a lot of robots have a significantly higher human injury rate than those that don&rsquo;t, according to Reveal. These injuries are happening because, as the robots have got better, Amazon has raised its expectations for how much work its humans need to do. The humans, agents in capitalism as they are, then cut corners and sacrifice their own safety to keep up with the machines (and therefore, keep their jobs).&nbsp; &nbsp;&nbsp;&ldquo;The robots were too efficient. They could bring items so quickly that the productivity expectations for workers more than doubled, according to a former senior operations manager who saw the transformation. And they kept climbing. At the most common kind of warehouse, workers called pickers &ndash; who previously had to grab and scan about 100 items an hour &ndash; were expected to hit rates of up to 400 an hour at robotic fulfillment centers,&rdquo; Reveal says.&nbsp; &nbsp;Read more: How Amazon hit its safety crisis (Reveal News).###################################################&nbsp;What does AI progress look like?&hellip;State of AI Report 2020 tries to measure and assess the …