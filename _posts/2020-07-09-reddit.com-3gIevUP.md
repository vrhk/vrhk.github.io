---

layout: post
category: threads
title: "[D] Shouldn't training using a maximum operation be much slower than softmax because of discontinuous gradient?"
date: 2020-07-09 21:17:33
link: https://vrhk.co/3gIevUP
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "If you use torch.max(...) When you differentiate that, it just passes gradient through the maximum value and has zero gradient for everything..."

---

### [D] Shouldn't training using a maximum operation be much slower than softmax because of discontinuous gradient?

If you use torch.max(...) When you differentiate that, it just passes gradient through the maximum value and has zero gradient for everything...