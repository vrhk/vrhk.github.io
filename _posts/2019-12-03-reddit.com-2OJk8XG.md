---

layout: post
category: threads
title: "[D] Why is unsupervised pre-training successful in NLP ?"
date: 2019-12-03 18:17:17
link: https://vrhk.co/2OJk8XG
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "For examples, BERT and GPT-2, both achieve sota across several NLP tasks."

---

### [D] Why is unsupervised pre-training successful in NLP ?

For examples, BERT and GPT-2, both achieve sota across several NLP tasks.