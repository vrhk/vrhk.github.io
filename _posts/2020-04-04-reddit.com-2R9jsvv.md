---

layout: post
category: threads
title: "[R] Between fine-tuning and pre-training, which has more precedence over a language model like GPT-2?"
date: 2020-04-04 00:17:37
link: https://vrhk.co/2R9jsvv
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Consider pre-trained language models like BERT and GPT-2. My principal question is the extent to which fine-tuning the model changes its..."

---

### [R] Between fine-tuning and pre-training, which has more precedence over a language model like GPT-2?

Consider pre-trained language models like BERT and GPT-2. My principal question is the extent to which fine-tuning the model changes its...