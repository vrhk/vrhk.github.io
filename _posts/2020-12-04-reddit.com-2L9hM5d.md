---

layout: post
category: threads
title: "[P] A PyTorch implementation of the paper - \"Synthesizer: Rethinking Self-Attention in Transformer Models\""
date: 2020-12-04 13:37:26
link: https://vrhk.co/2L9hM5d
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "This [paper](<https://arxiv.org/abs/2005.00743>) by google researchers proposed an efficient alternative to the regular scaled dot product attention..."

---

### [P] A PyTorch implementation of the paper - "Synthesizer: Rethinking Self-Attention in Transformer Models"

This [paper](<https://arxiv.org/abs/2005.00743>) by google researchers proposed an efficient alternative to the regular scaled dot product attention...