---

layout: post
category: threads
title: "[D] Has anyone pre-trained a GPT-2 model smaller than the OpenAI small(117M) model?"
date: 2020-05-07 22:27:34
link: https://vrhk.co/3cdDs93
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "I'm interested to know if anyone has set their own hyperparams for a smaller GPT-2 mode(ie 60M) size and pre-trained in favor of lower resource..."

---

### [D] Has anyone pre-trained a GPT-2 model smaller than the OpenAI small(117M) model?

I'm interested to know if anyone has set their own hyperparams for a smaller GPT-2 mode(ie 60M) size and pre-trained in favor of lower resource...