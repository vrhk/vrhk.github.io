---

layout: post
category: threads
title: "[D] Paper Explained - Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Full Video Analysis)"
date: 2021-01-22 20:37:42
link: https://vrhk.co/39aJN5K
image: https://external-preview.redd.it/QSEkx-i0Tyq9ug3-RVB7j7aaD05vyu6HrezKtBcRaxI.jpg?width=480&height=251.308900524&auto=webp&crop=480:251.308900524,smart&s=1e42c86b090e2bfb3f2fb023a43762932c1a9897
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "[<https://youtu.be/iAR8LkkMMIM>](<https://youtu.be/iAR8LkkMMIM>) Scale is the next frontier for AI. Google Brain uses sparsity and hard routing to..."

---

### [D] Paper Explained - Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Full Video Analysis)

[<https://youtu.be/iAR8LkkMMIM>](<https://youtu.be/iAR8LkkMMIM>) Scale is the next frontier for AI. Google Brain uses sparsity and hard routing to...