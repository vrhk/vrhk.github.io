---

layout: post
category: threads
title: "[D] any principled reason for cross entropy instead of L2 in language modelling? (more details in post)"
date: 2019-11-02 19:22:28
link: https://vrhk.co/2WyWNKM
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Is there any principled reason for doing softmax and cross entropy for the loss in for example transformers, rather than doing L2 over the target..."

---

### [D] any principled reason for cross entropy instead of L2 in language modelling? (more details in post)

Is there any principled reason for doing softmax and cross entropy for the loss in for example transformers, rather than doing L2 over the target...