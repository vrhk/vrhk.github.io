---

layout: post
category: threads
title: "[R]Research Guide: Model Distillation Techniques for Deep Learning"
date: 2019-11-20 16:27:28
link: https://vrhk.co/2QxeMjM
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Knowledge distillation is a model compression technique whereby a small network (student) is taught by a larger trained neural network (teacher)...."

---

### [R]Research Guide: Model Distillation Techniques for Deep Learning

Knowledge distillation is a model compression technique whereby a small network (student) is taught by a larger trained neural network (teacher)....