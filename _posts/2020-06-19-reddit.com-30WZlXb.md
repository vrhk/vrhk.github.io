---

layout: post
category: threads
title: "[D] Effectiveness of mixed precision weights/activation quantization?"
date: 2020-06-19 15:17:32
link: https://vrhk.co/30WZlXb
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Many model compression methods allocate a different number of bits to each layer/activation/weight (ie some weights only get 1 bit, other weights..."

---

### [D] Effectiveness of mixed precision weights/activation quantization?

Many model compression methods allocate a different number of bits to each layer/activation/weight (ie some weights only get 1 bit, other weights...