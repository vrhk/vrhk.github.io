---

layout: post
category: threads
title: "r/MachineLearning - [D] Research shows SGD with too large of a mini batch can lead to huge overfitting in deep learning. Why doesn't batch gradient descent have this problem?"
date: 2019-08-29 09:17:29
link: https://vrhk.co/32cxf7W
image: 
domain: reddit.com
author: "reddit"
icon: https://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "0 votes and 0 comments so far on Reddit"

---

### r/MachineLearning - [D] Research shows SGD with too large of a mini batch can lead to huge overfitting in deep learning. Why doesn't batch gradient descent have this problem?

0 votes and 0 comments so far on Reddit