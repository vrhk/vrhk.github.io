---

layout: post
category: product
title: "Import AI: 182: The Industrialization of AI, BERT goes Dutch, plus, AI metrics consolidation."
date: 2020-01-27 10:26:18
link: https://vrhk.co/30XBnch
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "DAWNBench is dead! Long live DAWNBench. MLPerf is our new king:&hellip;Metrics consolidation: hard, but necessary!&hellip;In the past few years, multiple initiatives have sprung up to assess the performance and cost of various AI systems when running on different hardware (and cloud) infrastructures. One of the original major competitions in this domain was DAWNBench, a Stanford-backed competition website for assessing things like inference cost, training cost, and training time for various AI tasks on different cloud infrastructures. Now, the creators of DAWNBench are retiring the benchmark in favor of MLPerf, a joint initiative from industry and academic players to &ldquo;build fair and useful benchmarks for measuring training and inference performance of ML hardware, software, and services&ldquo;.  &nbsp; Since MLPerf has become an increasingly popular benchmark &ndash; and to avoid a proliferation of inconsistent benchmarks &ndash; DAWNBench is being phased out. &ldquo;We are passing the torch to MLPerf to continue to provide fair and useful benchmarks for measuring training and inference performance,&rdquo; according to a DAWNBench blogpost. Why this matters: Benchmarks are useful. Overlapping benchmarks that split submissions across subtly different competitions are less useful &ndash; it takes a lot of discipline to avoid proliferation of overlapping evaluation systems, so kudos to the DAWNBench team for intentionally phasing out the project. I&rsquo;m looking forward to studying the new MLPerf evaluations as they come out.  &nbsp; Read more: Ending Rolling Submissions for DAWNBench (Stanford DAWNBench blog). &nbsp; Read more: about MLPerf (official MLPerf website).&nbsp; ####################################################
This week&rsquo;s Import A-Idea: The Industrialization of AI AI is a &ldquo;fourth industrial revolution&rdquo;, according to various CEOs and PR agencies around the world. They usually use this phrasing to indicate the apparent power of AI technology. Funnily enough, they don&rsquo;t use it to indicate the inherent inequality and power-structure changes enforced by an industrial resolution.So, what is the Industrialization of AI? (First mention: Import AI #115) It&rsquo;s what happens when AI goes from an artisanal, craftsperson-based profession to a repeatable, professional-based profession. The Industrialization of AI involves a combination of tooling improvement (e.g., the maturation of deep learning frameworks), as well as growing investment in the capital-intensive inputs to AI (e.g., rising investments in data and compute). We&rsquo;ve already seen the early hints of this as AI software frameworks have evolved from things built by individuals and random grad students at universities (Theano, Lasagne, etc), to industry-developed systems (TensorFlow, PyTorch).&nbsp; What happens next: Industrialization gave us: the luddites, populist anger, massive social and political change, and the rearrangement and consolidation of political power among capital-owners. It stands to reason that the rise of AI will lead to the same thing (at minimum) &ndash; leading me to ask, who will be the winners and the losers in this industrial revolution? And when various elites call AI a new industrial revolution, who stands to gain and lose? And what might the economic dividends be of industrialization, and how might the world around us change in response?
####################################################Using AI &amp; satellites data to spot refugee boats:..Space-Eye wants to use AI to count migrants and spot crises&hellip;European researchers are using machine learning to create AI systems that can identify refugee boats in satellite photos of the Mediterranean. The initial idea is to generate data about the migrant crisis and, in the long term, they hope such a system can help send aid to boats in real-time, in response to threats. Why this matters: One of the promises of AI is we can use it to monitor things we care about &ndash; human lives, the health of fragile ecosystems like rainforests, and so on. Things like Space-Eye show how AI industrialization is creating derivatives, like open datasets and open computer vision techniques, that researchers can use to carry out acts of social justice. Read more: Europe&rsquo;s migration crisis seen from orbit (Politico). Find out more about Space-Eye here at the official site.
####################################################Dutch BERT: Cultural representation through data selection:&hellip;Language models as implicitly political entities&hellip; Researchers with TU Berlin have built RobBERT, a RoBERTa-based language model trained on a large amount of Dutch data. Specifically, they train a model on top of 39 GB of text taken from the Dutch section of the multilingual &lsquo;OSCAR&rsquo; dataset. Why this matters: AI models are going to magnify whichever culture they&rsquo;ve been trained on. Most text-based AI models are trained on English or Chinese datasets, magnifying those cultures via their presence in these AI artefacts. Systems like RobBERT help broaden cultural representation in AI.  &nbsp; Read more: RobBERT: a Dutch RoBERTa-based Language Model (arXiv).  &nbsp; Get the code for RobBERT here (RobBERT GitHub).&nbsp;
####################################################
Is a safe autonomous machine an AGI? How should we make machines that deal with the unexpected?&hellip;Israeli researchers promote habits and procedures for when the world inevitably explodes&hellip;Researchers with IBM and the Weizmann Institute of Science in Israel know that the world is a cruel, unpredictable place. Now they&rsquo;re trying to work out principles we can imbue in machines to let them deal with this essential unpredictability. &ldquo;We propose several engineering practices that can help toward successful handling of the always-impending occurrence of unexpected events and conditions,&rdquo; they write. The paper summarizes a bunch of sensible approaches for increasing the safety and reliability of autonomous systems, but skips over many of the known-hard problems inherent to contemporary AI research.Dealing with the unexpected: So, what principles can we apply to machine design to make them safe in unexpected situations? The authors have a few ideas. These are:&ndash; Machines should run away from dangerous or confusing situations&ndash; Machines should try and &lsquo;probe&rsquo; their environment by exploring &ndash; e.g., if a robot finds its path is blocked by an object it should probably work out if the object is light and movable (for instance, a cardboard box) or immovable. &ndash; Any machine should &ldquo;be able to look at itself and recognize its own state and history, and use this information in its decision making,&rdquo; they write. &ndash; We should give machines as many sensors as possible so they can have a lot of knowledge about their environment. Such sensors should be generally accessible to software running on the machine, rather than silo&rsquo;d. &ndash; The machine should be able to collect data in real-time and integrate it into its planning&ndash; The machine should have &ldquo;access to General World Knowledge&rdquo; (that high-pitched scream you&rsquo;re hearing in response to this phrase is Doug Lenat sensing a disturbance in the force at Cyc and reacting appropriately). &ndash; The machine should know when to mimic others and when to do its own thing. It should have the same capability with regard to seeking advice, or following its own intuition. No AGI, no safety? One thing worth remarking on is that the above list is basically a description of the capabilities you might expect a generally intelligence machine to have. It&rsquo;s also a set of capabilities that are pretty distant from the capabilities of today&rsquo;s systems. Why this matters: Papers like this are, functionally, tools for socializing some of the wackier ideas inherent to long-term AI research and/…"

---

### Import AI: 182: The Industrialization of AI, BERT goes Dutch, plus, AI metrics consolidation.

DAWNBench is dead! Long live DAWNBench. MLPerf is our new king:&hellip;Metrics consolidation: hard, but necessary!&hellip;In the past few years, multiple initiatives have sprung up to assess the performance and cost of various AI systems when running on different hardware (and cloud) infrastructures. One of the original major competitions in this domain was DAWNBench, a Stanford-backed competition website for assessing things like inference cost, training cost, and training time for various AI tasks on different cloud infrastructures. Now, the creators of DAWNBench are retiring the benchmark in favor of MLPerf, a joint initiative from industry and academic players to &ldquo;build fair and useful benchmarks for measuring training and inference performance of ML hardware, software, and services&ldquo;.  &nbsp; Since MLPerf has become an increasingly popular benchmark &ndash; and to avoid a proliferation of inconsistent benchmarks &ndash; DAWNBench is being phased out. &ldquo;We are passing the torch to MLPerf to continue to provide fair and useful benchmarks for measuring training and inference performance,&rdquo; according to a DAWNBench blogpost. Why this matters: Benchmarks are useful. Overlapping benchmarks that split submissions across subtly different competitions are less useful &ndash; it takes a lot of discipline to avoid proliferation of overlapping evaluation systems, so kudos to the DAWNBench team for intentionally phasing out the project. I&rsquo;m looking forward to studying the new MLPerf evaluations as they come out.  &nbsp; Read more: Ending Rolling Submissions for DAWNBench (Stanford DAWNBench blog). &nbsp; Read more: about MLPerf (official MLPerf website).&nbsp; ####################################################
This week&rsquo;s Import A-Idea: The Industrialization of AI AI is a &ldquo;fourth industrial revolution&rdquo;, according to various CEOs and PR agencies around the world. They usually use this phrasing to indicate the apparent power of AI technology. Funnily enough, they don&rsquo;t use it to indicate the inherent inequality and power-structure changes enforced by an industrial resolution.So, what is the Industrialization of AI? (First mention: Import AI #115) It&rsquo;s what happens when AI goes from an artisanal, craftsperson-based profession to a repeatable, professional-based profession. The Industrialization of AI involves a combination of tooling improvement (e.g., the maturation of deep learning frameworks), as well as growing investment in the capital-intensive inputs to AI (e.g., rising investments in data and compute). We&rsquo;ve already seen the early hints of this as AI software frameworks have evolved from things built by individuals and random grad students at universities (Theano, Lasagne, etc), to industry-developed systems (TensorFlow, PyTorch).&nbsp; What happens next: Industrialization gave us: the luddites, populist anger, massive social and political change, and the rearrangement and consolidation of political power among capital-owners. It stands to reason that the rise of AI will lead to the same thing (at minimum) &ndash; leading me to ask, who will be the winners and the losers in this industrial revolution? And when various elites call AI a new industrial revolution, who stands to gain and lose? And what might the economic dividends be of industrialization, and how might the world around us change in response?
####################################################Using AI &amp; satellites data to spot refugee boats:..Space-Eye wants to use AI to count migrants and spot crises&hellip;European researchers are using machine learning to create AI systems that can identify refugee boats in satellite photos of the Mediterranean. The initial idea is to generate data about the migrant crisis and, in the long term, they hope such a system can help send aid to boats in real-time, in response to threats. Why this matters: One of the promises of AI is we can use it to monitor things we care about &ndash; human lives, the health of fragile ecosystems like rainforests, and so on. Things like Space-Eye show how AI industrialization is creating derivatives, like open datasets and open computer vision techniques, that researchers can use to carry out acts of social justice. Read more: Europe&rsquo;s migration crisis seen from orbit (Politico). Find out more about Space-Eye here at the official site.
####################################################Dutch BERT: Cultural representation through data selection:&hellip;Language models as implicitly political entities&hellip; Researchers with TU Berlin have built RobBERT, a RoBERTa-based language model trained on a large amount of Dutch data. Specifically, they train a model on top of 39 GB of text taken from the Dutch section of the multilingual &lsquo;OSCAR&rsquo; dataset. Why this matters: AI models are going to magnify whichever culture they&rsquo;ve been trained on. Most text-based AI models are trained on English or Chinese datasets, magnifying those cultures via their presence in these AI artefacts. Systems like RobBERT help broaden cultural representation in AI.  &nbsp; Read more: RobBERT: a Dutch RoBERTa-based Language Model (arXiv).  &nbsp; Get the code for RobBERT here (RobBERT GitHub).&nbsp;
####################################################
Is a safe autonomous machine an AGI? How should we make machines that deal with the unexpected?&hellip;Israeli researchers promote habits and procedures for when the world inevitably explodes&hellip;Researchers with IBM and the Weizmann Institute of Science in Israel know that the world is a cruel, unpredictable place. Now they&rsquo;re trying to work out principles we can imbue in machines to let them deal with this essential unpredictability. &ldquo;We propose several engineering practices that can help toward successful handling of the always-impending occurrence of unexpected events and conditions,&rdquo; they write. The paper summarizes a bunch of sensible approaches for increasing the safety and reliability of autonomous systems, but skips over many of the known-hard problems inherent to contemporary AI research.Dealing with the unexpected: So, what principles can we apply to machine design to make them safe in unexpected situations? The authors have a few ideas. These are:&ndash; Machines should run away from dangerous or confusing situations&ndash; Machines should try and &lsquo;probe&rsquo; their environment by exploring &ndash; e.g., if a robot finds its path is blocked by an object it should probably work out if the object is light and movable (for instance, a cardboard box) or immovable. &ndash; Any machine should &ldquo;be able to look at itself and recognize its own state and history, and use this information in its decision making,&rdquo; they write. &ndash; We should give machines as many sensors as possible so they can have a lot of knowledge about their environment. Such sensors should be generally accessible to software running on the machine, rather than silo&rsquo;d. &ndash; The machine should be able to collect data in real-time and integrate it into its planning&ndash; The machine should have &ldquo;access to General World Knowledge&rdquo; (that high-pitched scream you&rsquo;re hearing in response to this phrase is Doug Lenat sensing a disturbance in the force at Cyc and reacting appropriately). &ndash; The machine should know when to mimic others and when to do its own thing. It should have the same capability with regard to seeking advice, or following its own intuition. No AGI, no safety? One thing worth remarking on is that the above list is basically a description of the capabilities you might expect a generally intelligence machine to have. It&rsquo;s also a set of capabilities that are pretty distant from the capabilities of today&rsquo;s systems. Why this matters: Papers like this are, functionally, tools for socializing some of the wackier ideas inherent to long-term AI research and/…