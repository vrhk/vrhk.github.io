---

layout: post
category: product
title: "Import AI 203: DeepMind releases a synthetic robot dog;  harvesting kiwifruit with robots; plus, egocentric AI"
date: 2020-06-29 17:36:39
link: https://vrhk.co/38aUF1y
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Train first-person AI systems with EPIC-Kitchens:&hellip;Expanded dataset gives researchers a bunch of egocentric films to train AI systems against&hellip;Researchers with the University of Bristol and the University of Catania have introduced EPIC-KITCHENS-100, a dataset of&nbsp; first-person perspective videos of people doing a range of things like cooking or washing. The interesting thing about the dataset is that the videos are accompanied with narratives, which are the participants describing their actions as they record them. This means the dataset comes along with rich annotations developed in an open-ended format. Dataset details:
100 hours of recording
20 million frames
45 kitchens in four cities
90,000 distinct action segments&nbsp;
What can EPIC test? The EPIC dataset can help researchers test out AI systems against a few AI challenges, including: &ndash; Action recognition (e.g, figuring out if a video clip contains a given action)&ndash; Action detection (e.g, looks like they are washing dishes at this point in a long video clip)&ndash; Action anticipation (e.g, looks like someone is about to start washing dishes)&ndash; Action generalization (can you figure out actions in these videos, via pre-training on some other videos?)Why this matters: Egocentric video doesn&rsquo;t get as much research exploration as third-person video, likely as a consequence of the availability of data (there&rsquo;s a lot of third-person video online, but relatively little egocentric video). Making progress on this will make it easier to build embodied robots. It&rsquo;ll also let us build better systems for analyzing video uploaded to social media &ndash; recall how the Christchurch shooting videos posed a challenge to Facebook algorithms because of its first-person perspective, which its systems hadn&rsquo;t seen much of before.  &nbsp; Read the research paper: Rescaling Egocentric Vision (arXiv). &nbsp; Get the data from here: available July 1st (EPIC-Kitchens-100 GitHub website).####################################################China&rsquo;s Didi plans million-strong self-driving fleet:&hellip;But don&rsquo;t hold your breath&hellip;Chinese Uber-rival Didi Chuxing says it plans to deploy a million self-driving vehicles by 2030, according to comments reported by the BBC. That seems like a realistic goal, especially compared to the more ambitious proclamations from other companies. (Tesla said in April 2019 it planned to have a million self-driving &lsquo;robotaxis&rsquo; on the road within a year to a year and three months &ndash; this has not happened). &nbsp; Read more: Didi Chuxing: Apple-backed firm aims for one million robotaxis (BBC News).####################################################NVIDIA and Mercedes want to build the upgradable self-driving car: &hellip;Industry partnership tells us about what really matters in self-driving cars&hellip;What differentiates cars from each other? Engines? Non-strategic (and getting simpler, thanks to the switchover to electric cars). Speed? Cars are all pretty fast these days. Reliability? A non-issue with established brands, and also getting easier as we build electric cars. Computers? That might be an actual differentiator. Especially as we head into a world full of self-driving cars. For this reason, NVIDIA and Mercedes-Benz have announced a partnership where, starting in 2024, NVIDIA&rsquo;s self-driving car technology will be rolled out across the car company&rsquo;s next fleet of vehicles. The two firms plan to collaborate on self-driving features like smart cruise control and lane changing, as well as automated parking. They&rsquo;re also going to try and do some hard self-driving stuff as well &ndash; this includes the plan to &ldquo;automate driving of regular routes from address to address,&rdquo; according to an NVIDIA press release. &ldquo;It is so exciting to see my years of research on a cockpit AI that tracks drivers&rsquo; face and gaze @nvidia be a part of this partnership, writes one NVIDIA researcher on Twitter.The upgradeable car: Similar to Tesla, Mercedes plans to offer over-the-air updates to its cars, letting customers buy more intelligent capabilities as time goes on. Why this matters: If the 20th century was driven by the harnessing of oil and petroleum byproducts, then there&rsquo;s a good chance the 21st century (or at least the first half) will be defined by our ability to harness computers and computer by-products. Partnerships like NVIDIA and Mercedes highlight how strategic computers are seen to be by modern companies, and suggests the emergence of a new scarce resource in business &ndash; computational ability.  &nbsp; Read more: Mercedes-Benz and NVIDIA to Build Software-Defined Computing Architecture for Automated Driving Across Future Fleet (NVIDIA newsroom). ####################################################Coming soon: Kiwi Fruit-Harvesting Robots&hellip;But performance is still somewhat poor&hellip;One promising use case for real world robots is in the harvesting of fruits and vegetables. To be able to build useful machines here, some problems require better computer vision techniques so that our machines can see what they need to gather. Researchers with the University of Auckland, New Zealand, have built a system that can analyze a kiwifruit orchard and pick out individual kiwis automatically via semantic segmentation. The score: The model gets a score of 87% recall at detecting non-occluded kiwi fruits, and a score of 30% for occluded ones. It gets around 75% recall and 92% precision overall. The authors used a small dataset of 63 labeled pictures of kiwifruit in orchards. By comparison, a Faster-R-CNN model trained with 100X the amount of data a couple of years ago got a recall of 96.7 and precision of 89.3 (versus 92 here), suggesting their semantic segmentation approach has helped them get slightly better performance. Sidenote: I love this genre of research papers: identify a very narrow task / problem area, then build a specific model and/or dataset for this task, then publish the results. Concise and illuminating.  &nbsp; Read more: Kiwifruit detection in challenging conditions (arXiv).####################################################DeepMind expands its software for robot control:&hellip;Run, simulated dog, run!&hellip;DeepMind has published the latest version of the DeepMind Control Suite, dm_control. This software gives access to a MuJoCo-based simulator for training AI systems to solve continuous control problems, like figuring out how to operate a complex multi-jointed robot in a virtual domain. &ldquo;It offers a wide range of pre-designed RL tasks and a rich framework for designing new ones,&rdquo; DeepMind writes in a research paper discussing the software.Dogs, procedural bodies, and more: dm_control includes a few tools that aren&rsquo;t trivially available elsewhere. These include: a &lsquo;Phaero Dog&rsquo; model with 191 total state dimensions (making it very complex), as well as a quadruped robot, and a simulated robot arm. The software also includes support for PyMJCF &ndash; a language to let people procedurally compose new simulated entities that they can try and train AI systems to control. Things that make you go &lsquo;hmmm&rsquo;: In a research writeup, DeepMind also discusses a dog-grade complexity Rodent model, which it says it has developed &ldquo;in order to better compare learned behaviour with experimental settings common in the life sciences&rdquo;. Why this matters: Simulators are one of the key ingredients for AI research &ndash; they&rsquo;re basically engines for generating new datasets for complex problems, like learning to operate multi-jointed bodies. Systems like dm_control give us a sense of progression in the field (it was only a few years ago that most people were working on 2D simulated robot arms with something on the of 7 dimensions of control &ndash; now we&rsquo;re working on dogs with more than 20 times that num…"

---

### Import AI 203: DeepMind releases a synthetic robot dog;  harvesting kiwifruit with robots; plus, egocentric AI

Train first-person AI systems with EPIC-Kitchens:&hellip;Expanded dataset gives researchers a bunch of egocentric films to train AI systems against&hellip;Researchers with the University of Bristol and the University of Catania have introduced EPIC-KITCHENS-100, a dataset of&nbsp; first-person perspective videos of people doing a range of things like cooking or washing. The interesting thing about the dataset is that the videos are accompanied with narratives, which are the participants describing their actions as they record them. This means the dataset comes along with rich annotations developed in an open-ended format. Dataset details:
100 hours of recording
20 million frames
45 kitchens in four cities
90,000 distinct action segments&nbsp;
What can EPIC test? The EPIC dataset can help researchers test out AI systems against a few AI challenges, including: &ndash; Action recognition (e.g, figuring out if a video clip contains a given action)&ndash; Action detection (e.g, looks like they are washing dishes at this point in a long video clip)&ndash; Action anticipation (e.g, looks like someone is about to start washing dishes)&ndash; Action generalization (can you figure out actions in these videos, via pre-training on some other videos?)Why this matters: Egocentric video doesn&rsquo;t get as much research exploration as third-person video, likely as a consequence of the availability of data (there&rsquo;s a lot of third-person video online, but relatively little egocentric video). Making progress on this will make it easier to build embodied robots. It&rsquo;ll also let us build better systems for analyzing video uploaded to social media &ndash; recall how the Christchurch shooting videos posed a challenge to Facebook algorithms because of its first-person perspective, which its systems hadn&rsquo;t seen much of before.  &nbsp; Read the research paper: Rescaling Egocentric Vision (arXiv). &nbsp; Get the data from here: available July 1st (EPIC-Kitchens-100 GitHub website).####################################################China&rsquo;s Didi plans million-strong self-driving fleet:&hellip;But don&rsquo;t hold your breath&hellip;Chinese Uber-rival Didi Chuxing says it plans to deploy a million self-driving vehicles by 2030, according to comments reported by the BBC. That seems like a realistic goal, especially compared to the more ambitious proclamations from other companies. (Tesla said in April 2019 it planned to have a million self-driving &lsquo;robotaxis&rsquo; on the road within a year to a year and three months &ndash; this has not happened). &nbsp; Read more: Didi Chuxing: Apple-backed firm aims for one million robotaxis (BBC News).####################################################NVIDIA and Mercedes want to build the upgradable self-driving car: &hellip;Industry partnership tells us about what really matters in self-driving cars&hellip;What differentiates cars from each other? Engines? Non-strategic (and getting simpler, thanks to the switchover to electric cars). Speed? Cars are all pretty fast these days. Reliability? A non-issue with established brands, and also getting easier as we build electric cars. Computers? That might be an actual differentiator. Especially as we head into a world full of self-driving cars. For this reason, NVIDIA and Mercedes-Benz have announced a partnership where, starting in 2024, NVIDIA&rsquo;s self-driving car technology will be rolled out across the car company&rsquo;s next fleet of vehicles. The two firms plan to collaborate on self-driving features like smart cruise control and lane changing, as well as automated parking. They&rsquo;re also going to try and do some hard self-driving stuff as well &ndash; this includes the plan to &ldquo;automate driving of regular routes from address to address,&rdquo; according to an NVIDIA press release. &ldquo;It is so exciting to see my years of research on a cockpit AI that tracks drivers&rsquo; face and gaze @nvidia be a part of this partnership, writes one NVIDIA researcher on Twitter.The upgradeable car: Similar to Tesla, Mercedes plans to offer over-the-air updates to its cars, letting customers buy more intelligent capabilities as time goes on. Why this matters: If the 20th century was driven by the harnessing of oil and petroleum byproducts, then there&rsquo;s a good chance the 21st century (or at least the first half) will be defined by our ability to harness computers and computer by-products. Partnerships like NVIDIA and Mercedes highlight how strategic computers are seen to be by modern companies, and suggests the emergence of a new scarce resource in business &ndash; computational ability.  &nbsp; Read more: Mercedes-Benz and NVIDIA to Build Software-Defined Computing Architecture for Automated Driving Across Future Fleet (NVIDIA newsroom). ####################################################Coming soon: Kiwi Fruit-Harvesting Robots&hellip;But performance is still somewhat poor&hellip;One promising use case for real world robots is in the harvesting of fruits and vegetables. To be able to build useful machines here, some problems require better computer vision techniques so that our machines can see what they need to gather. Researchers with the University of Auckland, New Zealand, have built a system that can analyze a kiwifruit orchard and pick out individual kiwis automatically via semantic segmentation. The score: The model gets a score of 87% recall at detecting non-occluded kiwi fruits, and a score of 30% for occluded ones. It gets around 75% recall and 92% precision overall. The authors used a small dataset of 63 labeled pictures of kiwifruit in orchards. By comparison, a Faster-R-CNN model trained with 100X the amount of data a couple of years ago got a recall of 96.7 and precision of 89.3 (versus 92 here), suggesting their semantic segmentation approach has helped them get slightly better performance. Sidenote: I love this genre of research papers: identify a very narrow task / problem area, then build a specific model and/or dataset for this task, then publish the results. Concise and illuminating.  &nbsp; Read more: Kiwifruit detection in challenging conditions (arXiv).####################################################DeepMind expands its software for robot control:&hellip;Run, simulated dog, run!&hellip;DeepMind has published the latest version of the DeepMind Control Suite, dm_control. This software gives access to a MuJoCo-based simulator for training AI systems to solve continuous control problems, like figuring out how to operate a complex multi-jointed robot in a virtual domain. &ldquo;It offers a wide range of pre-designed RL tasks and a rich framework for designing new ones,&rdquo; DeepMind writes in a research paper discussing the software.Dogs, procedural bodies, and more: dm_control includes a few tools that aren&rsquo;t trivially available elsewhere. These include: a &lsquo;Phaero Dog&rsquo; model with 191 total state dimensions (making it very complex), as well as a quadruped robot, and a simulated robot arm. The software also includes support for PyMJCF &ndash; a language to let people procedurally compose new simulated entities that they can try and train AI systems to control. Things that make you go &lsquo;hmmm&rsquo;: In a research writeup, DeepMind also discusses a dog-grade complexity Rodent model, which it says it has developed &ldquo;in order to better compare learned behaviour with experimental settings common in the life sciences&rdquo;. Why this matters: Simulators are one of the key ingredients for AI research &ndash; they&rsquo;re basically engines for generating new datasets for complex problems, like learning to operate multi-jointed bodies. Systems like dm_control give us a sense of progression in the field (it was only a few years ago that most people were working on 2D simulated robot arms with something on the of 7 dimensions of control &ndash; now we&rsquo;re working on dogs with more than 20 times that num…