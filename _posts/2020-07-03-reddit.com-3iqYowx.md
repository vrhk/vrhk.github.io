---

layout: post
category: threads
title: "[R] Smooth Adversarial Training. They found that smooth activation functions are better than ReLU for adversarial training and can lead to substantial improvements in adversarial robustness."
date: 2020-07-03 15:17:33
link: https://vrhk.co/3iqYowx
image: https://www.redditstatic.com/new-icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Posted in r/MachineLearning by u/hardmaru • 2 points and 2 comments"

---

### [R] Smooth Adversarial Training. They found that smooth activation functions are better than ReLU for adversarial training and can lead to substantial improvements in adversarial robustness.

Posted in r/MachineLearning by u/hardmaru • 2 points and 2 comments