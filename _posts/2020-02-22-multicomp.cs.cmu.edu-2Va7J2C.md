---

layout: post
category: threads
title: "ACL2020-Challenge | MultiComp"
date: 2020-02-22 01:25:39
link: https://vrhk.co/2Va7J2C
image: 
domain: multicomp.cs.cmu.edu
author: "multicomp.cs.cmu.edu"
icon: http://multicomp.cs.cmu.edu/wp-content/themes/multicomp/images/favicon.ico
excerpt: "Second Grand Challenge and Workshop on Multimodal LanguageACL 2020Location: ACL2020 - Seattle, WA, USADate: TBD Sponsored by: News [1/07/2020] Deadline for submitting Grand-Challenge papers is May 1st. Workshop papers are due April 25th. [1/07/2020] CMU-MOSEI Grand-Challenge test data will be released on Feb 15th. Please check the challenge github link for access. [1/07/2020] Grand challenge GitHub page is released. Please check the github link. [1/05/2020] Workshop page is published. Submission template can be downloaded here (identical to ACL 2020): download zip, access overleaf ** ALL DEADLINE 11:59 PM ANYWHERE ON THE PLANET ** Table of Contents 1. Keynotes 2. Scope and Related Areas 3. Workshop Track 4. Grand-Challenge Track 5. Workshop Schedule 6. Organizing Committee Keynotes Speaker: Rada Mihalcea - University of Michigan (USA) Topic: Multimodal Language and Affect Biography: Rada Mihalcea is a Professor in the Computer Science and Engineering department at the University of Michigan. Her research interests are in computational linguistics, with a focus on lexical semantics, multilingual natural language processing, and computational social sciences. She serves or has served on the editorial boards of the Journals of Computational Linguistics, Language Resources and Evaluations, Natural Language Engineering, Research in Language in Computation, IEEE Transactions on Affective Computing, and Transactions of the Association for Computational Linguistics. She was a program co-chair for ACL (2011) and EMNLP (2009), and a general chair for the NAACL (2015). She is the recipient of a National Science Foundation CAREER award (2008) and a Presidential Early Career Award for Scientists and Engineers (2009). Speaker: Ruslan Salakhutdinov - Carnegie Mellon University (USA) Topic: Multimodal Dialogue and RL Biography: Ruslan Salakhutdinov received his Ph.D. in machine learning (computer science) from the University of Toronto in 2009. In February of 2016, he joined the Machine Learning Department at Carnegie Mellon University as an Associate Professor. Ruslan's primary interests lie in deep learning, machine learning, and large-scale optimization. He is an action editor of the Journal of Machine Learning Research and served on the senior program committee of several learning conferences including NeurIPS and ICML. He is an Alfred P. Sloan Research Fellow, Microsoft Research Faculty Fellow, Canada Research Chair in Statistical Machine Learning, a recipient of the Early Researcher Award, Connaught New Researcher Award, Google Faculty Award, Nvidia's Pioneers of AI award, and is a Senior Fellow of the Canadian Institute for Advanced Research. Speaker: M. Ehsan Hoque - University of Rochester (USA) Topic: Multimodal Healthcare and Education Biography: Dr. Hoque is an assistant professor of Computer Science and the Asaro-Biggar ('92) Family fellow at the University of Rochester. From Jan, 2018 to June 2019, he was the interim Director of the Goergen Institute for Data Science. He co-leads the Rochester Human-Computer Interaction (ROC HCI) Group. He received his PhD from MIT in 2013. His research interests span around developing computational tools to recognize the subtle nuances of human communication with a direct application of improving human ability. Speaker: Yejin Choi - University of Washington (USA) Topic: Multimodal Commonsense Biography: Yejin Choi is an associate professor of Paul G. Allen School of Computer Science &amp; Engineering at the University of Washington, adjunct of the Linguistics department, and affiliate of the Center for Statistics and Social Sciences. She is also a senior research manager at the Allen Institute for Artificial Intelligence. She is a co-recepient of the Marr Prize (best paper award) at ICCV 2013, a recepient of Borg Early Career Award (BECA) in 2018, and named among IEEE AI's 10 to Watch in 2016. She received her Ph.D. in Computer Science at Cornell University (advisor: Prof. Claire Cardie) and BS in Computer Science and Engineering at Seoul National University in Korea. Scope and Related Areas Humans adopt a structured multimodal signal to communicate with each other. This signal contains modalities of language (in terms of spoken text), visual (in terms of gestures and facial expressions) and acoustic (in terms of changes in tone of speech). This form of communication is commonly known as multimodal language. Modeling multimodal language is now a growing research area in NLP. It pushes the boundaries of multimodal learning, and requires advanced neural modeling of all three constituent modalities. Advances in this research area allow the field of NLP to take the leap towards better generalization to real-world communication (as opposed to limitation to textual applications), and better downstream impact on applications within fields such as Conversational AI, Virtual Reality, Robotics, HCI, Healthcare, and Education. In the past few years, there has been a surge of research papers in NLP conference including ACL, EMNLP, and NAACL on the topic of modeling multimodal language. Similar to first Challenge-HML in ACL18, the Second Grand-Challenge and Workshop on Multimodal Language (Chellenge-HML) brings the researchers across the globe once again together at ACL conference, to address the following fundamental current and future research challenges within multimodal language modeling. • Neural Modeling of Multimodal Language • Multimodal Dialogue Modeling and Generation • Multimodal Sentiment Analysis and Emotion Recognition • Language, Vision and Speech • Multimodal Artificial Social Intelligence Modeling • Multimodal Commonsense Reasoning • Multimodal RL and Control (Human-robot communication and multimodal language for robots) • Multimodal Healthcare • Multimodal Educational Systems • Multimodal Affective Computing • Multimodal Fusion and Alignment • Multimodal Representation Learning • Multimodal Sequential Modeling • Multimodal Co-learning and Transfer Learning • Multimodal Active Learning • Multimodal and Multimedia Resources • Creative Applications of Multimodal Learning in E-commerce, Art, and other Impactful Areas. Workshop Track Archival Track: Workshop papers are either full (8 pages) or short (4 pages) papers with infinite references. The formatting instructions are identical to ACL 2020 paper format. ACL guidelines suggest papers should be self-contained, with high presentation quality and properly compared to previous works. Publications related to all areas above are welcome to submit a paper. The submission should have at least two modalities present in their experiments (i.e. works should be related to multimodal learning). The workshop track encourages creative applications of multimodal learning, over new or understudied datasets. All accepted papers will be presented during poster sessions at the workshop. Selected papers will also be invited as contributed talks. All the papers (full or short) will be indexed as part of the ACL 2020 workshop proceedings. All submitted material should be considered novel at the time of submission. Further results and experiments on existing works is also acceptable for submission. Submission template: download zip, access overleaf. Non-archival Track: The workshop also includes a non-archival track to allow submission of previously published papers and double submissions to other conferences or journals. Accepted non-archival papers will still be presented as posters or talks at the workshop. There are no formatting or page restrictions for non-archival submissions. The accepted papers to the non-archival track will be displayed on the workshop website, but will NOT be included in the ACL 2020 Workshop proceedings or otherwise archived. Grand-Challenge Track START THE GRAND CHALLENGE HERE Grand-challenge (shared task) papers are between 6-8 pages of content with infinite references. The formatting instructions are identical to ACL 2020 paper format. ACL guidelines (se…"

---

### ACL2020-Challenge | MultiComp

Second Grand Challenge and Workshop on Multimodal LanguageACL 2020Location: ACL2020 - Seattle, WA, USADate: TBD Sponsored by: News [1/07/2020] Deadline for submitting Grand-Challenge papers is May 1st. Workshop papers are due April 25th. [1/07/2020] CMU-MOSEI Grand-Challenge test data will be released on Feb 15th. Please check the challenge github link for access. [1/07/2020] Grand challenge GitHub page is released. Please check the github link. [1/05/2020] Workshop page is published. Submission template can be downloaded here (identical to ACL 2020): download zip, access overleaf ** ALL DEADLINE 11:59 PM ANYWHERE ON THE PLANET ** Table of Contents 1. Keynotes 2. Scope and Related Areas 3. Workshop Track 4. Grand-Challenge Track 5. Workshop Schedule 6. Organizing Committee Keynotes Speaker: Rada Mihalcea - University of Michigan (USA) Topic: Multimodal Language and Affect Biography: Rada Mihalcea is a Professor in the Computer Science and Engineering department at the University of Michigan. Her research interests are in computational linguistics, with a focus on lexical semantics, multilingual natural language processing, and computational social sciences. She serves or has served on the editorial boards of the Journals of Computational Linguistics, Language Resources and Evaluations, Natural Language Engineering, Research in Language in Computation, IEEE Transactions on Affective Computing, and Transactions of the Association for Computational Linguistics. She was a program co-chair for ACL (2011) and EMNLP (2009), and a general chair for the NAACL (2015). She is the recipient of a National Science Foundation CAREER award (2008) and a Presidential Early Career Award for Scientists and Engineers (2009). Speaker: Ruslan Salakhutdinov - Carnegie Mellon University (USA) Topic: Multimodal Dialogue and RL Biography: Ruslan Salakhutdinov received his Ph.D. in machine learning (computer science) from the University of Toronto in 2009. In February of 2016, he joined the Machine Learning Department at Carnegie Mellon University as an Associate Professor. Ruslan's primary interests lie in deep learning, machine learning, and large-scale optimization. He is an action editor of the Journal of Machine Learning Research and served on the senior program committee of several learning conferences including NeurIPS and ICML. He is an Alfred P. Sloan Research Fellow, Microsoft Research Faculty Fellow, Canada Research Chair in Statistical Machine Learning, a recipient of the Early Researcher Award, Connaught New Researcher Award, Google Faculty Award, Nvidia's Pioneers of AI award, and is a Senior Fellow of the Canadian Institute for Advanced Research. Speaker: M. Ehsan Hoque - University of Rochester (USA) Topic: Multimodal Healthcare and Education Biography: Dr. Hoque is an assistant professor of Computer Science and the Asaro-Biggar ('92) Family fellow at the University of Rochester. From Jan, 2018 to June 2019, he was the interim Director of the Goergen Institute for Data Science. He co-leads the Rochester Human-Computer Interaction (ROC HCI) Group. He received his PhD from MIT in 2013. His research interests span around developing computational tools to recognize the subtle nuances of human communication with a direct application of improving human ability. Speaker: Yejin Choi - University of Washington (USA) Topic: Multimodal Commonsense Biography: Yejin Choi is an associate professor of Paul G. Allen School of Computer Science &amp; Engineering at the University of Washington, adjunct of the Linguistics department, and affiliate of the Center for Statistics and Social Sciences. She is also a senior research manager at the Allen Institute for Artificial Intelligence. She is a co-recepient of the Marr Prize (best paper award) at ICCV 2013, a recepient of Borg Early Career Award (BECA) in 2018, and named among IEEE AI's 10 to Watch in 2016. She received her Ph.D. in Computer Science at Cornell University (advisor: Prof. Claire Cardie) and BS in Computer Science and Engineering at Seoul National University in Korea. Scope and Related Areas Humans adopt a structured multimodal signal to communicate with each other. This signal contains modalities of language (in terms of spoken text), visual (in terms of gestures and facial expressions) and acoustic (in terms of changes in tone of speech). This form of communication is commonly known as multimodal language. Modeling multimodal language is now a growing research area in NLP. It pushes the boundaries of multimodal learning, and requires advanced neural modeling of all three constituent modalities. Advances in this research area allow the field of NLP to take the leap towards better generalization to real-world communication (as opposed to limitation to textual applications), and better downstream impact on applications within fields such as Conversational AI, Virtual Reality, Robotics, HCI, Healthcare, and Education. In the past few years, there has been a surge of research papers in NLP conference including ACL, EMNLP, and NAACL on the topic of modeling multimodal language. Similar to first Challenge-HML in ACL18, the Second Grand-Challenge and Workshop on Multimodal Language (Chellenge-HML) brings the researchers across the globe once again together at ACL conference, to address the following fundamental current and future research challenges within multimodal language modeling. • Neural Modeling of Multimodal Language • Multimodal Dialogue Modeling and Generation • Multimodal Sentiment Analysis and Emotion Recognition • Language, Vision and Speech • Multimodal Artificial Social Intelligence Modeling • Multimodal Commonsense Reasoning • Multimodal RL and Control (Human-robot communication and multimodal language for robots) • Multimodal Healthcare • Multimodal Educational Systems • Multimodal Affective Computing • Multimodal Fusion and Alignment • Multimodal Representation Learning • Multimodal Sequential Modeling • Multimodal Co-learning and Transfer Learning • Multimodal Active Learning • Multimodal and Multimedia Resources • Creative Applications of Multimodal Learning in E-commerce, Art, and other Impactful Areas. Workshop Track Archival Track: Workshop papers are either full (8 pages) or short (4 pages) papers with infinite references. The formatting instructions are identical to ACL 2020 paper format. ACL guidelines suggest papers should be self-contained, with high presentation quality and properly compared to previous works. Publications related to all areas above are welcome to submit a paper. The submission should have at least two modalities present in their experiments (i.e. works should be related to multimodal learning). The workshop track encourages creative applications of multimodal learning, over new or understudied datasets. All accepted papers will be presented during poster sessions at the workshop. Selected papers will also be invited as contributed talks. All the papers (full or short) will be indexed as part of the ACL 2020 workshop proceedings. All submitted material should be considered novel at the time of submission. Further results and experiments on existing works is also acceptable for submission. Submission template: download zip, access overleaf. Non-archival Track: The workshop also includes a non-archival track to allow submission of previously published papers and double submissions to other conferences or journals. Accepted non-archival papers will still be presented as posters or talks at the workshop. There are no formatting or page restrictions for non-archival submissions. The accepted papers to the non-archival track will be displayed on the workshop website, but will NOT be included in the ACL 2020 Workshop proceedings or otherwise archived. Grand-Challenge Track START THE GRAND CHALLENGE HERE Grand-challenge (shared task) papers are between 6-8 pages of content with infinite references. The formatting instructions are identical to ACL 2020 paper format. ACL guidelines (se…